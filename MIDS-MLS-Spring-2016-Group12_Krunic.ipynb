{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "\n",
    "#### Members\n",
    "Ron Cordell, Ted Dunmire, Filip Krunic\n",
    "\n",
    "#### Emails (resp.)\n",
    "glenn.dunmire.iv@gmail.com, *TedsEmail*, fkrunic@ischool.berkeley.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Question 3.0\n",
    "**\n",
    "\n",
    "#### Solution: \n",
    "\n",
    "Here's some text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Question 3.1\n",
    "**\n",
    "\n",
    "#### Solution: \n",
    "\n",
    "Here's some text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Question 3.2\n",
    "**\n",
    "\n",
    "#### Solution: \n",
    "\n",
    "Here's some text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Question 3.3\n",
    "*Using a single reducer: Report your findings such as:*\n",
    "1. *Number of unique products*\n",
    "2. *Largest basket*\n",
    "3. *Report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. *\n",
    "\n",
    "#### Solution: \n",
    "\n",
    "##### Mapper\n",
    "\n",
    "Below is the mapper used to accomplish this task. It emits each token along with a basket size to the reducers for additional processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapperQ33.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapperQ33.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "def readInput(file, separator=None):\n",
    "\tfor line in file:\n",
    "\t\tyield line.split(separator)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "\t# Read input \n",
    "\tdata = readInput(sys.stdin)\n",
    "\tfor line in data: \n",
    "\t\tholdingDict = defaultdict(int)\n",
    "\t\tbasketSize = len(line)\n",
    "\n",
    "\n",
    "\t\t# Append elements \n",
    "\t\tfor token in line: \n",
    "\t\t\tholdingDict[token] += 1\n",
    "\n",
    "\n",
    "\t\t# Emit results \n",
    "\t\tfor k, v in holdingDict.iteritems(): \n",
    "\t\t\tbasketInfo = str([v, basketSize])\n",
    "\t\t\tprint '%s%s%s' % (k, '\\t', basketInfo)\n",
    "            \n",
    "\t# Update counter \n",
    "\tsys.stderr.write(\"reporter:counter:User-Defined,Number of Mappers,1\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reducer\n",
    "\n",
    "Below is the reducer used to accomplish this task. The reducer uses a `defaultdict` object from `collections` to automatically collate tokens and their counts without having to explicitly instantiate the key. This is convenient when reading from `sys.stdin` as one can yield lines and store the tokens simultaneously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducerQ33.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducerQ33.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from __future__ import division \n",
    "import sys \n",
    "from collections import defaultdict\n",
    "import ast \n",
    "\n",
    "def readInput(file, separator='\\t'):\n",
    "\tfor line in file:\n",
    "\t\tyield line.split(separator)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\t# Final store \n",
    "\tstoringDict = defaultdict(int)\n",
    "\tmaxBasket = 0\n",
    "\ttotalTerms = 0\n",
    "\n",
    "\n",
    "\t# Read data \n",
    "\tdata = readInput(sys.stdin)\n",
    "\tfor line in data: \n",
    "\n",
    "\n",
    "\t\t# Parse value \n",
    "\t\ttoken = line[0]\n",
    "\t\ttermCount, basketSize = ast.literal_eval(line[1])\n",
    "\t\ttotalTerms += termCount\n",
    "\n",
    "\n",
    "\t\t# Store results \n",
    "\t\tstoringDict[token] += termCount\n",
    "\t\tmaxBasket = max(maxBasket, basketSize)\n",
    "\n",
    "\n",
    "\t# Metrics\n",
    "\tnumUniqueProducts = len(set(storingDict.keys()))\n",
    "\tlargestBasket = maxBasket\n",
    "\n",
    "\n",
    "\t# Compute frequencies \n",
    "\tfor k, v in storingDict.iteritems():\n",
    "\t\tstoringDict[k] = v\n",
    "\n",
    "\t\n",
    "\t# Find most frequent terms \n",
    "\tmostFrequentTerms = [(k, v, round(v / totalTerms, 4)) for k, v in storingDict.iteritems()]\n",
    "\tmostFrequentTerms = sorted(mostFrequentTerms, \n",
    "\t\t\t\t\t\t\t\tkey = lambda x: x[1], \n",
    "\t\t\t\t\t\t\t\treverse = True)\n",
    "\n",
    "\n",
    "\t# Get results \n",
    "\tprint '\\n' + '========== Number of Unique Products ==========' + '\\n'\n",
    "\tprint 'Answer: ' + str(numUniqueProducts) + '\\n'\n",
    "    \n",
    "\tprint '\\n' + '========= Largest Basket ==========' + '\\n'\n",
    "\tprint 'Answer: ' + str(largestBasket) + '\\n'    \n",
    "\n",
    "\tprint '========== Most Frequent Terms ==========' + '\\n'\n",
    "\ttemplate = \"{0:20}|{1:20}|{2:20}\"\n",
    "\tprint template.format(\"ITEM\", \"FREQUENCY\", \"RELATIVE FREQUENCY\")\n",
    "\t\n",
    "\t# Print terms \n",
    "\tfor termPair in mostFrequentTerms[:50]: \n",
    "\t\tprint template.format(*termPair)\n",
    "        \n",
    "\t# Update counter \n",
    "\tsys.stderr.write(\"reporter:counter:User-Defined,Number of Reducers,1\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Wrapper\n",
    "\n",
    "Below is the bash script that's used to submit the Hadoop Streaming job. It maps user input to variables and specifies the options for the job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wrapperQ33.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile wrapperQ33.sh\n",
    "#!/bin/bash\n",
    "\n",
    "\n",
    "# Initialize\n",
    "RAW_DATA=$1\n",
    "RAW_MAPPER=$2\n",
    "RAW_REDUCER=$3\n",
    "\n",
    "\n",
    "# Hadoop variables \n",
    "HDFS_DIR=\"/user/john/notebook\"\n",
    "HDFS_INPUT=\"$HDFS_DIR/input\"\n",
    "HDFS_OUTPUT=\"$HDFS_DIR/output\"\n",
    "HDFS_FILES=\"$HDFS_DIR/files\"\n",
    "\n",
    "\n",
    "# Local variables \n",
    "PROJECT_DIR=\"/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook\"\n",
    "DATA=\"$PROJECT_DIR/$RAW_DATA\"\n",
    "MAPPER=\"$PROJECT_DIR/$RAW_MAPPER\"\n",
    "REDUCER=\"$PROJECT_DIR/$RAW_REDUCER\"\n",
    "\n",
    "\n",
    "# NAIVE=\"$PROJECT_DIR/naiveBayes.py\"\n",
    "STREAMING_JAR=\"$PROJECT_DIR/hadoop-streaming-2.6.0.jar\"\n",
    "\n",
    "\n",
    "# Make directories and put file \n",
    "hdfs dfs -rm -r $HDFS_DIR\n",
    "hdfs dfs -mkdir $HDFS_DIR $HDFS_INPUT $HDFS_FILES\n",
    "hdfs dfs -put $DATA $HDFS_INPUT\n",
    "\n",
    "\n",
    "# Execute\n",
    "hadoop jar $STREAMING_JAR \\\n",
    "\t-file \"$MAPPER\" -mapper \"$MAPPER\" \\\n",
    "\t-file \"$REDUCER\" -reducer \"$REDUCER\" \\\n",
    "\t-input $HDFS_INPUT \\\n",
    "\t-output $HDFS_OUTPUT \n",
    "\n",
    "\n",
    "# Output results  \n",
    "if [ $? -eq 0 ]; then \n",
    "\thdfs dfs -cat $HDFS_OUTPUT/part-00000\n",
    "fi\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Submit Job\n",
    "\n",
    "Below we submit the job. The wrapper takes as arguments the data, mapper, and reducer. Later versions of the wrapper also allow combiner arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 16:05:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 16:05:42 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/john/notebook\n",
      "16/01/30 16:05:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 16:05:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 16:05:49 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/30 16:05:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/mapperQ33.py, /Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/reducerQ33.py] [] /var/folders/0w/8hzv7rsj3qgdynsjlqy3gjsc0000gn/T/streamjob3210653833237786513.jar tmpDir=null\n",
      "16/01/30 16:05:50 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/30 16:05:50 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/30 16:05:50 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/30 16:05:51 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/30 16:05:51 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/30 16:05:51 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1059638235_0001\n",
      "16/01/30 16:05:52 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/mapperQ33.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1454187951883/mapperQ33.py\n",
      "16/01/30 16:05:52 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/reducerQ33.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1454187951884/reducerQ33.py\n",
      "16/01/30 16:05:52 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/30 16:05:52 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/30 16:05:52 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 16:05:52 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/30 16:05:52 INFO mapreduce.Job: Running job: job_local1059638235_0001\n",
      "16/01/30 16:05:52 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/30 16:05:52 INFO mapred.LocalJobRunner: Starting task: attempt_local1059638235_0001_m_000000_0\n",
      "16/01/30 16:05:52 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 16:05:52 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 16:05:52 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 16:05:52 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/john/notebook/input/ProductPurchaseData.txt:0+3458517\n",
      "16/01/30 16:05:52 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/30 16:05:52 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/30 16:05:52 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/30 16:05:52 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/30 16:05:52 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/30 16:05:52 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/30 16:05:52 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/30 16:05:53 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./mapperQ33.py]\n",
      "16/01/30 16:05:53 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/30 16:05:53 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/30 16:05:53 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/30 16:05:53 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/30 16:05:53 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/30 16:05:53 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/30 16:05:53 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/30 16:05:53 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/30 16:05:53 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/30 16:05:53 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/30 16:05:53 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/30 16:05:53 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/30 16:05:53 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:05:53 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:05:53 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:05:53 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:05:53 INFO streaming.PipeMapRed: Records R/W=1216/1\n",
      "16/01/30 16:05:53 INFO mapreduce.Job: Job job_local1059638235_0001 running in uber mode : false\n",
      "16/01/30 16:05:53 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/30 16:05:54 INFO streaming.PipeMapRed: R/W/S=10000/119481/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:05:55 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 16:05:55 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 16:05:55 INFO mapred.LocalJobRunner: \n",
      "16/01/30 16:05:55 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/30 16:05:55 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 16:05:55 INFO mapred.MapTask: bufstart = 0; bufend = 6397909; bufvoid = 104857600\n",
      "16/01/30 16:05:55 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24691116(98764464); length = 1523281/6553600\n",
      "16/01/30 16:05:56 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/30 16:05:56 INFO mapred.Task: Task:attempt_local1059638235_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/30 16:05:56 INFO mapred.LocalJobRunner: Records R/W=1216/1\n",
      "16/01/30 16:05:56 INFO mapred.Task: Task 'attempt_local1059638235_0001_m_000000_0' done.\n",
      "16/01/30 16:05:56 INFO mapred.LocalJobRunner: Finishing task: attempt_local1059638235_0001_m_000000_0\n",
      "16/01/30 16:05:56 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/30 16:05:56 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/30 16:05:56 INFO mapred.LocalJobRunner: Starting task: attempt_local1059638235_0001_r_000000_0\n",
      "16/01/30 16:05:56 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 16:05:56 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 16:05:56 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 16:05:56 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@70e26d9a\n",
      "16/01/30 16:05:56 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/30 16:05:56 INFO reduce.EventFetcher: attempt_local1059638235_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/30 16:05:56 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1059638235_0001_m_000000_0 decomp: 7159553 len: 7159557 to MEMORY\n",
      "16/01/30 16:05:56 INFO reduce.InMemoryMapOutput: Read 7159553 bytes from map-output for attempt_local1059638235_0001_m_000000_0\n",
      "16/01/30 16:05:56 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 7159553, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->7159553\n",
      "16/01/30 16:05:56 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/30 16:05:56 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 16:05:56 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/30 16:05:56 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/30 16:05:56 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 7159542 bytes\n",
      "16/01/30 16:05:57 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/30 16:05:57 INFO reduce.MergeManagerImpl: Merged 1 segments, 7159553 bytes to disk to satisfy reduce memory limit\n",
      "16/01/30 16:05:57 INFO reduce.MergeManagerImpl: Merging 1 files, 7159557 bytes from disk\n",
      "16/01/30 16:05:57 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/30 16:05:57 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/30 16:05:57 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 7159542 bytes\n",
      "16/01/30 16:05:57 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 16:05:57 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./reducerQ33.py]\n",
      "16/01/30 16:05:57 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/30 16:05:57 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/30 16:05:57 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:05:57 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:05:57 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:05:57 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:05:57 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:06:00 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:33333=100000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 16:06:02 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 16:06:03 INFO mapreduce.Job:  map 100% reduce 82%\n",
      "16/01/30 16:06:03 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:40000=200000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 16:06:05 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 16:06:06 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:37500=300000/8 [rec/s] out:0=0/8 [rec/s]\n",
      "16/01/30 16:06:06 INFO mapreduce.Job:  map 100% reduce 91%\n",
      "16/01/30 16:06:08 INFO streaming.PipeMapRed: Records R/W=380821/1\n",
      "16/01/30 16:06:08 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 16:06:08 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 16:06:08 INFO mapred.LocalJobRunner: Records R/W=380821/1 > reduce\n",
      "16/01/30 16:06:08 INFO mapred.Task: Task:attempt_local1059638235_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/30 16:06:08 INFO mapred.LocalJobRunner: Records R/W=380821/1 > reduce\n",
      "16/01/30 16:06:08 INFO mapred.Task: Task attempt_local1059638235_0001_r_000000_0 is allowed to commit now\n",
      "16/01/30 16:06:08 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1059638235_0001_r_000000_0' to hdfs://localhost:9000/user/john/notebook/output/_temporary/0/task_local1059638235_0001_r_000000\n",
      "16/01/30 16:06:08 INFO mapred.LocalJobRunner: Records R/W=380821/1 > reduce\n",
      "16/01/30 16:06:08 INFO mapred.Task: Task 'attempt_local1059638235_0001_r_000000_0' done.\n",
      "16/01/30 16:06:08 INFO mapred.LocalJobRunner: Finishing task: attempt_local1059638235_0001_r_000000_0\n",
      "16/01/30 16:06:08 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/30 16:06:09 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/30 16:06:09 INFO mapreduce.Job: Job job_local1059638235_0001 completed successfully\n",
      "16/01/30 16:06:09 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=14326178\n",
      "\t\tFILE: Number of bytes written=22073423\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6917034\n",
      "\t\tHDFS: Number of bytes written=3434\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=380821\n",
      "\t\tMap output bytes=6397909\n",
      "\t\tMap output materialized bytes=7159557\n",
      "\t\tInput split bytes=122\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12592\n",
      "\t\tReduce shuffle bytes=7159557\n",
      "\t\tReduce input records=380821\n",
      "\t\tReduce output records=63\n",
      "\t\tSpilled Records=761642\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=500170752\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3434\n",
      "16/01/30 16:06:09 INFO streaming.StreamJob: Output directory: /user/john/notebook/output\n",
      "16/01/30 16:06:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\t\n",
      "========== Number of Unique Products ==========\t\n",
      "\t\n",
      "Answer: 12592\t\n",
      "\t\n",
      "\t\n",
      "========= Largest Basket ==========\t\n",
      "\t\n",
      "Answer: 37\t\n",
      "\t\n",
      "========== Most Frequent Terms ==========\t\n",
      "\t\n",
      "ITEM                |FREQUENCY           |RELATIVE FREQUENCY  \t\n",
      "DAI62779            |                6667|              0.0175\t\n",
      "FRO40251            |                3881|              0.0102\t\n",
      "ELE17451            |                3875|              0.0102\t\n",
      "GRO73461            |                3602|              0.0095\t\n",
      "SNA80324            |                3044|               0.008\t\n",
      "ELE32164            |                2851|              0.0075\t\n",
      "DAI75645            |                2736|              0.0072\t\n",
      "SNA45677            |                2455|              0.0064\t\n",
      "FRO31317            |                2330|              0.0061\t\n",
      "DAI85309            |                2293|               0.006\t\n",
      "ELE26917            |                2292|               0.006\t\n",
      "FRO80039            |                2233|              0.0059\t\n",
      "GRO21487            |                2115|              0.0056\t\n",
      "SNA99873            |                2083|              0.0055\t\n",
      "GRO59710            |                2004|              0.0053\t\n",
      "GRO71621            |                1920|               0.005\t\n",
      "FRO85978            |                1918|               0.005\t\n",
      "GRO30386            |                1840|              0.0048\t\n",
      "ELE74009            |                1816|              0.0048\t\n",
      "GRO56726            |                1784|              0.0047\t\n",
      "DAI63921            |                1773|              0.0047\t\n",
      "GRO46854            |                1756|              0.0046\t\n",
      "ELE66600            |                1713|              0.0045\t\n",
      "DAI83733            |                1712|              0.0045\t\n",
      "FRO32293            |                1702|              0.0045\t\n",
      "ELE66810            |                1697|              0.0045\t\n",
      "SNA55762            |                1646|              0.0043\t\n",
      "DAI22177            |                1627|              0.0043\t\n",
      "FRO78087            |                1531|               0.004\t\n",
      "ELE99737            |                1516|               0.004\t\n",
      "ELE34057            |                1489|              0.0039\t\n",
      "GRO94758            |                1489|              0.0039\t\n",
      "FRO35904            |                1436|              0.0038\t\n",
      "FRO53271            |                1420|              0.0037\t\n",
      "SNA93860            |                1407|              0.0037\t\n",
      "SNA90094            |                1390|              0.0036\t\n",
      "GRO38814            |                1352|              0.0036\t\n",
      "ELE56788            |                1345|              0.0035\t\n",
      "GRO61133            |                1321|              0.0035\t\n",
      "ELE74482            |                1316|              0.0035\t\n",
      "DAI88807            |                1316|              0.0035\t\n",
      "ELE59935            |                1311|              0.0034\t\n",
      "SNA96271            |                1295|              0.0034\t\n",
      "DAI43223            |                1290|              0.0034\t\n",
      "ELE91337            |                1289|              0.0034\t\n",
      "GRO15017            |                1275|              0.0033\t\n",
      "DAI31081            |                1261|              0.0033\t\n",
      "GRO81087            |                1220|              0.0032\t\n",
      "DAI22896            |                1219|              0.0032\t\n",
      "GRO85051            |                1214|              0.0032\t\n"
     ]
    }
   ],
   "source": [
    "!bash wrapperQ33.sh ProductPurchaseData.txt mapperQ33.py reducerQ33.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Question 3.4\n",
    "*Write a map-reduce program \n",
    "to find products which are frequently browsed together. Fix the support count (cooccurence count) to s = 100 \n",
    "(i.e. product pairs need to occur together at least 100 times to be considered frequent) \n",
    "and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more.*\n",
    "\n",
    "*List the top 50 product pairs with corresponding support count (aka frequency), and relative frequency or support (number of records where they coccur, the number of records where they coccur/the number of baskets in the dataset)  in decreasing order of support  for frequent (100>count) itemsets of size 2. *\n",
    "\n",
    "*Use the Pairs pattern (lecture 3)  to  extract these frequent itemsets of size 2. Free free to use combiners if they bring value. Instrument your code with counters for count the number of times your mapper, combiner and reducers are called.*\n",
    "\n",
    "#### Solution: \n",
    "\n",
    "##### Mapper\n",
    "\n",
    "Below is the mapper code for the pairs implementation. It generates tuple-combinations from the input lines, sorts them to get unique keys, then emits them to the reducer or optional combiner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapperQ34.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapperQ34.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from __future__ import division \n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "def readInput(file, separator=None):\n",
    "\tfor line in file:\n",
    "\t\tyield line.split(separator)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\t# Read input \n",
    "\tdata = readInput(sys.stdin)\n",
    "\tfor line in data: \n",
    "\n",
    "\t\t# Get unique keys \n",
    "\t\tpairs = list(combinations(line, 2))\n",
    "\t\t\n",
    "\n",
    "\t\t# Sort keys \n",
    "\t\tsortedPairs = []\n",
    "\t\tfor pair in pairs: \n",
    "\t\t\tpList = list(pair)\n",
    "\t\t\tpList.sort()\n",
    "\t\t\tsortedPairs.append(tuple(pList))\n",
    "\n",
    "\t\t# Emit\n",
    "\t\tfor pair in sortedPairs: \n",
    "\t\t\tprint '%s%s%s' % (pair, '\\t', 1)\n",
    "\t\t\t\n",
    "\t\t\n",
    "\t# Update counter \n",
    "\tsys.stderr.write(\"reporter:counter:User-Defined,Number of Mappers,1\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reducer\n",
    "\n",
    "Below is the reducer for this process. It uses a simple key-aggregation to collate the results from the mappers, where the keys are unique tuples from the lines read in by the mapper. The uniqueness condition is specified at the line-level when read by the mapper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducerQ34.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducerQ34.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from __future__ import division \n",
    "import sys \n",
    "from collections import defaultdict\n",
    "import ast \n",
    "\n",
    "\n",
    "def readInput(file, separator='\\t'):\n",
    "\tfor line in file:\n",
    "\t\tyield line.split(separator)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\t# Final store \n",
    "\tstoringDict = defaultdict(int)\n",
    "\tsupport = 100\n",
    "\n",
    "\n",
    "\t# Read data \n",
    "\tdata = readInput(sys.stdin)\n",
    "\tfor line in data: \n",
    "\n",
    "\t\t# Parse value \n",
    "\t\ttoken = line[0]\n",
    "\t\ttermCount = int(line[1])\n",
    "\n",
    "\n",
    "\t\t# Store results \n",
    "\t\tstoringDict[token] += termCount\n",
    "\n",
    "\t\n",
    "\t# Filter\n",
    "\tfilterDict = defaultdict(int)\n",
    "\tfor k, v in storingDict.iteritems(): \n",
    "\t\tif v >= support:\n",
    "\t\t\tfilterDict[k] += v \n",
    "\n",
    "\n",
    "\t# Find most frequent terms \n",
    "\tmostFrequentTerms = [(k, v, support) for k, v in filterDict.iteritems()]\n",
    "\tmostFrequentTerms = sorted(mostFrequentTerms, \n",
    "\t\t\t\t\t\t\t\tkey = lambda x: x[1], \n",
    "\t\t\t\t\t\t\t\treverse = True)\n",
    "\n",
    "\n",
    "\t# Get results \n",
    "\tprint '\\n' + '========== Most Frequent Terms ==========' + '\\n'\n",
    "\ttemplate = \"{0:30}|{1:20}|{2:20}\"\n",
    "\tprint template.format(\"PAIR\", \"SUPPORT COUNT\", \"SUPPORT\")\n",
    "\t\n",
    "\t# Print terms \n",
    "\tfor termPair in mostFrequentTerms[:50]: \n",
    "\t\tprint template.format(*termPair)\n",
    "        \n",
    "\t# Update counter \n",
    "\tsys.stderr.write(\"reporter:counter:User-Defined,Number of Reducers,1\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combiner (Optional)\n",
    "\n",
    "Below is the code for an optional combiner. It has the same signature as the reducer and does the same essential aggregation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combinerQ34.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combinerQ34.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from __future__ import division \n",
    "import sys \n",
    "from collections import defaultdict\n",
    "import ast \n",
    "\n",
    "\n",
    "def readInput(file, separator='\\t'):\n",
    "\tfor line in file:\n",
    "\t\tyield line.split(separator)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\t# Final store \n",
    "\tstoringDict = defaultdict(int)\n",
    "\n",
    "\n",
    "\t# Read data \n",
    "\tdata = readInput(sys.stdin)\n",
    "\tfor line in data: \n",
    "\n",
    "\t\t# Parse value \n",
    "\t\ttoken = line[0]\n",
    "\t\ttermCount = int(line[1])\n",
    "\t\tstoringDict[token] += termCount\n",
    "\n",
    "\n",
    "\t# Emit\n",
    "\tfor k, v in storingDict.iteritems(): \n",
    "\t\tprint '%s%s%s' % (k, '\\t', v)\n",
    "        \n",
    "\t# Update counter \n",
    "\tsys.stderr.write(\"reporter:counter:User-Defined,Number of Combiners,1\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Wrapper\n",
    "\n",
    "Here is the wrapper for this particular submission. The noticeable difference is that it now has been modified to pass a combiner file to Hadoop Streaming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wrapperQ34.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile wrapperQ34.sh\n",
    "#!/bin/bash\n",
    "\n",
    "\n",
    "# Initialize\n",
    "RAW_DATA=$1\n",
    "RAW_MAPPER=$2\n",
    "RAW_REDUCER=$3\n",
    "RAW_COMBINER=$4\n",
    "\n",
    "\n",
    "# Hadoop variables \n",
    "HDFS_DIR=\"/user/john/notebook\"\n",
    "HDFS_INPUT=\"$HDFS_DIR/input\"\n",
    "HDFS_OUTPUT=\"$HDFS_DIR/output\"\n",
    "HDFS_FILES=\"$HDFS_DIR/files\"\n",
    "\n",
    "\n",
    "# Local variables \n",
    "PROJECT_DIR=\"/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook\"\n",
    "DATA=\"$PROJECT_DIR/$RAW_DATA\"\n",
    "MAPPER=\"$PROJECT_DIR/$RAW_MAPPER\"\n",
    "REDUCER=\"$PROJECT_DIR/$RAW_REDUCER\"\n",
    "COMBINER=\"$PROJECT_DIR/$RAW_COMBINER\"\n",
    "\n",
    "\n",
    "# NAIVE=\"$PROJECT_DIR/naiveBayes.py\"\n",
    "STREAMING_JAR=\"$PROJECT_DIR/hadoop-streaming-2.6.0.jar\"\n",
    "\n",
    "\n",
    "# Make directories and put file \n",
    "hdfs dfs -rm -r $HDFS_DIR\n",
    "hdfs dfs -mkdir $HDFS_DIR $HDFS_INPUT $HDFS_FILES\n",
    "hdfs dfs -put $DATA $HDFS_INPUT\n",
    "\n",
    "\n",
    "# Execute\n",
    "hadoop jar $STREAMING_JAR \\\n",
    "\t-file \"$MAPPER\" -mapper \"$MAPPER\" \\\n",
    "\t-file \"$REDUCER\" -reducer \"$REDUCER\" \\\n",
    "    -file \"$COMBINER\" -combiner \"$COMBINER\" \\\n",
    "\t-input $HDFS_INPUT \\\n",
    "\t-output $HDFS_OUTPUT \\\n",
    "\t-cmdenv mapred.map.max.attempts=1 \\\n",
    "\t-cmdenv mapred.reduce.max.attempts=1 \\\n",
    "\n",
    "\n",
    "# Output results  \n",
    "if [ $? -eq 0 ]; then \n",
    "\thdfs dfs -cat $HDFS_OUTPUT/part-00000\n",
    "fi\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Submit Job\n",
    "\n",
    "We now submit the job for this process. Notice that an argument has been added for the combiner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 19:12:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 19:12:57 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/john/notebook\n",
      "16/01/30 19:12:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 19:13:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 19:13:05 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/30 19:13:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/mapperQ34.py, /Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/reducerQ34.py, /Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/combinerQ34.py] [] /var/folders/0w/8hzv7rsj3qgdynsjlqy3gjsc0000gn/T/streamjob5876133748798368217.jar tmpDir=null\n",
      "16/01/30 19:13:07 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/30 19:13:07 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/30 19:13:07 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/30 19:13:08 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/30 19:13:08 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/30 19:13:08 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local18481770_0001\n",
      "16/01/30 19:13:08 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/mapperQ34.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1454199188721/mapperQ34.py\n",
      "16/01/30 19:13:09 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/reducerQ34.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1454199188722/reducerQ34.py\n",
      "16/01/30 19:13:09 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/combinerQ34.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1454199188723/combinerQ34.py\n",
      "16/01/30 19:13:09 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/30 19:13:09 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/30 19:13:09 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/30 19:13:09 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 19:13:09 INFO mapreduce.Job: Running job: job_local18481770_0001\n",
      "16/01/30 19:13:09 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/30 19:13:09 INFO mapred.LocalJobRunner: Starting task: attempt_local18481770_0001_m_000000_0\n",
      "16/01/30 19:13:09 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 19:13:09 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 19:13:09 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 19:13:09 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/john/notebook/input/ProductPurchaseData.txt:0+3458517\n",
      "16/01/30 19:13:09 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/30 19:13:09 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/30 19:13:09 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/30 19:13:09 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/30 19:13:09 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/30 19:13:09 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/30 19:13:09 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/30 19:13:09 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./mapperQ34.py]\n",
      "16/01/30 19:13:09 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/30 19:13:09 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/30 19:13:09 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/30 19:13:09 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/30 19:13:09 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/30 19:13:09 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/30 19:13:09 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/30 19:13:09 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/30 19:13:09 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/30 19:13:09 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/30 19:13:09 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/30 19:13:09 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/30 19:13:09 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:13:09 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:13:09 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:13:09 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:13:09 INFO streaming.PipeMapRed: Records R/W=1216/1\n",
      "16/01/30 19:13:10 INFO mapreduce.Job: Job job_local18481770_0001 running in uber mode : false\n",
      "16/01/30 19:13:10 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/30 19:13:13 INFO streaming.PipeMapRed: R/W/S=10000/854395/0 in:2500=10000/4 [rec/s] out:213598=854395/4 [rec/s]\n",
      "16/01/30 19:13:15 INFO mapred.LocalJobRunner: Records R/W=1216/1 > map\n",
      "16/01/30 19:13:16 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "16/01/30 19:13:18 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 19:13:18 INFO mapred.MapTask: bufstart = 0; bufend = 52672653; bufvoid = 104857600\n",
      "16/01/30 19:13:18 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 18411044(73644176); length = 7803353/6553600\n",
      "16/01/30 19:13:18 INFO mapred.MapTask: (EQUATOR) 60475997 kvi 15118992(60475968)\n",
      "16/01/30 19:13:18 INFO mapred.LocalJobRunner: Records R/W=1216/1 > map\n",
      "16/01/30 19:13:19 INFO mapreduce.Job:  map 53% reduce 0%\n",
      "16/01/30 19:13:19 INFO streaming.PipeMapRed: Records R/W=28078/2194850\n",
      "16/01/30 19:13:21 INFO mapred.LocalJobRunner: Records R/W=28078/2194850 > map\n",
      "16/01/30 19:13:22 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/01/30 19:13:24 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./combinerQ34.py]\n",
      "16/01/30 19:13:24 INFO Configuration.deprecation: mapred.skip.map.auto.incr.proc.count is deprecated. Instead, use mapreduce.map.skip.proc-count.auto-incr\n",
      "16/01/30 19:13:24 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:13:24 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:13:24 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:13:24 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:13:24 INFO mapred.LocalJobRunner: Records R/W=28078/2194850 > map\n",
      "16/01/30 19:13:24 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:13:24 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:13:25 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:13:25 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 19:13:25 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 19:13:26 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:500000=500000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 19:13:26 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:300000=600000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 19:13:26 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:350000=700000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 19:13:27 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:400000=800000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 19:13:27 INFO mapred.LocalJobRunner: Records R/W=28078/2194850 > map\n",
      "16/01/30 19:13:27 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:300000=900000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 19:13:27 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:333333=1000000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 19:13:28 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:366666=1100000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 19:13:28 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:300000=1200000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 19:13:28 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:325000=1300000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 19:13:29 INFO streaming.PipeMapRed: R/W/S=1400000/0/0 in:350000=1400000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 19:13:29 INFO streaming.PipeMapRed: R/W/S=1500000/0/0 in:300000=1500000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 19:13:29 INFO streaming.PipeMapRed: R/W/S=1600000/0/0 in:320000=1600000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 19:13:30 INFO streaming.PipeMapRed: R/W/S=1700000/0/0 in:283333=1700000/6 [rec/s] out:0=0/6 [rec/s]\n",
      "16/01/30 19:13:30 INFO mapred.LocalJobRunner: Records R/W=28078/2194850 > map\n",
      "16/01/30 19:13:30 INFO streaming.PipeMapRed: R/W/S=1800000/0/0 in:300000=1800000/6 [rec/s] out:0=0/6 [rec/s]\n",
      "16/01/30 19:13:31 INFO streaming.PipeMapRed: R/W/S=1900000/0/0 in:316666=1900000/6 [rec/s] out:0=0/6 [rec/s]\n",
      "16/01/30 19:13:31 INFO streaming.PipeMapRed: Records R/W=1950839/1\n",
      "16/01/30 19:13:32 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 19:13:32 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 19:13:32 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/30 19:13:32 INFO mapred.MapTask: (RESET) equator 60475997 kv 15118992(60475968) kvi 13168168(52672672)\n",
      "16/01/30 19:13:32 INFO streaming.PipeMapRed: Records R/W=31101/2438546\n",
      "16/01/30 19:13:33 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 19:13:33 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 19:13:33 INFO mapred.LocalJobRunner: Records R/W=28078/2194850 > map\n",
      "16/01/30 19:13:33 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/30 19:13:33 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 19:13:33 INFO mapred.MapTask: bufstart = 60475997; bufend = 76222883; bufvoid = 104857600\n",
      "16/01/30 19:13:33 INFO mapred.MapTask: kvstart = 15118992(60475968); kvend = 12786124(51144496); length = 2332869/6553600\n",
      "16/01/30 19:13:33 INFO mapred.LocalJobRunner: Records R/W=31101/2438546 > sort\n",
      "16/01/30 19:13:34 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./combinerQ34.py]\n",
      "16/01/30 19:13:34 INFO Configuration.deprecation: mapred.skip.reduce.auto.incr.proc.count is deprecated. Instead, use mapreduce.reduce.skip.proc-count.auto-incr\n",
      "16/01/30 19:13:34 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:13:34 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:13:34 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:13:34 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:13:34 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:13:34 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:13:35 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:13:35 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 19:13:35 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 19:13:36 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:500000=500000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 19:13:36 INFO streaming.PipeMapRed: Records R/W=583218/1\n",
      "16/01/30 19:13:36 INFO mapred.LocalJobRunner: Records R/W=583218/1 > sort\n",
      "16/01/30 19:13:36 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 19:13:36 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 19:13:36 INFO mapred.MapTask: Finished spill 1\n",
      "16/01/30 19:13:36 INFO mapred.Merger: Merging 2 sorted segments\n",
      "16/01/30 19:13:36 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 28603073 bytes\n",
      "16/01/30 19:13:39 INFO mapred.Task: Task:attempt_local18481770_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/30 19:13:39 INFO mapred.LocalJobRunner: Records R/W=583218/1 > sort\n",
      "16/01/30 19:13:39 INFO mapred.Task: Task 'attempt_local18481770_0001_m_000000_0' done.\n",
      "16/01/30 19:13:39 INFO mapred.LocalJobRunner: Finishing task: attempt_local18481770_0001_m_000000_0\n",
      "16/01/30 19:13:39 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/30 19:13:39 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/30 19:13:39 INFO mapred.LocalJobRunner: Starting task: attempt_local18481770_0001_r_000000_0\n",
      "16/01/30 19:13:39 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 19:13:39 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 19:13:39 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 19:13:39 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@301bfb31\n",
      "16/01/30 19:13:39 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/30 19:13:39 INFO reduce.EventFetcher: attempt_local18481770_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/30 19:13:39 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local18481770_0001_m_000000_0 decomp: 28603117 len: 28603121 to MEMORY\n",
      "16/01/30 19:13:39 INFO reduce.InMemoryMapOutput: Read 28603117 bytes from map-output for attempt_local18481770_0001_m_000000_0\n",
      "16/01/30 19:13:39 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 28603117, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->28603117\n",
      "16/01/30 19:13:39 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/30 19:13:39 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 19:13:39 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/30 19:13:39 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/30 19:13:39 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 28603090 bytes\n",
      "16/01/30 19:13:39 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/30 19:13:40 INFO reduce.MergeManagerImpl: Merged 1 segments, 28603117 bytes to disk to satisfy reduce memory limit\n",
      "16/01/30 19:13:40 INFO reduce.MergeManagerImpl: Merging 1 files, 28603121 bytes from disk\n",
      "16/01/30 19:13:40 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/30 19:13:40 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/30 19:13:40 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 28603090 bytes\n",
      "16/01/30 19:13:40 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 19:13:40 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./reducerQ34.py]\n",
      "16/01/30 19:13:40 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/30 19:13:40 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/30 19:13:40 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:13:40 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:13:40 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:13:40 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:13:40 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:13:41 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:13:41 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:200000=200000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 19:13:42 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 19:13:43 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:200000=400000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 19:13:43 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:250000=500000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 19:13:43 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:200000=600000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 19:13:44 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:233333=700000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 19:13:44 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:200000=800000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 19:13:45 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 19:13:45 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:225000=900000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 19:13:45 INFO mapreduce.Job:  map 100% reduce 96%\n",
      "16/01/30 19:13:46 INFO streaming.PipeMapRed: Records R/W=985037/1\n",
      "16/01/30 19:13:46 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 19:13:46 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 19:13:46 INFO mapred.Task: Task:attempt_local18481770_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/30 19:13:46 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 19:13:46 INFO mapred.Task: Task attempt_local18481770_0001_r_000000_0 is allowed to commit now\n",
      "16/01/30 19:13:46 INFO output.FileOutputCommitter: Saved output of task 'attempt_local18481770_0001_r_000000_0' to hdfs://localhost:9000/user/john/notebook/output/_temporary/0/task_local18481770_0001_r_000000\n",
      "16/01/30 19:13:46 INFO mapred.LocalJobRunner: Records R/W=985037/1 > reduce\n",
      "16/01/30 19:13:46 INFO mapred.Task: Task 'attempt_local18481770_0001_r_000000_0' done.\n",
      "16/01/30 19:13:46 INFO mapred.LocalJobRunner: Finishing task: attempt_local18481770_0001_r_000000_0\n",
      "16/01/30 19:13:46 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/30 19:13:47 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/30 19:13:47 INFO mapreduce.Job: Job job_local18481770_0001 completed successfully\n",
      "16/01/30 19:13:47 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=114421392\n",
      "\t\tFILE: Number of bytes written=143608919\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6917034\n",
      "\t\tHDFS: Number of bytes written=3821\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=2534057\n",
      "\t\tMap output bytes=68419539\n",
      "\t\tMap output materialized bytes=28603121\n",
      "\t\tInput split bytes=122\n",
      "\t\tCombine input records=2534057\n",
      "\t\tCombine output records=985037\n",
      "\t\tReduce input groups=985037\n",
      "\t\tReduce shuffle bytes=28603121\n",
      "\t\tReduce input records=985037\n",
      "\t\tReduce output records=54\n",
      "\t\tSpilled Records=2955111\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=15\n",
      "\t\tTotal committed heap usage (bytes)=536346624\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tUser-Defined\n",
      "\t\tNumber of Combiners=2\n",
      "\t\tNumber of Mappers=1\n",
      "\t\tNumber of Reducers=1\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3821\n",
      "16/01/30 19:13:47 INFO streaming.StreamJob: Output directory: /user/john/notebook/output\n",
      "16/01/30 19:13:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\t\n",
      "========== Most Frequent Terms ==========\t\n",
      "\t\n",
      "PAIR                          |SUPPORT COUNT       |SUPPORT             \t\n",
      "('DAI62779', 'ELE17451')      |                1592|                 100\t\n",
      "('FRO40251', 'SNA80324')      |                1412|                 100\t\n",
      "('DAI75645', 'FRO40251')      |                1254|                 100\t\n",
      "('FRO40251', 'GRO85051')      |                1213|                 100\t\n",
      "('DAI62779', 'GRO73461')      |                1139|                 100\t\n",
      "('DAI75645', 'SNA80324')      |                1130|                 100\t\n",
      "('DAI62779', 'FRO40251')      |                1070|                 100\t\n",
      "('DAI62779', 'SNA80324')      |                 923|                 100\t\n",
      "('DAI62779', 'DAI85309')      |                 918|                 100\t\n",
      "('ELE32164', 'GRO59710')      |                 911|                 100\t\n",
      "('FRO40251', 'GRO73461')      |                 882|                 100\t\n",
      "('DAI62779', 'DAI75645')      |                 882|                 100\t\n",
      "('DAI62779', 'ELE92920')      |                 877|                 100\t\n",
      "('FRO40251', 'FRO92469')      |                 835|                 100\t\n",
      "('DAI62779', 'ELE32164')      |                 832|                 100\t\n",
      "('DAI75645', 'GRO73461')      |                 712|                 100\t\n",
      "('DAI43223', 'ELE32164')      |                 711|                 100\t\n",
      "('DAI62779', 'GRO30386')      |                 709|                 100\t\n",
      "('ELE17451', 'FRO40251')      |                 697|                 100\t\n",
      "('DAI85309', 'ELE99737')      |                 659|                 100\t\n",
      "('DAI62779', 'ELE26917')      |                 650|                 100\t\n",
      "('GRO21487', 'GRO73461')      |                 631|                 100\t\n",
      "('DAI62779', 'SNA45677')      |                 604|                 100\t\n",
      "('ELE17451', 'SNA80324')      |                 597|                 100\t\n",
      "('DAI62779', 'GRO71621')      |                 595|                 100\t\n",
      "('DAI62779', 'SNA55762')      |                 593|                 100\t\n",
      "('DAI62779', 'DAI83733')      |                 586|                 100\t\n",
      "('ELE17451', 'GRO73461')      |                 580|                 100\t\n",
      "('GRO73461', 'SNA80324')      |                 562|                 100\t\n",
      "('DAI62779', 'GRO59710')      |                 561|                 100\t\n",
      "('DAI62779', 'FRO80039')      |                 550|                 100\t\n",
      "('DAI75645', 'ELE17451')      |                 547|                 100\t\n",
      "('DAI62779', 'SNA93860')      |                 537|                 100\t\n",
      "('DAI55148', 'DAI62779')      |                 526|                 100\t\n",
      "('DAI43223', 'GRO59710')      |                 512|                 100\t\n",
      "('ELE17451', 'ELE32164')      |                 511|                 100\t\n",
      "('DAI62779', 'SNA18336')      |                 506|                 100\t\n",
      "('ELE32164', 'GRO73461')      |                 486|                 100\t\n",
      "('DAI85309', 'ELE17451')      |                 482|                 100\t\n",
      "('DAI62779', 'FRO78087')      |                 482|                 100\t\n",
      "('DAI62779', 'GRO94758')      |                 479|                 100\t\n",
      "('GRO85051', 'SNA80324')      |                 471|                 100\t\n",
      "('DAI62779', 'GRO21487')      |                 471|                 100\t\n",
      "('ELE17451', 'GRO30386')      |                 468|                 100\t\n",
      "('FRO85978', 'SNA95666')      |                 463|                 100\t\n",
      "('DAI62779', 'FRO19221')      |                 462|                 100\t\n",
      "('DAI62779', 'GRO46854')      |                 461|                 100\t\n",
      "('DAI43223', 'DAI62779')      |                 459|                 100\t\n",
      "('ELE92920', 'SNA18336')      |                 455|                 100\t\n",
      "('DAI88079', 'FRO40251')      |                 446|                 100\t\n"
     ]
    }
   ],
   "source": [
    "!bash wrapperQ34.sh ProductPurchaseData.txt mapperQ34.py reducerQ34.py combinerQ34.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the counters, check the 'User-Defined' category in the job logs appearing just prior to the reducer results. It should appear as follows: \n",
    "\n",
    "\tUser-Defined\n",
    "\t\tNumber of Combiners=2\n",
    "\t\tNumber of Mappers=1\n",
    "\t\tNumber of Reducers=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Question 3.5\n",
    "*Repeat 3.4 using the stripes design pattern for finding cooccuring pairs.\n",
    "\n",
    "*Report  the compute times for stripes job versus the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)*\n",
    "\n",
    "*Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts. Discuss the differences in these counts between the Pairs and Stripes jobs*\n",
    "\n",
    "#### Solution: \n",
    "\n",
    "##### Mapper\n",
    "\n",
    "The mapper for the stripes implementation is noticeably different than the one used for the pairs implementation. In particular, a dictionary stripe is emitted which is then parsed literally by the reducer or optional combiner. From here, the stripes are aggregated per token then divided by two to compensate for the two combinations in which a key can be updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapperQ35.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapperQ35.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "def readInput(file, separator=None):\n",
    "\tfor line in file:\n",
    "\t\tyield line.split(separator)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\t# Read input \n",
    "\tdata = readInput(sys.stdin)\n",
    "\n",
    "\tfor line in data: \n",
    "\t\tfor token in line: \n",
    "\n",
    "\t\t\toccurrence = defaultdict(int) \n",
    "\n",
    "\t\t\t# Remove token from neighbors\n",
    "\t\t\tstripe = [x for x in line if x != token]\n",
    "\t\t\t\n",
    "\t\t\t# Create co-occurrence array \n",
    "\t\t\tfor neighbor in stripe: \n",
    "\t\t\t\toccurrence[neighbor] += 1\n",
    "\n",
    "\t\t\t# Emit\n",
    "\t\t\tcArray = dict(occurrence)\n",
    "\n",
    "\t\t\tprint '%s%s%s' % (token, '\\t', str(cArray))\n",
    "\n",
    "\t# Update counter \n",
    "\tsys.stderr.write(\"reporter:counter:User-Defined,Number of Mappers,1\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reducer\n",
    "\n",
    "Here, the reducer has been changed to compensate for the adjusted input. The reducer aggregates the stripes by aggregating the incoming dictionaries. It does this efficiently using `defaultdict`, an automatically instantiating key-value dictionary included in `collections`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducerQ35.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducerQ35.py \n",
    "#!/usr/bin/env python\n",
    "\n",
    "from __future__ import division \n",
    "import sys \n",
    "from collections import defaultdict\n",
    "from itertools import combinations, chain \n",
    "import ast \n",
    "\n",
    "\n",
    "# Read input from mapper \n",
    "def readInput(file, separator='\\t'):\n",
    "\tfor line in file:\n",
    "\t\tyield line.split(separator)\t\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\ttokenDict = defaultdict(list)\n",
    "\tmergedDict = defaultdict(dict)\n",
    "\ttupleList = defaultdict(float)\n",
    "\tflattenedTerms = defaultdict(int)\n",
    "\tsupport = 100\n",
    "\n",
    "\n",
    "\t# Read data \n",
    "\tdata = readInput(sys.stdin)\n",
    "\tfor line in data: \n",
    "\n",
    "\t\ttoken = line[0]\n",
    "\t\tstripe = ast.literal_eval(line[1])\n",
    "\t\t\t\n",
    "\t\t# Combine dictionaries \n",
    "\t\ttokenDict[token].append(stripe) \n",
    "\n",
    "\n",
    "\t# Merge stripes \n",
    "\tfor k, v in tokenDict.iteritems(): \n",
    "\t\tmerged = defaultdict(int)\n",
    "\t\t\n",
    "\t\t# Loop and aggregate \n",
    "\t\tfor stripe in v: \n",
    "\t\t\tfor stripeKey in stripe: \n",
    "\t\t\t\tmerged[stripeKey] += stripe[stripeKey]\n",
    "\n",
    "\t\tmergedDict[k] = merged\n",
    "\n",
    "\n",
    "\t# Create key-value pairs \n",
    "\tfor token, stripe in mergedDict.iteritems(): \n",
    "\t\tfor innerToken, count in stripe.iteritems(): \n",
    "\n",
    "\t\t\t# Get unique keys \n",
    "\t\t\ttokenPair = [token, innerToken]\n",
    "\t\t\ttokenPair.sort()\n",
    "\t\t\ttuplePair = tuple(tokenPair)\n",
    "\n",
    "\t\t\t# Overcounting exactly twice per pair\n",
    "\t\t\ttupleList[tuplePair] += count / 2\n",
    "\t\n",
    "\n",
    "\n",
    "\t# Find most frequent terms and filter\n",
    "\tmostFrequentTerms = [(k, int(v), support) for k, v in tupleList.iteritems() \n",
    "                         if int(v) >= support]\n",
    "    \n",
    "\tmostFrequentTerms = sorted(mostFrequentTerms, \n",
    "\t\t\t\t\t\t\t\tkey = lambda x: x[1], \n",
    "\t\t\t\t\t\t\t\treverse = True)\n",
    "\n",
    "\n",
    "\t# Get results \n",
    "\tprint '========== Most Frequent Terms ==========' + '\\n'\n",
    "\ttemplate = \"{0:30}|{1:20}|{2:20}\"\n",
    "\tprint template.format(\"PAIR\", \"SUPPORT COUNT\", \"SUPPORT\")\n",
    "\t\n",
    "\t# Print terms \n",
    "\tfor termPair in mostFrequentTerms[:50]: \n",
    "\t\tprint template.format(*termPair)\n",
    "\n",
    "\t# Update counter \n",
    "\tsys.stderr.write(\"reporter:counter:User-Defined,Number of Reducers,1\\n\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combiner\n",
    "\n",
    "Below is the combiner used optionally in the process. It aggregates in a similar way to the reduce and outputes partially aggregated stripes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing combinerQ35.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combinerQ35.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from __future__ import division \n",
    "import sys \n",
    "from collections import defaultdict \n",
    "import ast \n",
    "\n",
    "\n",
    "# Read input from mapper \n",
    "def readInput(file, separator='\\t'):\n",
    "\tfor line in file:\n",
    "\t\tyield line.split(separator)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\ttokenDict = defaultdict(list)\n",
    "\tmergedDict = defaultdict(dict)\n",
    "\n",
    "\n",
    "\t# Read data \n",
    "\tdata = readInput(sys.stdin)\n",
    "\tfor line in data: \n",
    "\n",
    "\t\ttoken = line[0]\n",
    "\t\tstripe = ast.literal_eval(line[1])\n",
    "\t\t\t\n",
    "\t\t# Combine dictionaries \n",
    "\t\ttokenDict[token].append(stripe) \n",
    "\n",
    "\n",
    "\t# # Merge stripes \n",
    "\tfor k, v in tokenDict.iteritems(): \n",
    "\t\tmerged = defaultdict(int)\n",
    "\t\t\n",
    "\t\t# Loop and aggregate \n",
    "\t\tfor stripe in v: \n",
    "\t\t\tfor stripeKey in stripe: \n",
    "\t\t\t\tmerged[stripeKey] += stripe[stripeKey]\n",
    "\n",
    "\t\tmergedDict[k] = dict(merged)\n",
    "\n",
    "\t# Emit results \n",
    "\tfor k, v in mergedDict.iteritems(): \n",
    "\t\tprint '%s%s%s' % (k, '\\t', str(v))\n",
    "\n",
    "\t# Update counter \n",
    "\tsys.stderr.write(\"reporter:counter:User-Defined,Number of Combiners,1\\n\")                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Submit Job \n",
    "\n",
    "Our wrapper remains unchanged from the prior implementation that has an argument input for the combiner. We re-use it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 19:52:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 19:52:47 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/john/notebook\n",
      "16/01/30 19:52:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 19:52:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 19:52:54 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/30 19:52:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/mapperQ35.py, /Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/reducerQ35.py, /Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/combinerQ35.py] [] /var/folders/0w/8hzv7rsj3qgdynsjlqy3gjsc0000gn/T/streamjob95192711038331829.jar tmpDir=null\n",
      "16/01/30 19:52:56 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/30 19:52:56 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/30 19:52:56 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/30 19:52:56 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/30 19:52:56 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/30 19:52:57 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1032608858_0001\n",
      "16/01/30 19:52:57 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/mapperQ35.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1454201577376/mapperQ35.py\n",
      "16/01/30 19:52:57 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/reducerQ35.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1454201577377/reducerQ35.py\n",
      "16/01/30 19:52:57 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/combinerQ35.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1454201577378/combinerQ35.py\n",
      "16/01/30 19:52:57 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/30 19:52:57 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/30 19:52:57 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/30 19:52:57 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 19:52:57 INFO mapreduce.Job: Running job: job_local1032608858_0001\n",
      "16/01/30 19:52:57 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/30 19:52:57 INFO mapred.LocalJobRunner: Starting task: attempt_local1032608858_0001_m_000000_0\n",
      "16/01/30 19:52:58 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 19:52:58 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 19:52:58 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 19:52:58 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/john/notebook/input/ProductPurchaseData.txt:0+3458517\n",
      "16/01/30 19:52:58 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/30 19:52:58 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/30 19:52:58 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/30 19:52:58 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/30 19:52:58 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/30 19:52:58 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/30 19:52:58 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/30 19:52:58 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./mapperQ35.py]\n",
      "16/01/30 19:52:58 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/30 19:52:58 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/30 19:52:58 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/30 19:52:58 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/30 19:52:58 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/30 19:52:58 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/30 19:52:58 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/30 19:52:58 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/30 19:52:58 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/30 19:52:58 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/30 19:52:58 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/30 19:52:58 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/30 19:52:58 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:52:58 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:52:58 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:52:58 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:52:58 INFO streaming.PipeMapRed: Records R/W=1216/1\n",
      "16/01/30 19:52:58 INFO mapreduce.Job: Job job_local1032608858_0001 running in uber mode : false\n",
      "16/01/30 19:52:58 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/30 19:53:04 INFO mapred.LocalJobRunner: Records R/W=1216/1 > map\n",
      "16/01/30 19:53:04 INFO streaming.PipeMapRed: R/W/S=10000/120238/0 in:2000=10000/5 [rec/s] out:24047=120238/5 [rec/s]\n",
      "16/01/30 19:53:04 INFO mapreduce.Job:  map 23% reduce 0%\n",
      "16/01/30 19:53:07 INFO mapred.LocalJobRunner: Records R/W=1216/1 > map\n",
      "16/01/30 19:53:07 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "16/01/30 19:53:08 INFO streaming.PipeMapRed: Records R/W=24268/288602\n",
      "16/01/30 19:53:10 INFO mapred.LocalJobRunner: Records R/W=24268/288602 > map\n",
      "16/01/30 19:53:10 INFO mapreduce.Job:  map 61% reduce 0%\n",
      "16/01/30 19:53:11 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 19:53:11 INFO mapred.MapTask: bufstart = 0; bufend = 77965406; bufvoid = 104857600\n",
      "16/01/30 19:53:11 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24734212(98936848); length = 1480185/6553600\n",
      "16/01/30 19:53:11 INFO mapred.MapTask: (EQUATOR) 79450094 kvi 19862516(79450064)\n",
      "16/01/30 19:53:12 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 19:53:12 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 19:53:12 INFO mapred.LocalJobRunner: Records R/W=24268/288602 > map\n",
      "16/01/30 19:53:12 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/30 19:53:12 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./combinerQ35.py]\n",
      "16/01/30 19:53:12 INFO Configuration.deprecation: mapred.skip.map.auto.incr.proc.count is deprecated. Instead, use mapreduce.map.skip.proc-count.auto-incr\n",
      "16/01/30 19:53:12 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:53:12 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:53:12 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:53:12 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:53:13 INFO mapred.LocalJobRunner: Records R/W=24268/288602 > sort\n",
      "16/01/30 19:53:13 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/01/30 19:53:14 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:10000=10000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 19:53:16 INFO mapred.LocalJobRunner: Records R/W=24268/288602 > sort\n",
      "16/01/30 19:53:19 INFO mapred.LocalJobRunner: Records R/W=24268/288602 > sort\n",
      "16/01/30 19:53:22 INFO mapred.LocalJobRunner: Records R/W=24268/288602 > sort\n",
      "16/01/30 19:53:25 INFO mapred.LocalJobRunner: Records R/W=24268/288602 > sort\n",
      "16/01/30 19:53:28 INFO mapred.LocalJobRunner: Records R/W=24268/288602 > sort\n",
      "16/01/30 19:53:31 INFO mapred.LocalJobRunner: Records R/W=24268/288602 > sort\n",
      "16/01/30 19:53:31 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:5555=100000/18 [rec/s] out:0=0/18 [rec/s]\n",
      "16/01/30 19:53:34 INFO mapred.LocalJobRunner: Records R/W=24268/288602 > sort\n",
      "16/01/30 19:53:37 INFO mapred.LocalJobRunner: Records R/W=24268/288602 > sort\n",
      "16/01/30 19:53:40 INFO mapred.LocalJobRunner: Records R/W=24268/288602 > sort\n",
      "16/01/30 19:53:43 INFO mapred.LocalJobRunner: Records R/W=24268/288602 > sort\n",
      "16/01/30 19:53:46 INFO mapred.LocalJobRunner: Records R/W=24268/288602 > sort\n",
      "16/01/30 19:53:49 INFO mapred.LocalJobRunner: Records R/W=24268/288602 > sort\n",
      "16/01/30 19:53:49 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:5405=200000/37 [rec/s] out:0=0/37 [rec/s]\n",
      "16/01/30 19:53:52 INFO mapred.LocalJobRunner: Records R/W=24268/288602 > sort\n",
      "16/01/30 19:53:55 INFO mapred.LocalJobRunner: Records R/W=24268/288602 > sort\n",
      "16/01/30 19:53:58 INFO mapred.LocalJobRunner: Records R/W=24268/288602 > sort\n",
      "16/01/30 19:54:01 INFO mapred.LocalJobRunner: Records R/W=24268/288602 > sort\n",
      "16/01/30 19:54:04 INFO mapred.LocalJobRunner: Records R/W=24268/288602 > sort\n",
      "16/01/30 19:54:04 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:5882=300000/51 [rec/s] out:0=0/51 [rec/s]\n",
      "16/01/30 19:54:07 INFO mapred.LocalJobRunner: Records R/W=24268/288602 > sort\n",
      "16/01/30 19:54:10 INFO mapred.LocalJobRunner: Records R/W=24268/288602 > sort\n",
      "16/01/30 19:54:13 INFO mapred.LocalJobRunner: Records R/W=24268/288602 > sort\n",
      "16/01/30 19:54:16 INFO mapred.LocalJobRunner: Records R/W=24268/288602 > sort\n",
      "16/01/30 19:54:19 INFO mapred.LocalJobRunner: Records R/W=24268/288602 > sort\n",
      "16/01/30 19:54:21 INFO streaming.PipeMapRed: Records R/W=370047/1\n",
      "16/01/30 19:54:23 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 19:54:23 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 19:54:23 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/30 19:54:23 INFO mapred.MapTask: (RESET) equator 79450094 kv 19862516(79450064) kvi 19819408(79277632)\n",
      "16/01/30 19:54:23 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 19:54:23 INFO mapred.MapTask: bufstart = 79450094; bufend = 81699119; bufvoid = 104857600\n",
      "16/01/30 19:54:23 INFO mapred.MapTask: kvstart = 19862516(79450064); kvend = 19819412(79277648); length = 43105/6553600\n",
      "16/01/30 19:54:23 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./combinerQ35.py]\n",
      "16/01/30 19:54:23 INFO Configuration.deprecation: mapred.skip.reduce.auto.incr.proc.count is deprecated. Instead, use mapreduce.reduce.skip.proc-count.auto-incr\n",
      "16/01/30 19:54:23 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:54:23 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:54:23 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:54:23 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:54:25 INFO mapred.LocalJobRunner: Records R/W=370047/1 > sort\n",
      "16/01/30 19:54:25 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:10000=10000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 19:54:25 INFO streaming.PipeMapRed: Records R/W=10777/1\n",
      "16/01/30 19:54:25 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 19:54:25 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 19:54:25 INFO mapred.MapTask: Finished spill 1\n",
      "16/01/30 19:54:25 INFO mapred.Merger: Merging 2 sorted segments\n",
      "16/01/30 19:54:25 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 27515013 bytes\n",
      "16/01/30 19:54:26 INFO mapred.Task: Task:attempt_local1032608858_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/30 19:54:26 INFO mapred.LocalJobRunner: Records R/W=10777/1 > sort\n",
      "16/01/30 19:54:26 INFO mapred.Task: Task 'attempt_local1032608858_0001_m_000000_0' done.\n",
      "16/01/30 19:54:26 INFO mapred.LocalJobRunner: Finishing task: attempt_local1032608858_0001_m_000000_0\n",
      "16/01/30 19:54:26 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/30 19:54:26 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/30 19:54:26 INFO mapred.LocalJobRunner: Starting task: attempt_local1032608858_0001_r_000000_0\n",
      "16/01/30 19:54:26 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 19:54:26 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 19:54:26 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 19:54:26 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@72dfa023\n",
      "16/01/30 19:54:26 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/30 19:54:26 INFO reduce.EventFetcher: attempt_local1032608858_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/30 19:54:26 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1032608858_0001_m_000000_0 decomp: 27515028 len: 27515032 to MEMORY\n",
      "16/01/30 19:54:26 INFO reduce.InMemoryMapOutput: Read 27515028 bytes from map-output for attempt_local1032608858_0001_m_000000_0\n",
      "16/01/30 19:54:26 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 27515028, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->27515028\n",
      "16/01/30 19:54:26 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/30 19:54:26 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 19:54:26 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/30 19:54:26 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/30 19:54:26 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 27515015 bytes\n",
      "16/01/30 19:54:27 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/30 19:54:27 INFO reduce.MergeManagerImpl: Merged 1 segments, 27515028 bytes to disk to satisfy reduce memory limit\n",
      "16/01/30 19:54:27 INFO reduce.MergeManagerImpl: Merging 1 files, 27515032 bytes from disk\n",
      "16/01/30 19:54:27 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/30 19:54:27 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/30 19:54:27 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 27515015 bytes\n",
      "16/01/30 19:54:27 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 19:54:27 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./reducerQ35.py]\n",
      "16/01/30 19:54:27 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/30 19:54:27 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/30 19:54:27 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:54:27 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:54:27 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 19:54:28 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:1000=1000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 19:54:32 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 19:54:33 INFO mapreduce.Job:  map 100% reduce 74%\n",
      "16/01/30 19:54:35 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 19:54:36 INFO mapreduce.Job:  map 100% reduce 78%\n",
      "16/01/30 19:54:38 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 19:54:39 INFO mapreduce.Job:  map 100% reduce 83%\n",
      "16/01/30 19:54:41 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:769=10000/13 [rec/s] out:0=0/13 [rec/s]\n",
      "16/01/30 19:54:41 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 19:54:42 INFO mapreduce.Job:  map 100% reduce 89%\n",
      "16/01/30 19:54:44 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 19:54:45 INFO mapreduce.Job:  map 100% reduce 95%\n",
      "16/01/30 19:54:47 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 19:54:48 INFO mapreduce.Job:  map 100% reduce 99%\n",
      "16/01/30 19:54:50 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 19:54:51 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/30 19:54:53 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 19:54:59 INFO streaming.PipeMapRed: Records R/W=14617/1\n",
      "16/01/30 19:54:59 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 19:54:59 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 19:54:59 INFO mapred.Task: Task:attempt_local1032608858_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/30 19:54:59 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 19:54:59 INFO mapred.Task: Task attempt_local1032608858_0001_r_000000_0 is allowed to commit now\n",
      "16/01/30 19:55:00 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1032608858_0001_r_000000_0' to hdfs://localhost:9000/user/john/notebook/output/_temporary/0/task_local1032608858_0001_r_000000\n",
      "16/01/30 19:55:00 INFO mapred.LocalJobRunner: Records R/W=14617/1 > reduce\n",
      "16/01/30 19:55:00 INFO mapred.Task: Task 'attempt_local1032608858_0001_r_000000_0' done.\n",
      "16/01/30 19:55:00 INFO mapred.LocalJobRunner: Finishing task: attempt_local1032608858_0001_r_000000_0\n",
      "16/01/30 19:55:00 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/30 19:55:00 INFO mapreduce.Job: Job job_local1032608858_0001 completed successfully\n",
      "16/01/30 19:55:00 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=110071742\n",
      "\t\tFILE: Number of bytes written=138177260\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6917034\n",
      "\t\tHDFS: Number of bytes written=3819\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=380824\n",
      "\t\tMap output bytes=80214431\n",
      "\t\tMap output materialized bytes=27515032\n",
      "\t\tInput split bytes=122\n",
      "\t\tCombine input records=380824\n",
      "\t\tCombine output records=14617\n",
      "\t\tReduce input groups=14615\n",
      "\t\tReduce shuffle bytes=27515032\n",
      "\t\tReduce input records=14617\n",
      "\t\tReduce output records=53\n",
      "\t\tSpilled Records=43851\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=26\n",
      "\t\tTotal committed heap usage (bytes)=536346624\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tUser-Defined\n",
      "\t\tNumber of Combiners=2\n",
      "\t\tNumber of Mappers=1\n",
      "\t\tNumber of Reducers=1\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3819\n",
      "16/01/30 19:55:00 INFO streaming.StreamJob: Output directory: /user/john/notebook/output\n",
      "16/01/30 19:55:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "========== Most Frequent Terms ==========\t\n",
      "\t\n",
      "PAIR                          |SUPPORT COUNT       |SUPPORT             \t\n",
      "('DAI62779', 'ELE17451')      |                1592|                 100\t\n",
      "('FRO40251', 'SNA80324')      |                1412|                 100\t\n",
      "('DAI75645', 'FRO40251')      |                1254|                 100\t\n",
      "('FRO40251', 'GRO85051')      |                1213|                 100\t\n",
      "('DAI62779', 'GRO73461')      |                1139|                 100\t\n",
      "('DAI75645', 'SNA80324')      |                1130|                 100\t\n",
      "('DAI62779', 'FRO40251')      |                1070|                 100\t\n",
      "('DAI62779', 'SNA80324')      |                 923|                 100\t\n",
      "('DAI62779', 'DAI85309')      |                 918|                 100\t\n",
      "('ELE32164', 'GRO59710')      |                 911|                 100\t\n",
      "('FRO40251', 'GRO73461')      |                 882|                 100\t\n",
      "('DAI62779', 'DAI75645')      |                 882|                 100\t\n",
      "('DAI62779', 'ELE92920')      |                 877|                 100\t\n",
      "('FRO40251', 'FRO92469')      |                 835|                 100\t\n",
      "('DAI62779', 'ELE32164')      |                 832|                 100\t\n",
      "('DAI75645', 'GRO73461')      |                 712|                 100\t\n",
      "('DAI43223', 'ELE32164')      |                 711|                 100\t\n",
      "('DAI62779', 'GRO30386')      |                 709|                 100\t\n",
      "('ELE17451', 'FRO40251')      |                 697|                 100\t\n",
      "('DAI85309', 'ELE99737')      |                 659|                 100\t\n",
      "('DAI62779', 'ELE26917')      |                 650|                 100\t\n",
      "('GRO21487', 'GRO73461')      |                 631|                 100\t\n",
      "('DAI62779', 'SNA45677')      |                 604|                 100\t\n",
      "('ELE17451', 'SNA80324')      |                 597|                 100\t\n",
      "('DAI62779', 'GRO71621')      |                 595|                 100\t\n",
      "('DAI62779', 'SNA55762')      |                 593|                 100\t\n",
      "('DAI62779', 'DAI83733')      |                 586|                 100\t\n",
      "('ELE17451', 'GRO73461')      |                 580|                 100\t\n",
      "('GRO73461', 'SNA80324')      |                 562|                 100\t\n",
      "('DAI62779', 'GRO59710')      |                 561|                 100\t\n",
      "('DAI62779', 'FRO80039')      |                 550|                 100\t\n",
      "('DAI75645', 'ELE17451')      |                 547|                 100\t\n",
      "('DAI62779', 'SNA93860')      |                 537|                 100\t\n",
      "('DAI55148', 'DAI62779')      |                 526|                 100\t\n",
      "('DAI43223', 'GRO59710')      |                 512|                 100\t\n",
      "('ELE17451', 'ELE32164')      |                 511|                 100\t\n",
      "('DAI62779', 'SNA18336')      |                 506|                 100\t\n",
      "('ELE32164', 'GRO73461')      |                 486|                 100\t\n",
      "('DAI85309', 'ELE17451')      |                 482|                 100\t\n",
      "('DAI62779', 'FRO78087')      |                 482|                 100\t\n",
      "('DAI62779', 'GRO94758')      |                 479|                 100\t\n",
      "('DAI62779', 'GRO21487')      |                 471|                 100\t\n",
      "('GRO85051', 'SNA80324')      |                 471|                 100\t\n",
      "('ELE17451', 'GRO30386')      |                 468|                 100\t\n",
      "('FRO85978', 'SNA95666')      |                 463|                 100\t\n",
      "('DAI62779', 'FRO19221')      |                 462|                 100\t\n",
      "('DAI62779', 'GRO46854')      |                 461|                 100\t\n",
      "('DAI43223', 'DAI62779')      |                 459|                 100\t\n",
      "('ELE92920', 'SNA18336')      |                 455|                 100\t\n",
      "('DAI88079', 'FRO40251')      |                 446|                 100\t\n"
     ]
    }
   ],
   "source": [
    "!bash wrapperQ34.sh ProductPurchaseData.txt mapperQ35.py reducerQ35.py combinerQ35.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job was run on a Macbook Pro with a 2.66GHz Intel dual-core processor and 4GB of memory. \n",
    "\n",
    "Based on the results from Question 3.4 and Question 3.5, the two jobs use the same number of combiners, mappers, and reducers to accomplish their respective tasks. These are 2 combiners, 1 mapper, and 1 reducer for both jobs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Question 3.6 (Optional)\n",
    "### Part B\n",
    "\n",
    "*What is the Apriori algorithm? Describe an example use in your domain of expertise.*\n",
    "\n",
    "#### Solution: \n",
    "\n",
    "The Apriori algorithm is an algorithm for associative rule learning over transactional data sets. In particular, it implements a multi-scan approach with different tuning parameters like minimum support and minimum confidence. These are used to improve efficiency and control performance. In particular, the Apriori algorithm will identify frequent terms from the given support, and recurisvely compute larger baskets and search for additional frequent items. The Apriori algorithm stops when no further baskets can be found that are frequent. \n",
    "\n",
    "Within the domain of telecommunications, Apriori-like algorithms are used to describe the customer journey from signing-up to porting out. Identifying patterns that lead to churn is very important for Big Telecom, as the market continues to grow more volatile and competitive. Looking at information like clicks, calls to customer support, or billing events and building associative rules is important in finding business insights that improve customer retention. \n",
    "\n",
    "\n",
    "### Part B\n",
    "*Define confidence and lift*\n",
    "\n",
    "#### Solution: \n",
    "\n",
    "Given elements A and B, the confidence of A => B is equal to the support of A and B divided by the support of A. In words, confidence represents the likelihood of observing a basket containing B given a basket containing A. \n",
    "\n",
    "Lift is defined as a measure of performance of a model at predicting enhanced responses against a random choice targetting. In other words, Lift describes how well a model performs in identifying association rules that are more confident than selecting association rules at random. Given elements A and B, the lift of A => B is defined as the confidence of A and B given the average confidence across the entire basket list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Question 3.7 (Optional)\n",
    "*Write a program using the A-priori algorithm\n",
    "to find products which are frequently browsed together. Fix the support to s = 100 \n",
    "(i.e. product pairs need to occur together at least 100 times to be considered frequent) \n",
    "and find itemsets of size 2 and 3.*\n",
    "\n",
    "#### Solution: \n",
    "\n",
    "##### Mapper\n",
    "\n",
    "We use the pairs implementation here as it requires minimal code changes, albeit being less efficient. To this end, the mapper now outputs both 2 and 3-tuples which are treated the same by the reducer. The reducer changes its printing functionality to this end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapperQ37.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapperQ37.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from __future__ import division \n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "def readInput(file, separator=None):\n",
    "\tfor line in file:\n",
    "\t\tyield line.split(separator)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\t# Read input \n",
    "\tdata = readInput(sys.stdin)\n",
    "\tfor line in data: \n",
    "\n",
    "\t\t# Get unique keys \n",
    "\t\tdoublePairs = list(combinations(line, 2))\n",
    "\t\ttriplePairs = list(combinations(line, 3))\n",
    "\t\t\n",
    "\n",
    "\t\t# Sort doubles \n",
    "\t\tsortedDoubles = []\n",
    "\t\tfor pair in doublePairs: \n",
    "\t\t\tpList = list(pair)\n",
    "\t\t\tpList.sort()\n",
    "\t\t\tsortedDoubles.append(tuple(pList))\n",
    "\n",
    "\t\t# Emit\n",
    "\t\tfor pair in sortedDoubles: \n",
    "\t\t\tprint '%s%s%s' % (pair, '\\t', 1)\n",
    "\n",
    "\n",
    "\t\t# Sort triples \n",
    "\t\tsortedTriples = []\n",
    "\t\tfor pair in triplePairs: \n",
    "\t\t\tpList = list(pair)\n",
    "\t\t\tpList.sort()\n",
    "\t\t\tsortedTriples.append(tuple(pList))\n",
    "\n",
    "\t\t# Emit\n",
    "\t\tfor pair in sortedTriples: \n",
    "\t\t\tprint '%s%s%s' % (pair, '\\t', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reducer\n",
    "\n",
    "We modify the reducer so it outputs the top 50 of both the length-2 and length-3 tuples. This is mainly a cosmetic change in the printing functionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducerQ37.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducerQ37.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from __future__ import division \n",
    "import sys \n",
    "from collections import defaultdict\n",
    "import ast \n",
    "\n",
    "\n",
    "def readInput(file, separator='\\t'):\n",
    "\tfor line in file:\n",
    "\t\tyield line.split(separator)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\t# Final store \n",
    "\tstoringDict = defaultdict(int)\n",
    "\tsupport = 100\n",
    "\n",
    "\n",
    "\t# Read data \n",
    "\tdata = readInput(sys.stdin)\n",
    "\tfor line in data: \n",
    "\n",
    "\t\t# Parse value \n",
    "\t\ttoken = line[0]\n",
    "\t\ttermCount = int(line[1])\n",
    "\n",
    "\n",
    "\t\t# Store results \n",
    "\t\tstoringDict[token] += termCount\n",
    "\n",
    "\t\n",
    "\t# Filter\n",
    "\tfilterDict = defaultdict(int)\n",
    "\tfor k, v in storingDict.iteritems(): \n",
    "\t\tif v >= support:\n",
    "\t\t\tfilterDict[k] += v \n",
    "\n",
    "\n",
    "\t# Find most frequent doubles \n",
    "\tmfqDoubles = [(k, v, support) for k, v in filterDict.iteritems()\n",
    "\t\t\t\t\tif k.count(',') == 1]\n",
    "\n",
    "\tmfqDoubles = sorted(mfqDoubles, \n",
    "\t\t\t\t\t\t\t\tkey = lambda x: x[1], \n",
    "\t\t\t\t\t\t\t\treverse = True)\n",
    "\n",
    "\n",
    "\t# Get results \n",
    "\tprint '\\n' + '========== Most Frequent Doubles ==========' + '\\n'\n",
    "\ttemplate = \"{0:30}|{1:20}|{2:20}\"\n",
    "\tprint template.format(\"PAIR\", \"SUPPORT COUNT\", \"SUPPORT\")\n",
    "\t\n",
    "\t# Print terms \n",
    "\tfor termPair in mfqDoubles[:50]: \n",
    "\t\tprint template.format(*termPair)\n",
    "\n",
    "\n",
    "\n",
    "\t# Find most frequent triples \n",
    "\tmfqTriples = [(k, v, support) for k, v in filterDict.iteritems()\n",
    "\t\t\t\t\tif k.count(',') == 2]\n",
    "\t\t\t\t\t\n",
    "\tmfqTriples = sorted(mfqTriples, \n",
    "\t\t\t\t\t\t\t\tkey = lambda x: x[1], \n",
    "\t\t\t\t\t\t\t\treverse = True)\n",
    "\n",
    "\n",
    "\t# Get results \n",
    "\tprint '\\n' + '========== Most Frequent Triples ==========' + '\\n'\n",
    "\ttemplate = \"{0:50}|{1:20}|{2:20}\"\n",
    "\tprint template.format(\"PAIR\", \"SUPPORT COUNT\", \"SUPPORT\")\n",
    "\t\n",
    "\t# Print terms \n",
    "\tfor termPair in mfqTriples[:50]: \n",
    "\t\tprint template.format(*termPair)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Submit Job \n",
    "\n",
    "We now submit the job using the combiner and wrapper defined in Question 3.4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 20:39:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 20:39:33 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/john/notebook\n",
      "16/01/30 20:39:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 20:39:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 20:39:42 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/30 20:39:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/mapperQ37.py, /Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/reducerQ37.py, /Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/combinerQ34.py] [] /var/folders/0w/8hzv7rsj3qgdynsjlqy3gjsc0000gn/T/streamjob5966942647828294044.jar tmpDir=null\n",
      "16/01/30 20:39:44 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/30 20:39:44 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/30 20:39:44 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/30 20:39:45 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/30 20:39:45 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/30 20:39:45 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local583153488_0001\n",
      "16/01/30 20:39:45 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/mapperQ37.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1454204385598/mapperQ37.py\n",
      "16/01/30 20:39:46 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/reducerQ37.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1454204385599/reducerQ37.py\n",
      "16/01/30 20:39:46 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/combinerQ34.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1454204385600/combinerQ34.py\n",
      "16/01/30 20:39:46 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/30 20:39:46 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/30 20:39:46 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/30 20:39:46 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 20:39:46 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/30 20:39:46 INFO mapred.LocalJobRunner: Starting task: attempt_local583153488_0001_m_000000_0\n",
      "16/01/30 20:39:46 INFO mapreduce.Job: Running job: job_local583153488_0001\n",
      "16/01/30 20:39:46 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 20:39:46 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 20:39:46 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 20:39:46 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/john/notebook/input/ProductPurchaseData.txt:0+3458517\n",
      "16/01/30 20:39:46 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/30 20:39:46 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/30 20:39:46 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/30 20:39:46 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/30 20:39:46 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/30 20:39:46 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/30 20:39:46 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/30 20:39:46 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./mapperQ37.py]\n",
      "16/01/30 20:39:46 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/30 20:39:46 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/30 20:39:46 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/30 20:39:46 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/30 20:39:46 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/30 20:39:46 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/30 20:39:46 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/30 20:39:46 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/30 20:39:46 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/30 20:39:46 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/30 20:39:46 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/30 20:39:46 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/30 20:39:46 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:39:46 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:39:46 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:39:46 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:39:46 INFO streaming.PipeMapRed: Records R/W=1216/1\n",
      "16/01/30 20:39:47 INFO mapreduce.Job: Job job_local583153488_0001 running in uber mode : false\n",
      "16/01/30 20:39:47 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/30 20:39:52 INFO mapred.LocalJobRunner: Records R/W=1216/1 > map\n",
      "16/01/30 20:39:53 INFO mapreduce.Job:  map 8% reduce 0%\n",
      "16/01/30 20:39:54 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 20:39:54 INFO mapred.MapTask: bufstart = 0; bufend = 58481682; bufvoid = 104857600\n",
      "16/01/30 20:39:54 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 19863304(79453216); length = 6351093/6553600\n",
      "16/01/30 20:39:54 INFO mapred.MapTask: (EQUATOR) 64934450 kvi 16233608(64934432)\n",
      "16/01/30 20:39:55 INFO mapred.LocalJobRunner: Records R/W=1216/1 > map\n",
      "16/01/30 20:39:56 INFO mapreduce.Job:  map 10% reduce 0%\n",
      "16/01/30 20:39:56 INFO streaming.PipeMapRed: Records R/W=5743/1928237\n",
      "16/01/30 20:39:58 INFO mapred.LocalJobRunner: Records R/W=5743/1928237 > map\n",
      "16/01/30 20:39:59 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "16/01/30 20:39:59 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./combinerQ34.py]\n",
      "16/01/30 20:39:59 INFO Configuration.deprecation: mapred.skip.map.auto.incr.proc.count is deprecated. Instead, use mapreduce.map.skip.proc-count.auto-incr\n",
      "16/01/30 20:39:59 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:39:59 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:39:59 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:39:59 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:39:59 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:00 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:00 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:00 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:40:01 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:40:01 INFO mapred.LocalJobRunner: Records R/W=5743/1928237 > map\n",
      "16/01/30 20:40:01 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:500000=500000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:40:01 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:300000=600000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:40:02 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:350000=700000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:40:02 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:400000=800000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:40:02 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:300000=900000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:40:03 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:333333=1000000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:40:03 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:366666=1100000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:40:03 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:300000=1200000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:40:04 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:325000=1300000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:40:04 INFO mapred.LocalJobRunner: Records R/W=5743/1928237 > map\n",
      "16/01/30 20:40:04 INFO streaming.PipeMapRed: R/W/S=1400000/0/0 in:350000=1400000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:40:04 INFO streaming.PipeMapRed: R/W/S=1500000/0/0 in:300000=1500000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:40:05 INFO streaming.PipeMapRed: Records R/W=1587774/1\n",
      "16/01/30 20:40:07 INFO mapred.LocalJobRunner: Records R/W=1587774/1 > map\n",
      "16/01/30 20:40:07 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 20:40:07 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 20:40:07 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/30 20:40:07 INFO mapred.MapTask: (RESET) equator 64934450 kv 16233608(64934432) kvi 14659840(58639360)\n",
      "16/01/30 20:40:07 INFO streaming.PipeMapRed: Records R/W=5743/1981217\n",
      "16/01/30 20:40:10 INFO mapred.LocalJobRunner: Records R/W=5743/1981217 > map\n",
      "16/01/30 20:40:11 INFO mapreduce.Job:  map 15% reduce 0%\n",
      "16/01/30 20:40:13 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 20:40:13 INFO mapred.MapTask: bufstart = 64934450; bufend = 18648555; bufvoid = 104857577\n",
      "16/01/30 20:40:13 INFO mapred.MapTask: kvstart = 16233608(64934432); kvend = 9905012(39620048); length = 6328597/6553600\n",
      "16/01/30 20:40:13 INFO mapred.MapTask: (EQUATOR) 25101307 kvi 6275320(25101280)\n",
      "16/01/30 20:40:13 INFO mapred.LocalJobRunner: Records R/W=5743/1981217 > map\n",
      "16/01/30 20:40:13 INFO mapreduce.Job:  map 18% reduce 0%\n",
      "16/01/30 20:40:16 INFO mapred.LocalJobRunner: Records R/W=5743/1981217 > map\n",
      "16/01/30 20:40:17 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./combinerQ34.py]\n",
      "16/01/30 20:40:17 INFO Configuration.deprecation: mapred.skip.reduce.auto.incr.proc.count is deprecated. Instead, use mapreduce.reduce.skip.proc-count.auto-incr\n",
      "16/01/30 20:40:17 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:17 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:17 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:17 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:17 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:17 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:18 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:18 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:40:18 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:40:19 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:500000=500000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:40:19 INFO mapred.LocalJobRunner: Records R/W=5743/1981217 > map\n",
      "16/01/30 20:40:19 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:300000=600000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:40:19 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:350000=700000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:40:20 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:400000=800000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:40:20 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:300000=900000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:40:20 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:333333=1000000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:40:21 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:275000=1100000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:40:21 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:300000=1200000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:40:22 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:325000=1300000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:40:22 INFO mapred.LocalJobRunner: Records R/W=5743/1981217 > map\n",
      "16/01/30 20:40:22 INFO streaming.PipeMapRed: R/W/S=1400000/0/0 in:280000=1400000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:40:23 INFO streaming.PipeMapRed: R/W/S=1500000/0/0 in:300000=1500000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:40:23 INFO streaming.PipeMapRed: Records R/W=1582150/1\n",
      "16/01/30 20:40:25 INFO mapred.LocalJobRunner: Records R/W=1582150/1 > map\n",
      "16/01/30 20:40:26 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 20:40:26 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 20:40:26 INFO mapred.MapTask: Finished spill 1\n",
      "16/01/30 20:40:26 INFO mapred.MapTask: (RESET) equator 25101307 kv 6275320(25101280) kvi 4710532(18842128)\n",
      "16/01/30 20:40:26 INFO streaming.PipeMapRed: Records R/W=7778/3561122\n",
      "16/01/30 20:40:28 INFO mapred.LocalJobRunner: Records R/W=7778/3561122 > map\n",
      "16/01/30 20:40:28 INFO mapreduce.Job:  map 20% reduce 0%\n",
      "16/01/30 20:40:31 INFO mapred.LocalJobRunner: Records R/W=7778/3561122 > map\n",
      "16/01/30 20:40:31 INFO mapreduce.Job:  map 23% reduce 0%\n",
      "16/01/30 20:40:32 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 20:40:32 INFO mapred.MapTask: bufstart = 25101307; bufend = 83736112; bufvoid = 104857600\n",
      "16/01/30 20:40:32 INFO mapred.MapTask: kvstart = 6275320(25101280); kvend = 26176912(104707648); length = 6312809/6553600\n",
      "16/01/30 20:40:32 INFO mapred.MapTask: (EQUATOR) 90067136 kvi 22516780(90067120)\n",
      "16/01/30 20:40:34 INFO mapred.LocalJobRunner: Records R/W=7778/3561122 > map\n",
      "16/01/30 20:40:37 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./combinerQ34.py]\n",
      "16/01/30 20:40:37 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:37 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:37 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:37 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:37 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:37 INFO mapred.LocalJobRunner: Records R/W=7778/3561122 > map\n",
      "16/01/30 20:40:37 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:38 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:38 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:40:38 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:40:39 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:500000=500000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:40:39 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:300000=600000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:40:39 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:350000=700000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:40:40 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:400000=800000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:40:40 INFO mapred.LocalJobRunner: Records R/W=7778/3561122 > map\n",
      "16/01/30 20:40:41 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:300000=900000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:40:42 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:200000=1000000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:40:42 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:220000=1100000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:40:43 INFO mapred.LocalJobRunner: Records R/W=7778/3561122 > map\n",
      "16/01/30 20:40:43 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:200000=1200000/6 [rec/s] out:0=0/6 [rec/s]\n",
      "16/01/30 20:40:43 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:216666=1300000/6 [rec/s] out:0=0/6 [rec/s]\n",
      "16/01/30 20:40:44 INFO streaming.PipeMapRed: R/W/S=1400000/0/0 in:200000=1400000/7 [rec/s] out:0=0/7 [rec/s]\n",
      "16/01/30 20:40:45 INFO streaming.PipeMapRed: R/W/S=1500000/0/0 in:214285=1500000/7 [rec/s] out:0=0/7 [rec/s]\n",
      "16/01/30 20:40:45 INFO streaming.PipeMapRed: Records R/W=1578203/1\n",
      "16/01/30 20:40:46 INFO mapred.LocalJobRunner: Records R/W=1578203/1 > map\n",
      "16/01/30 20:40:49 INFO mapred.LocalJobRunner: Records R/W=1578203/1 > map\n",
      "16/01/30 20:40:49 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 20:40:49 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 20:40:49 INFO mapred.MapTask: Finished spill 2\n",
      "16/01/30 20:40:49 INFO mapred.MapTask: (RESET) equator 90067136 kv 22516780(90067120) kvi 20941520(83766080)\n",
      "16/01/30 20:40:49 INFO streaming.PipeMapRed: Records R/W=9736/5141943\n",
      "16/01/30 20:40:49 INFO streaming.PipeMapRed: R/W/S=10000/5165981/0 in:158=10000/63 [rec/s] out:81999=5165981/63 [rec/s]\n",
      "16/01/30 20:40:52 INFO mapred.LocalJobRunner: Records R/W=9736/5141943 > map\n",
      "16/01/30 20:40:52 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "16/01/30 20:40:55 INFO mapred.LocalJobRunner: Records R/W=9736/5141943 > map\n",
      "16/01/30 20:40:55 INFO mapreduce.Job:  map 28% reduce 0%\n",
      "16/01/30 20:40:56 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 20:40:56 INFO mapred.MapTask: bufstart = 90067136; bufend = 43824763; bufvoid = 104857600\n",
      "16/01/30 20:40:56 INFO mapred.MapTask: kvstart = 22516780(90067120); kvend = 16199068(64796272); length = 6317713/6553600\n",
      "16/01/30 20:40:56 INFO mapred.MapTask: (EQUATOR) 50155771 kvi 12538936(50155744)\n",
      "16/01/30 20:40:58 INFO mapred.LocalJobRunner: Records R/W=9736/5141943 > map\n",
      "16/01/30 20:40:58 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "16/01/30 20:41:00 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./combinerQ34.py]\n",
      "16/01/30 20:41:00 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:00 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:00 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:00 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:00 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:01 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:01 INFO mapred.LocalJobRunner: Records R/W=9736/5141943 > map\n",
      "16/01/30 20:41:01 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:02 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:41:02 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:200000=400000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:41:03 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:250000=500000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:41:03 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:200000=600000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:41:04 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:233333=700000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:41:04 INFO mapred.LocalJobRunner: Records R/W=9736/5141943 > map\n",
      "16/01/30 20:41:04 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:266666=800000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:41:05 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:225000=900000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:41:05 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:250000=1000000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:41:05 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:220000=1100000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:41:06 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:240000=1200000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:41:06 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:260000=1300000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:41:07 INFO streaming.PipeMapRed: R/W/S=1400000/0/0 in:233333=1400000/6 [rec/s] out:0=0/6 [rec/s]\n",
      "16/01/30 20:41:07 INFO mapred.LocalJobRunner: Records R/W=9736/5141943 > map\n",
      "16/01/30 20:41:07 INFO streaming.PipeMapRed: R/W/S=1500000/0/0 in:250000=1500000/6 [rec/s] out:0=0/6 [rec/s]\n",
      "16/01/30 20:41:07 INFO streaming.PipeMapRed: Records R/W=1579429/1\n",
      "16/01/30 20:41:10 INFO mapred.LocalJobRunner: Records R/W=1579429/1 > map\n",
      "16/01/30 20:41:10 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 20:41:10 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 20:41:10 INFO mapred.MapTask: Finished spill 3\n",
      "16/01/30 20:41:10 INFO mapred.MapTask: (RESET) equator 50155771 kv 12538936(50155744) kvi 10963592(43854368)\n",
      "16/01/30 20:41:10 INFO streaming.PipeMapRed: Records R/W=13103/6721393\n",
      "16/01/30 20:41:13 INFO mapred.LocalJobRunner: Records R/W=13103/6721393 > map\n",
      "16/01/30 20:41:16 INFO mapred.LocalJobRunner: Records R/W=13103/6721393 > map\n",
      "16/01/30 20:41:16 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 20:41:16 INFO mapred.MapTask: bufstart = 50155771; bufend = 3874239; bufvoid = 104857588\n",
      "16/01/30 20:41:16 INFO mapred.MapTask: kvstart = 12538936(50155744); kvend = 6211436(24845744); length = 6327501/6553600\n",
      "16/01/30 20:41:16 INFO mapred.MapTask: (EQUATOR) 10205247 kvi 2551304(10205216)\n",
      "16/01/30 20:41:16 INFO mapreduce.Job:  map 35% reduce 0%\n",
      "16/01/30 20:41:19 INFO mapred.LocalJobRunner: Records R/W=13103/6721393 > map\n",
      "16/01/30 20:41:19 INFO mapreduce.Job:  map 40% reduce 0%\n",
      "16/01/30 20:41:20 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./combinerQ34.py]\n",
      "16/01/30 20:41:20 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:20 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:20 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:20 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:20 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:20 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:20 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:21 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:41:21 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:41:22 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:500000=500000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:41:22 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:300000=600000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:41:22 INFO mapred.LocalJobRunner: Records R/W=13103/6721393 > map\n",
      "16/01/30 20:41:22 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:350000=700000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:41:23 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:400000=800000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:41:23 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:300000=900000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:41:23 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:333333=1000000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:41:24 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:366666=1100000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:41:24 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:300000=1200000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:41:24 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:325000=1300000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:41:25 INFO streaming.PipeMapRed: R/W/S=1400000/0/0 in:350000=1400000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:41:25 INFO mapred.LocalJobRunner: Records R/W=13103/6721393 > map\n",
      "16/01/30 20:41:25 INFO streaming.PipeMapRed: R/W/S=1500000/0/0 in:300000=1500000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:41:25 INFO streaming.PipeMapRed: Records R/W=1581876/1\n",
      "16/01/30 20:41:28 INFO mapred.LocalJobRunner: Records R/W=1581876/1 > map\n",
      "16/01/30 20:41:28 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 20:41:28 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 20:41:28 INFO mapred.MapTask: Finished spill 4\n",
      "16/01/30 20:41:28 INFO mapred.MapTask: (RESET) equator 10205247 kv 2551304(10205216) kvi 968564(3874256)\n",
      "16/01/30 20:41:28 INFO streaming.PipeMapRed: Records R/W=18924/8305118\n",
      "16/01/30 20:41:31 INFO mapred.LocalJobRunner: Records R/W=18924/8305118 > map\n",
      "16/01/30 20:41:31 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "16/01/30 20:41:34 INFO mapred.LocalJobRunner: Records R/W=18924/8305118 > map\n",
      "16/01/30 20:41:34 INFO mapreduce.Job:  map 45% reduce 0%\n",
      "16/01/30 20:41:34 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 20:41:34 INFO mapred.MapTask: bufstart = 10205247; bufend = 68611041; bufvoid = 104857600\n",
      "16/01/30 20:41:34 INFO mapred.MapTask: kvstart = 2551304(10205216); kvend = 22395636(89582544); length = 6370069/6553600\n",
      "16/01/30 20:41:34 INFO mapred.MapTask: (EQUATOR) 75063793 kvi 18765944(75063776)\n",
      "16/01/30 20:41:37 INFO mapred.LocalJobRunner: Records R/W=18924/8305118 > map\n",
      "16/01/30 20:41:38 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./combinerQ34.py]\n",
      "16/01/30 20:41:38 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:38 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:38 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:38 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:38 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:38 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:39 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:39 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:41:39 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:41:40 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:500000=500000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:41:40 INFO mapred.LocalJobRunner: Records R/W=18924/8305118 > map\n",
      "16/01/30 20:41:40 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:300000=600000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:41:41 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:350000=700000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:41:41 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:400000=800000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:41:41 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:300000=900000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:41:42 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:333333=1000000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:41:42 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:366666=1100000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:41:42 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:300000=1200000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:41:43 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:325000=1300000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:41:43 INFO mapred.LocalJobRunner: Records R/W=18924/8305118 > map\n",
      "16/01/30 20:41:43 INFO streaming.PipeMapRed: R/W/S=1400000/0/0 in:350000=1400000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:41:43 INFO streaming.PipeMapRed: R/W/S=1500000/0/0 in:300000=1500000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:41:44 INFO streaming.PipeMapRed: Records R/W=1592518/1\n",
      "16/01/30 20:41:46 INFO mapred.LocalJobRunner: Records R/W=1592518/1 > map\n",
      "16/01/30 20:41:46 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 20:41:46 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 20:41:46 INFO mapred.MapTask: Finished spill 5\n",
      "16/01/30 20:41:46 INFO mapred.MapTask: (RESET) equator 75063793 kv 18765944(75063776) kvi 17202988(68811952)\n",
      "16/01/30 20:41:46 INFO streaming.PipeMapRed: Records R/W=20935/9892690\n",
      "16/01/30 20:41:49 INFO mapred.LocalJobRunner: Records R/W=20935/9892690 > map\n",
      "16/01/30 20:41:49 INFO mapreduce.Job:  map 48% reduce 0%\n",
      "16/01/30 20:41:51 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 20:41:51 INFO mapred.MapTask: bufstart = 75063793; bufend = 28812705; bufvoid = 104857567\n",
      "16/01/30 20:41:51 INFO mapred.MapTask: kvstart = 18765944(75063776); kvend = 12446056(49784224); length = 6319889/6553600\n",
      "16/01/30 20:41:51 INFO mapred.MapTask: (EQUATOR) 35265473 kvi 8816364(35265456)\n",
      "16/01/30 20:41:52 INFO mapred.LocalJobRunner: Records R/W=20935/9892690 > map\n",
      "16/01/30 20:41:52 INFO mapreduce.Job:  map 51% reduce 0%\n",
      "16/01/30 20:41:55 INFO mapred.LocalJobRunner: Records R/W=20935/9892690 > map\n",
      "16/01/30 20:41:55 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./combinerQ34.py]\n",
      "16/01/30 20:41:55 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:55 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:55 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:55 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:55 INFO mapreduce.Job:  map 53% reduce 0%\n",
      "16/01/30 20:41:55 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:56 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:56 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:57 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:41:57 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:200000=400000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:41:58 INFO mapred.LocalJobRunner: Records R/W=20935/9892690 > map\n",
      "16/01/30 20:41:58 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:250000=500000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:41:59 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:200000=600000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:42:00 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:175000=700000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:42:00 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:160000=800000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:42:01 INFO mapred.LocalJobRunner: Records R/W=20935/9892690 > map\n",
      "16/01/30 20:42:01 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:150000=900000/6 [rec/s] out:0=0/6 [rec/s]\n",
      "16/01/30 20:42:02 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:166666=1000000/6 [rec/s] out:0=0/6 [rec/s]\n",
      "16/01/30 20:42:02 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:157142=1100000/7 [rec/s] out:0=0/7 [rec/s]\n",
      "16/01/30 20:42:03 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:171428=1200000/7 [rec/s] out:0=0/7 [rec/s]\n",
      "16/01/30 20:42:03 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:185714=1300000/7 [rec/s] out:0=0/7 [rec/s]\n",
      "16/01/30 20:42:03 INFO streaming.PipeMapRed: R/W/S=1400000/0/0 in:175000=1400000/8 [rec/s] out:0=0/8 [rec/s]\n",
      "16/01/30 20:42:04 INFO streaming.PipeMapRed: R/W/S=1500000/0/0 in:187500=1500000/8 [rec/s] out:0=0/8 [rec/s]\n",
      "16/01/30 20:42:04 INFO mapred.LocalJobRunner: Records R/W=20935/9892690 > map\n",
      "16/01/30 20:42:04 INFO streaming.PipeMapRed: Records R/W=1579973/1\n",
      "16/01/30 20:42:07 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 20:42:07 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 20:42:07 INFO mapred.MapTask: Finished spill 6\n",
      "16/01/30 20:42:07 INFO mapred.MapTask: (RESET) equator 35265473 kv 8816364(35265456) kvi 7254956(29019824)\n",
      "16/01/30 20:42:07 INFO streaming.PipeMapRed: Records R/W=24268/11472276\n",
      "16/01/30 20:42:07 INFO mapred.LocalJobRunner: Records R/W=24268/11472276 > map\n",
      "16/01/30 20:42:10 INFO mapred.LocalJobRunner: Records R/W=24268/11472276 > map\n",
      "16/01/30 20:42:10 INFO mapreduce.Job:  map 56% reduce 0%\n",
      "16/01/30 20:42:11 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 20:42:11 INFO mapred.MapTask: bufstart = 35265473; bufend = 93773681; bufvoid = 104857600\n",
      "16/01/30 20:42:11 INFO mapred.MapTask: kvstart = 8816364(35265456); kvend = 2471904(9887616); length = 6344461/6553600\n",
      "16/01/30 20:42:11 INFO mapred.MapTask: (EQUATOR) 100226449 kvi 25056608(100226432)\n",
      "16/01/30 20:42:13 INFO mapred.LocalJobRunner: Records R/W=24268/11472276 > map\n",
      "16/01/30 20:42:13 INFO mapreduce.Job:  map 61% reduce 0%\n",
      "16/01/30 20:42:15 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./combinerQ34.py]\n",
      "16/01/30 20:42:15 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:15 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:15 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:15 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:15 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:16 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:16 INFO mapred.LocalJobRunner: Records R/W=24268/11472276 > map\n",
      "16/01/30 20:42:16 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:16 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:42:17 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:42:17 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:500000=500000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:42:17 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:300000=600000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:42:18 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:350000=700000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:42:18 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:400000=800000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:42:19 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:300000=900000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:42:19 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:333333=1000000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:42:19 INFO mapred.LocalJobRunner: Records R/W=24268/11472276 > map\n",
      "16/01/30 20:42:19 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:275000=1100000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:42:20 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:300000=1200000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:42:20 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:325000=1300000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:42:20 INFO streaming.PipeMapRed: R/W/S=1400000/0/0 in:280000=1400000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:42:21 INFO streaming.PipeMapRed: R/W/S=1500000/0/0 in:300000=1500000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:42:21 INFO streaming.PipeMapRed: Records R/W=1586116/1\n",
      "16/01/30 20:42:22 INFO mapred.LocalJobRunner: Records R/W=1586116/1 > map\n",
      "16/01/30 20:42:25 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 20:42:25 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 20:42:25 INFO mapred.MapTask: Finished spill 7\n",
      "16/01/30 20:42:25 INFO mapred.MapTask: (RESET) equator 100226449 kv 25056608(100226432) kvi 23468064(93872256)\n",
      "16/01/30 20:42:25 INFO streaming.PipeMapRed: Records R/W=28078/13065176\n",
      "16/01/30 20:42:25 INFO mapred.LocalJobRunner: Records R/W=28078/13065176 > map\n",
      "16/01/30 20:42:28 INFO mapred.LocalJobRunner: Records R/W=28078/13065176 > map\n",
      "16/01/30 20:42:28 INFO mapreduce.Job:  map 63% reduce 0%\n",
      "16/01/30 20:42:31 INFO mapred.LocalJobRunner: Records R/W=28078/13065176 > map\n",
      "16/01/30 20:42:31 INFO mapreduce.Job:  map 66% reduce 0%\n",
      "16/01/30 20:42:32 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 20:42:32 INFO mapred.MapTask: bufstart = 100226449; bufend = 53764836; bufvoid = 104857582\n",
      "16/01/30 20:42:32 INFO mapred.MapTask: kvstart = 25056608(100226432); kvend = 18684088(74736352); length = 6372521/6553600\n",
      "16/01/30 20:42:32 INFO mapred.MapTask: (EQUATOR) 60217604 kvi 15054396(60217584)\n",
      "16/01/30 20:42:34 INFO mapred.LocalJobRunner: Records R/W=28078/13065176 > map\n",
      "16/01/30 20:42:34 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/01/30 20:42:36 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./combinerQ34.py]\n",
      "16/01/30 20:42:36 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:36 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:36 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:36 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:36 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:36 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:37 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:37 INFO mapred.LocalJobRunner: Records R/W=28078/13065176 > map\n",
      "16/01/30 20:42:37 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:42:37 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:42:38 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:500000=500000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:42:38 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:300000=600000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:42:39 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:350000=700000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:42:39 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:400000=800000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:42:39 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:300000=900000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:42:40 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:333333=1000000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:42:40 INFO mapred.LocalJobRunner: Records R/W=28078/13065176 > map\n",
      "16/01/30 20:42:40 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:275000=1100000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:42:40 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:300000=1200000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:42:41 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:325000=1300000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:42:41 INFO streaming.PipeMapRed: R/W/S=1400000/0/0 in:280000=1400000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:42:41 INFO streaming.PipeMapRed: R/W/S=1500000/0/0 in:300000=1500000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:42:42 INFO streaming.PipeMapRed: Records R/W=1593131/1\n",
      "16/01/30 20:42:43 INFO mapred.LocalJobRunner: Records R/W=1593131/1 > map\n",
      "16/01/30 20:42:44 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 20:42:44 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 20:42:44 INFO mapred.MapTask: Finished spill 8\n",
      "16/01/30 20:42:44 INFO mapred.MapTask: (RESET) equator 60217604 kv 15054396(60217584) kvi 13476216(53904864)\n",
      "16/01/30 20:42:44 INFO streaming.PipeMapRed: Records R/W=31101/14655716\n",
      "16/01/30 20:42:45 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 20:42:45 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 20:42:45 INFO mapred.LocalJobRunner: Records R/W=1593131/1 > map\n",
      "16/01/30 20:42:45 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/30 20:42:45 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 20:42:45 INFO mapred.MapTask: bufstart = 60217604; bufend = 77432318; bufvoid = 104857600\n",
      "16/01/30 20:42:45 INFO mapred.MapTask: kvstart = 15054396(60217584); kvend = 13184280(52737120); length = 1870117/6553600\n",
      "16/01/30 20:42:46 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./combinerQ34.py]\n",
      "16/01/30 20:42:46 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:46 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:46 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:46 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:46 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:46 INFO mapred.LocalJobRunner: Records R/W=31101/14655716 > sort\n",
      "16/01/30 20:42:46 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:46 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:47 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:42:47 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:42:47 INFO streaming.PipeMapRed: Records R/W=467530/1\n",
      "16/01/30 20:42:48 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 20:42:48 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 20:42:48 INFO mapred.MapTask: Finished spill 9\n",
      "16/01/30 20:42:48 INFO mapred.Merger: Merging 10 sorted segments\n",
      "16/01/30 20:42:48 INFO mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 477989196 bytes\n",
      "16/01/30 20:42:48 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./combinerQ34.py]\n",
      "16/01/30 20:42:48 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:48 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:48 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:48 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:49 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:49 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:42:49 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:50 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:200000=200000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:42:50 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:150000=300000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:42:51 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:200000=400000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:42:51 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:166666=500000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:42:52 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:200000=600000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:42:52 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:42:52 INFO mapreduce.Job:  map 68% reduce 0%\n",
      "16/01/30 20:42:52 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:233333=700000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:42:54 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:160000=800000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:42:55 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:150000=900000/6 [rec/s] out:0=0/6 [rec/s]\n",
      "16/01/30 20:42:55 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:42:55 INFO mapreduce.Job:  map 69% reduce 0%\n",
      "16/01/30 20:42:56 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:142857=1000000/7 [rec/s] out:0=0/7 [rec/s]\n",
      "16/01/30 20:42:56 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:157142=1100000/7 [rec/s] out:0=0/7 [rec/s]\n",
      "16/01/30 20:42:57 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:150000=1200000/8 [rec/s] out:0=0/8 [rec/s]\n",
      "16/01/30 20:42:58 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:144444=1300000/9 [rec/s] out:0=0/9 [rec/s]\n",
      "16/01/30 20:42:58 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:42:58 INFO streaming.PipeMapRed: R/W/S=1400000/0/0 in:155555=1400000/9 [rec/s] out:0=0/9 [rec/s]\n",
      "16/01/30 20:42:58 INFO mapreduce.Job:  map 70% reduce 0%\n",
      "16/01/30 20:42:59 INFO streaming.PipeMapRed: R/W/S=1500000/0/0 in:150000=1500000/10 [rec/s] out:0=0/10 [rec/s]\n",
      "16/01/30 20:43:00 INFO streaming.PipeMapRed: R/W/S=1600000/0/0 in:145454=1600000/11 [rec/s] out:0=0/11 [rec/s]\n",
      "16/01/30 20:43:00 INFO streaming.PipeMapRed: R/W/S=1700000/0/0 in:154545=1700000/11 [rec/s] out:0=0/11 [rec/s]\n",
      "16/01/30 20:43:01 INFO streaming.PipeMapRed: R/W/S=1800000/0/0 in:150000=1800000/12 [rec/s] out:0=0/12 [rec/s]\n",
      "16/01/30 20:43:01 INFO streaming.PipeMapRed: R/W/S=1900000/0/0 in:158333=1900000/12 [rec/s] out:0=0/12 [rec/s]\n",
      "16/01/30 20:43:01 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:01 INFO streaming.PipeMapRed: R/W/S=2000000/0/0 in:166666=2000000/12 [rec/s] out:0=0/12 [rec/s]\n",
      "16/01/30 20:43:01 INFO mapreduce.Job:  map 72% reduce 0%\n",
      "16/01/30 20:43:02 INFO streaming.PipeMapRed: R/W/S=2100000/0/0 in:161538=2100000/13 [rec/s] out:0=0/13 [rec/s]\n",
      "16/01/30 20:43:02 INFO streaming.PipeMapRed: R/W/S=2200000/0/0 in:169230=2200000/13 [rec/s] out:0=0/13 [rec/s]\n",
      "16/01/30 20:43:02 INFO streaming.PipeMapRed: R/W/S=2300000/0/0 in:164285=2300000/14 [rec/s] out:0=0/14 [rec/s]\n",
      "16/01/30 20:43:03 INFO streaming.PipeMapRed: R/W/S=2400000/0/0 in:171428=2400000/14 [rec/s] out:0=0/14 [rec/s]\n",
      "16/01/30 20:43:03 INFO streaming.PipeMapRed: R/W/S=2500000/0/0 in:178571=2500000/14 [rec/s] out:0=0/14 [rec/s]\n",
      "16/01/30 20:43:04 INFO streaming.PipeMapRed: R/W/S=2600000/0/0 in:173333=2600000/15 [rec/s] out:0=0/15 [rec/s]\n",
      "16/01/30 20:43:04 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:04 INFO mapreduce.Job:  map 74% reduce 0%\n",
      "16/01/30 20:43:05 INFO streaming.PipeMapRed: R/W/S=2700000/0/0 in:168750=2700000/16 [rec/s] out:0=0/16 [rec/s]\n",
      "16/01/30 20:43:05 INFO streaming.PipeMapRed: R/W/S=2800000/0/0 in:175000=2800000/16 [rec/s] out:0=0/16 [rec/s]\n",
      "16/01/30 20:43:05 INFO streaming.PipeMapRed: R/W/S=2900000/0/0 in:170588=2900000/17 [rec/s] out:0=0/17 [rec/s]\n",
      "16/01/30 20:43:06 INFO streaming.PipeMapRed: R/W/S=3000000/0/0 in:176470=3000000/17 [rec/s] out:0=0/17 [rec/s]\n",
      "16/01/30 20:43:07 INFO streaming.PipeMapRed: R/W/S=3100000/0/0 in:172222=3100000/18 [rec/s] out:0=0/18 [rec/s]\n",
      "16/01/30 20:43:07 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:07 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "16/01/30 20:43:07 INFO streaming.PipeMapRed: R/W/S=3200000/0/0 in:168421=3200000/19 [rec/s] out:0=0/19 [rec/s]\n",
      "16/01/30 20:43:08 INFO streaming.PipeMapRed: R/W/S=3300000/0/0 in:173684=3300000/19 [rec/s] out:0=0/19 [rec/s]\n",
      "16/01/30 20:43:09 INFO streaming.PipeMapRed: R/W/S=3400000/0/0 in:170000=3400000/20 [rec/s] out:0=0/20 [rec/s]\n",
      "16/01/30 20:43:10 INFO streaming.PipeMapRed: R/W/S=3500000/0/0 in:166666=3500000/21 [rec/s] out:0=0/21 [rec/s]\n",
      "16/01/30 20:43:10 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:10 INFO streaming.PipeMapRed: R/W/S=3600000/0/0 in:171428=3600000/21 [rec/s] out:0=0/21 [rec/s]\n",
      "16/01/30 20:43:10 INFO mapreduce.Job:  map 77% reduce 0%\n",
      "16/01/30 20:43:11 INFO streaming.PipeMapRed: R/W/S=3700000/0/0 in:168181=3700000/22 [rec/s] out:0=0/22 [rec/s]\n",
      "16/01/30 20:43:11 INFO streaming.PipeMapRed: R/W/S=3800000/0/0 in:172727=3800000/22 [rec/s] out:0=0/22 [rec/s]\n",
      "16/01/30 20:43:12 INFO streaming.PipeMapRed: R/W/S=3900000/0/0 in:169565=3900000/23 [rec/s] out:0=0/23 [rec/s]\n",
      "16/01/30 20:43:12 INFO streaming.PipeMapRed: R/W/S=4000000/0/0 in:173913=4000000/23 [rec/s] out:0=0/23 [rec/s]\n",
      "16/01/30 20:43:12 INFO streaming.PipeMapRed: R/W/S=4100000/0/0 in:178260=4100000/23 [rec/s] out:0=0/23 [rec/s]\n",
      "16/01/30 20:43:13 INFO streaming.PipeMapRed: R/W/S=4200000/0/0 in:175000=4200000/24 [rec/s] out:0=0/24 [rec/s]\n",
      "16/01/30 20:43:13 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:13 INFO streaming.PipeMapRed: R/W/S=4300000/0/0 in:179166=4300000/24 [rec/s] out:0=0/24 [rec/s]\n",
      "16/01/30 20:43:13 INFO mapreduce.Job:  map 78% reduce 0%\n",
      "16/01/30 20:43:14 INFO streaming.PipeMapRed: R/W/S=4400000/0/0 in:176000=4400000/25 [rec/s] out:0=0/25 [rec/s]\n",
      "16/01/30 20:43:14 INFO streaming.PipeMapRed: R/W/S=4500000/0/0 in:180000=4500000/25 [rec/s] out:0=0/25 [rec/s]\n",
      "16/01/30 20:43:14 INFO streaming.PipeMapRed: R/W/S=4600000/0/0 in:176923=4600000/26 [rec/s] out:0=0/26 [rec/s]\n",
      "16/01/30 20:43:15 INFO streaming.PipeMapRed: R/W/S=4700000/0/0 in:180769=4700000/26 [rec/s] out:0=0/26 [rec/s]\n",
      "16/01/30 20:43:15 INFO streaming.PipeMapRed: R/W/S=4800000/0/0 in:184615=4800000/26 [rec/s] out:0=0/26 [rec/s]\n",
      "16/01/30 20:43:16 INFO streaming.PipeMapRed: R/W/S=4900000/0/0 in:181481=4900000/27 [rec/s] out:0=0/27 [rec/s]\n",
      "16/01/30 20:43:16 INFO streaming.PipeMapRed: R/W/S=5000000/0/0 in:185185=5000000/27 [rec/s] out:0=0/27 [rec/s]\n",
      "16/01/30 20:43:16 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:16 INFO mapreduce.Job:  map 81% reduce 0%\n",
      "16/01/30 20:43:16 INFO streaming.PipeMapRed: R/W/S=5100000/0/0 in:182142=5100000/28 [rec/s] out:0=0/28 [rec/s]\n",
      "16/01/30 20:43:17 INFO streaming.PipeMapRed: R/W/S=5200000/0/0 in:185714=5200000/28 [rec/s] out:0=0/28 [rec/s]\n",
      "16/01/30 20:43:18 INFO streaming.PipeMapRed: R/W/S=5300000/0/0 in:182758=5300000/29 [rec/s] out:0=0/29 [rec/s]\n",
      "16/01/30 20:43:18 INFO streaming.PipeMapRed: R/W/S=5400000/0/0 in:186206=5400000/29 [rec/s] out:0=0/29 [rec/s]\n",
      "16/01/30 20:43:19 INFO streaming.PipeMapRed: R/W/S=5500000/0/0 in:183333=5500000/30 [rec/s] out:0=0/30 [rec/s]\n",
      "16/01/30 20:43:19 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:19 INFO mapreduce.Job:  map 82% reduce 0%\n",
      "16/01/30 20:43:20 INFO streaming.PipeMapRed: R/W/S=5600000/0/0 in:180645=5600000/31 [rec/s] out:0=0/31 [rec/s]\n",
      "16/01/30 20:43:20 INFO streaming.PipeMapRed: R/W/S=5700000/0/0 in:183870=5700000/31 [rec/s] out:0=0/31 [rec/s]\n",
      "16/01/30 20:43:21 INFO streaming.PipeMapRed: R/W/S=5800000/0/0 in:181250=5800000/32 [rec/s] out:0=0/32 [rec/s]\n",
      "16/01/30 20:43:21 INFO streaming.PipeMapRed: R/W/S=5900000/0/0 in:178787=5900000/33 [rec/s] out:0=0/33 [rec/s]\n",
      "16/01/30 20:43:22 INFO streaming.PipeMapRed: R/W/S=6000000/0/0 in:181818=6000000/33 [rec/s] out:0=0/33 [rec/s]\n",
      "16/01/30 20:43:22 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:22 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "16/01/30 20:43:23 INFO streaming.PipeMapRed: R/W/S=6100000/0/0 in:179411=6100000/34 [rec/s] out:0=0/34 [rec/s]\n",
      "16/01/30 20:43:24 INFO streaming.PipeMapRed: R/W/S=6200000/0/0 in:177142=6200000/35 [rec/s] out:0=0/35 [rec/s]\n",
      "16/01/30 20:43:25 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:25 INFO mapreduce.Job:  map 84% reduce 0%\n",
      "16/01/30 20:43:27 INFO streaming.PipeMapRed: R/W/S=6300000/0/0 in:165789=6300000/38 [rec/s] out:0=0/38 [rec/s]\n",
      "16/01/30 20:43:28 INFO streaming.PipeMapRed: R/W/S=6400000/0/0 in:164102=6400000/39 [rec/s] out:0=0/39 [rec/s]\n",
      "16/01/30 20:43:28 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:28 INFO streaming.PipeMapRed: R/W/S=6500000/0/0 in:166666=6500000/39 [rec/s] out:0=0/39 [rec/s]\n",
      "16/01/30 20:43:29 INFO streaming.PipeMapRed: R/W/S=6600000/0/0 in:165000=6600000/40 [rec/s] out:0=0/40 [rec/s]\n",
      "16/01/30 20:43:29 INFO streaming.PipeMapRed: R/W/S=6700000/0/0 in:163414=6700000/41 [rec/s] out:0=0/41 [rec/s]\n",
      "16/01/30 20:43:30 INFO streaming.PipeMapRed: R/W/S=6800000/0/0 in:165853=6800000/41 [rec/s] out:0=0/41 [rec/s]\n",
      "16/01/30 20:43:30 INFO streaming.PipeMapRed: R/W/S=6900000/0/0 in:164285=6900000/42 [rec/s] out:0=0/42 [rec/s]\n",
      "16/01/30 20:43:31 INFO streaming.PipeMapRed: R/W/S=7000000/0/0 in:166666=7000000/42 [rec/s] out:0=0/42 [rec/s]\n",
      "16/01/30 20:43:31 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:31 INFO mapreduce.Job:  map 86% reduce 0%\n",
      "16/01/30 20:43:32 INFO streaming.PipeMapRed: R/W/S=7100000/0/0 in:165116=7100000/43 [rec/s] out:0=0/43 [rec/s]\n",
      "16/01/30 20:43:32 INFO streaming.PipeMapRed: R/W/S=7200000/0/0 in:167441=7200000/43 [rec/s] out:0=0/43 [rec/s]\n",
      "16/01/30 20:43:33 INFO streaming.PipeMapRed: R/W/S=7300000/0/0 in:165909=7300000/44 [rec/s] out:0=0/44 [rec/s]\n",
      "16/01/30 20:43:33 INFO streaming.PipeMapRed: R/W/S=7400000/0/0 in:168181=7400000/44 [rec/s] out:0=0/44 [rec/s]\n",
      "16/01/30 20:43:34 INFO streaming.PipeMapRed: R/W/S=7500000/0/0 in:166666=7500000/45 [rec/s] out:0=0/45 [rec/s]\n",
      "16/01/30 20:43:34 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:34 INFO streaming.PipeMapRed: R/W/S=7600000/0/0 in:168888=7600000/45 [rec/s] out:0=0/45 [rec/s]\n",
      "16/01/30 20:43:34 INFO mapreduce.Job:  map 88% reduce 0%\n",
      "16/01/30 20:43:35 INFO streaming.PipeMapRed: R/W/S=7700000/0/0 in:167391=7700000/46 [rec/s] out:0=0/46 [rec/s]\n",
      "16/01/30 20:43:35 INFO streaming.PipeMapRed: R/W/S=7800000/0/0 in:169565=7800000/46 [rec/s] out:0=0/46 [rec/s]\n",
      "16/01/30 20:43:36 INFO streaming.PipeMapRed: R/W/S=7900000/0/0 in:168085=7900000/47 [rec/s] out:0=0/47 [rec/s]\n",
      "16/01/30 20:43:36 INFO streaming.PipeMapRed: R/W/S=8000000/0/0 in:170212=8000000/47 [rec/s] out:0=0/47 [rec/s]\n",
      "16/01/30 20:43:37 INFO streaming.PipeMapRed: R/W/S=8100000/0/0 in:168750=8100000/48 [rec/s] out:0=0/48 [rec/s]\n",
      "16/01/30 20:43:37 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:37 INFO streaming.PipeMapRed: R/W/S=8200000/0/0 in:170833=8200000/48 [rec/s] out:0=0/48 [rec/s]\n",
      "16/01/30 20:43:37 INFO mapreduce.Job:  map 89% reduce 0%\n",
      "16/01/30 20:43:38 INFO streaming.PipeMapRed: R/W/S=8300000/0/0 in:169387=8300000/49 [rec/s] out:0=0/49 [rec/s]\n",
      "16/01/30 20:43:38 INFO streaming.PipeMapRed: R/W/S=8400000/0/0 in:171428=8400000/49 [rec/s] out:0=0/49 [rec/s]\n",
      "16/01/30 20:43:39 INFO streaming.PipeMapRed: R/W/S=8500000/0/0 in:170000=8500000/50 [rec/s] out:0=0/50 [rec/s]\n",
      "16/01/30 20:43:39 INFO streaming.PipeMapRed: R/W/S=8600000/0/0 in:172000=8600000/50 [rec/s] out:0=0/50 [rec/s]\n",
      "16/01/30 20:43:40 INFO streaming.PipeMapRed: R/W/S=8700000/0/0 in:170588=8700000/51 [rec/s] out:0=0/51 [rec/s]\n",
      "16/01/30 20:43:40 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:40 INFO streaming.PipeMapRed: R/W/S=8800000/0/0 in:172549=8800000/51 [rec/s] out:0=0/51 [rec/s]\n",
      "16/01/30 20:43:40 INFO mapreduce.Job:  map 91% reduce 0%\n",
      "16/01/30 20:43:41 INFO streaming.PipeMapRed: R/W/S=8900000/0/0 in:171153=8900000/52 [rec/s] out:0=0/52 [rec/s]\n",
      "16/01/30 20:43:41 INFO streaming.PipeMapRed: R/W/S=9000000/0/0 in:173076=9000000/52 [rec/s] out:0=0/52 [rec/s]\n",
      "16/01/30 20:43:42 INFO streaming.PipeMapRed: R/W/S=9100000/0/0 in:171698=9100000/53 [rec/s] out:0=0/53 [rec/s]\n",
      "16/01/30 20:43:42 INFO streaming.PipeMapRed: R/W/S=9200000/0/0 in:173584=9200000/53 [rec/s] out:0=0/53 [rec/s]\n",
      "16/01/30 20:43:43 INFO streaming.PipeMapRed: R/W/S=9300000/0/0 in:172222=9300000/54 [rec/s] out:0=0/54 [rec/s]\n",
      "16/01/30 20:43:43 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:43 INFO streaming.PipeMapRed: R/W/S=9400000/0/0 in:174074=9400000/54 [rec/s] out:0=0/54 [rec/s]\n",
      "16/01/30 20:43:43 INFO mapreduce.Job:  map 93% reduce 0%\n",
      "16/01/30 20:43:44 INFO streaming.PipeMapRed: R/W/S=9500000/0/0 in:172727=9500000/55 [rec/s] out:0=0/55 [rec/s]\n",
      "16/01/30 20:43:44 INFO streaming.PipeMapRed: R/W/S=9600000/0/0 in:174545=9600000/55 [rec/s] out:0=0/55 [rec/s]\n",
      "16/01/30 20:43:45 INFO streaming.PipeMapRed: R/W/S=9700000/0/0 in:173214=9700000/56 [rec/s] out:0=0/56 [rec/s]\n",
      "16/01/30 20:43:45 INFO streaming.PipeMapRed: R/W/S=9800000/0/0 in:171929=9800000/57 [rec/s] out:0=0/57 [rec/s]\n",
      "16/01/30 20:43:46 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:46 INFO streaming.PipeMapRed: R/W/S=9900000/0/0 in:173684=9900000/57 [rec/s] out:0=0/57 [rec/s]\n",
      "16/01/30 20:43:46 INFO mapreduce.Job:  map 94% reduce 0%\n",
      "16/01/30 20:43:47 INFO streaming.PipeMapRed: R/W/S=10000000/0/0 in:172413=10000000/58 [rec/s] out:0=0/58 [rec/s]\n",
      "16/01/30 20:43:47 INFO streaming.PipeMapRed: R/W/S=10100000/0/0 in:174137=10100000/58 [rec/s] out:0=0/58 [rec/s]\n",
      "16/01/30 20:43:48 INFO streaming.PipeMapRed: R/W/S=10200000/0/0 in:172881=10200000/59 [rec/s] out:0=0/59 [rec/s]\n",
      "16/01/30 20:43:48 INFO streaming.PipeMapRed: R/W/S=10300000/0/0 in:174576=10300000/59 [rec/s] out:0=0/59 [rec/s]\n",
      "16/01/30 20:43:49 INFO streaming.PipeMapRed: R/W/S=10400000/0/0 in:173333=10400000/60 [rec/s] out:0=0/60 [rec/s]\n",
      "16/01/30 20:43:49 INFO streaming.PipeMapRed: R/W/S=10500000/0/0 in:175000=10500000/60 [rec/s] out:0=0/60 [rec/s]\n",
      "16/01/30 20:43:49 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:49 INFO mapreduce.Job:  map 96% reduce 0%\n",
      "16/01/30 20:43:49 INFO streaming.PipeMapRed: R/W/S=10600000/0/0 in:173770=10600000/61 [rec/s] out:0=0/61 [rec/s]\n",
      "16/01/30 20:43:50 INFO streaming.PipeMapRed: R/W/S=10700000/0/0 in:175409=10700000/61 [rec/s] out:0=0/61 [rec/s]\n",
      "16/01/30 20:43:50 INFO streaming.PipeMapRed: R/W/S=10800000/0/0 in:177049=10800000/61 [rec/s] out:0=0/61 [rec/s]\n",
      "16/01/30 20:43:51 INFO streaming.PipeMapRed: R/W/S=10900000/0/0 in:175806=10900000/62 [rec/s] out:0=0/62 [rec/s]\n",
      "16/01/30 20:43:51 INFO streaming.PipeMapRed: R/W/S=11000000/0/0 in:174603=11000000/63 [rec/s] out:0=0/63 [rec/s]\n",
      "16/01/30 20:43:52 INFO streaming.PipeMapRed: R/W/S=11100000/0/0 in:176190=11100000/63 [rec/s] out:0=0/63 [rec/s]\n",
      "16/01/30 20:43:52 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:52 INFO mapreduce.Job:  map 97% reduce 0%\n",
      "16/01/30 20:43:53 INFO streaming.PipeMapRed: R/W/S=11200000/0/0 in:175000=11200000/64 [rec/s] out:0=0/64 [rec/s]\n",
      "16/01/30 20:43:54 INFO streaming.PipeMapRed: R/W/S=11300000/0/0 in:173846=11300000/65 [rec/s] out:0=0/65 [rec/s]\n",
      "16/01/30 20:43:54 INFO streaming.PipeMapRed: R/W/S=11400000/0/0 in:175384=11400000/65 [rec/s] out:0=0/65 [rec/s]\n",
      "16/01/30 20:43:55 INFO streaming.PipeMapRed: R/W/S=11500000/0/0 in:174242=11500000/66 [rec/s] out:0=0/66 [rec/s]\n",
      "16/01/30 20:43:55 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:55 INFO mapreduce.Job:  map 98% reduce 0%\n",
      "16/01/30 20:43:55 INFO streaming.PipeMapRed: R/W/S=11600000/0/0 in:173134=11600000/67 [rec/s] out:0=0/67 [rec/s]\n",
      "16/01/30 20:43:56 INFO streaming.PipeMapRed: R/W/S=11700000/0/0 in:174626=11700000/67 [rec/s] out:0=0/67 [rec/s]\n",
      "16/01/30 20:43:56 INFO streaming.PipeMapRed: R/W/S=11800000/0/0 in:173529=11800000/68 [rec/s] out:0=0/68 [rec/s]\n",
      "16/01/30 20:43:57 INFO streaming.PipeMapRed: R/W/S=11900000/0/0 in:175000=11900000/68 [rec/s] out:0=0/68 [rec/s]\n",
      "16/01/30 20:43:57 INFO streaming.PipeMapRed: R/W/S=12000000/0/0 in:176470=12000000/68 [rec/s] out:0=0/68 [rec/s]\n",
      "16/01/30 20:43:58 INFO streaming.PipeMapRed: Records R/W=12070340/1\n",
      "16/01/30 20:43:58 INFO mapred.LocalJobRunner: Records R/W=12070340/1 > sort > \n",
      "16/01/30 20:43:58 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/30 20:44:01 INFO mapred.LocalJobRunner: Records R/W=12070340/1 > sort > \n",
      "16/01/30 20:44:04 INFO mapred.LocalJobRunner: Records R/W=12070340/1 > sort > \n",
      "16/01/30 20:44:07 INFO mapred.LocalJobRunner: Records R/W=12070340/1 > sort > \n",
      "16/01/30 20:44:08 INFO streaming.PipeMapRed: Records R/W=12070340/5951298\n",
      "16/01/30 20:44:10 INFO mapred.LocalJobRunner: Records R/W=12070340/5951298 > sort > \n",
      "16/01/30 20:44:13 INFO mapred.LocalJobRunner: Records R/W=12070340/5951298 > sort > \n",
      "16/01/30 20:44:16 INFO mapred.LocalJobRunner: Records R/W=12070340/5951298 > sort > \n",
      "16/01/30 20:44:19 INFO mapred.LocalJobRunner: Records R/W=12070340/5951298 > sort > \n",
      "16/01/30 20:44:19 INFO streaming.PipeMapRed: Records R/W=12070340/10092674\n",
      "16/01/30 20:44:19 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 20:44:19 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 20:44:19 INFO mapred.Task: Task:attempt_local583153488_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/30 20:44:19 INFO mapred.LocalJobRunner: Records R/W=12070340/10092674 > sort\n",
      "16/01/30 20:44:19 INFO mapred.Task: Task 'attempt_local583153488_0001_m_000000_0' done.\n",
      "16/01/30 20:44:19 INFO mapred.LocalJobRunner: Finishing task: attempt_local583153488_0001_m_000000_0\n",
      "16/01/30 20:44:19 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/30 20:44:19 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/30 20:44:19 INFO mapred.LocalJobRunner: Starting task: attempt_local583153488_0001_r_000000_0\n",
      "16/01/30 20:44:19 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 20:44:19 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 20:44:19 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 20:44:20 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6f95197a\n",
      "16/01/30 20:44:20 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/30 20:44:20 INFO reduce.EventFetcher: attempt_local583153488_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/30 20:44:20 INFO reduce.MergeManagerImpl: attempt_local583153488_0001_m_000000_0: Shuffling to disk since 403369367 is greater than maxSingleShuffleLimit (83584616)\n",
      "16/01/30 20:44:20 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local583153488_0001_m_000000_0 decomp: 403369367 len: 403369371 to DISK\n",
      "16/01/30 20:44:23 INFO reduce.OnDiskMapOutput: Read 403369371 bytes from map-output for attempt_local583153488_0001_m_000000_0\n",
      "16/01/30 20:44:23 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/30 20:44:23 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 20:44:23 INFO reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 1 on-disk map-outputs\n",
      "16/01/30 20:44:23 INFO reduce.MergeManagerImpl: Merging 1 files, 403369371 bytes from disk\n",
      "16/01/30 20:44:23 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/30 20:44:23 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/30 20:44:23 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 403369328 bytes\n",
      "16/01/30 20:44:23 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 20:44:23 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./reducerQ37.py]\n",
      "16/01/30 20:44:23 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/30 20:44:23 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/30 20:44:23 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:44:23 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:44:23 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:44:23 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:44:23 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:44:24 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:44:24 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:200000=200000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:44:25 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:44:25 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:200000=400000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:44:26 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:44:26 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:250000=500000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:44:26 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:300000=600000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:44:27 INFO mapreduce.Job:  map 100% reduce 68%\n",
      "16/01/30 20:44:27 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:233333=700000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:44:27 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:266666=800000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:44:27 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:225000=900000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:44:28 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:250000=1000000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:44:28 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:220000=1100000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:44:29 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:44:29 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:240000=1200000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:44:29 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:260000=1300000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:44:29 INFO streaming.PipeMapRed: R/W/S=1400000/0/0 in:233333=1400000/6 [rec/s] out:0=0/6 [rec/s]\n",
      "16/01/30 20:44:30 INFO mapreduce.Job:  map 100% reduce 71%\n",
      "16/01/30 20:44:30 INFO streaming.PipeMapRed: R/W/S=1500000/0/0 in:250000=1500000/6 [rec/s] out:0=0/6 [rec/s]\n",
      "16/01/30 20:44:30 INFO streaming.PipeMapRed: R/W/S=1600000/0/0 in:228571=1600000/7 [rec/s] out:0=0/7 [rec/s]\n",
      "16/01/30 20:44:31 INFO streaming.PipeMapRed: R/W/S=1700000/0/0 in:212500=1700000/8 [rec/s] out:0=0/8 [rec/s]\n",
      "16/01/30 20:44:32 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:44:32 INFO mapreduce.Job:  map 100% reduce 72%\n",
      "16/01/30 20:44:32 INFO streaming.PipeMapRed: R/W/S=1800000/0/0 in:225000=1800000/8 [rec/s] out:0=0/8 [rec/s]\n",
      "16/01/30 20:44:32 INFO streaming.PipeMapRed: R/W/S=1900000/0/0 in:211111=1900000/9 [rec/s] out:0=0/9 [rec/s]\n",
      "16/01/30 20:44:33 INFO streaming.PipeMapRed: R/W/S=2000000/0/0 in:222222=2000000/9 [rec/s] out:0=0/9 [rec/s]\n",
      "16/01/30 20:44:33 INFO streaming.PipeMapRed: R/W/S=2100000/0/0 in:233333=2100000/9 [rec/s] out:0=0/9 [rec/s]\n",
      "16/01/30 20:44:34 INFO streaming.PipeMapRed: R/W/S=2200000/0/0 in:220000=2200000/10 [rec/s] out:0=0/10 [rec/s]\n",
      "16/01/30 20:44:34 INFO streaming.PipeMapRed: R/W/S=2300000/0/0 in:230000=2300000/10 [rec/s] out:0=0/10 [rec/s]\n",
      "16/01/30 20:44:34 INFO streaming.PipeMapRed: R/W/S=2400000/0/0 in:218181=2400000/11 [rec/s] out:0=0/11 [rec/s]\n",
      "16/01/30 20:44:35 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:44:35 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "16/01/30 20:44:35 INFO streaming.PipeMapRed: R/W/S=2500000/0/0 in:227272=2500000/11 [rec/s] out:0=0/11 [rec/s]\n",
      "16/01/30 20:44:36 INFO streaming.PipeMapRed: R/W/S=2600000/0/0 in:216666=2600000/12 [rec/s] out:0=0/12 [rec/s]\n",
      "16/01/30 20:44:36 INFO streaming.PipeMapRed: R/W/S=2700000/0/0 in:207692=2700000/13 [rec/s] out:0=0/13 [rec/s]\n",
      "16/01/30 20:44:37 INFO streaming.PipeMapRed: R/W/S=2800000/0/0 in:215384=2800000/13 [rec/s] out:0=0/13 [rec/s]\n",
      "16/01/30 20:44:38 INFO streaming.PipeMapRed: R/W/S=2900000/0/0 in:207142=2900000/14 [rec/s] out:0=0/14 [rec/s]\n",
      "16/01/30 20:44:38 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:44:38 INFO mapreduce.Job:  map 100% reduce 76%\n",
      "16/01/30 20:44:38 INFO streaming.PipeMapRed: R/W/S=3000000/0/0 in:214285=3000000/14 [rec/s] out:0=0/14 [rec/s]\n",
      "16/01/30 20:44:39 INFO streaming.PipeMapRed: R/W/S=3100000/0/0 in:206666=3100000/15 [rec/s] out:0=0/15 [rec/s]\n",
      "16/01/30 20:44:39 INFO streaming.PipeMapRed: R/W/S=3200000/0/0 in:213333=3200000/15 [rec/s] out:0=0/15 [rec/s]\n",
      "16/01/30 20:44:39 INFO streaming.PipeMapRed: R/W/S=3300000/0/0 in:206250=3300000/16 [rec/s] out:0=0/16 [rec/s]\n",
      "16/01/30 20:44:40 INFO streaming.PipeMapRed: R/W/S=3400000/0/0 in:212500=3400000/16 [rec/s] out:0=0/16 [rec/s]\n",
      "16/01/30 20:44:40 INFO streaming.PipeMapRed: R/W/S=3500000/0/0 in:218750=3500000/16 [rec/s] out:0=0/16 [rec/s]\n",
      "16/01/30 20:44:40 INFO streaming.PipeMapRed: R/W/S=3600000/0/0 in:211764=3600000/17 [rec/s] out:0=0/17 [rec/s]\n",
      "16/01/30 20:44:41 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:44:41 INFO mapreduce.Job:  map 100% reduce 79%\n",
      "16/01/30 20:44:41 INFO streaming.PipeMapRed: R/W/S=3700000/0/0 in:217647=3700000/17 [rec/s] out:0=0/17 [rec/s]\n",
      "16/01/30 20:44:41 INFO streaming.PipeMapRed: R/W/S=3800000/0/0 in:223529=3800000/17 [rec/s] out:0=0/17 [rec/s]\n",
      "16/01/30 20:44:42 INFO streaming.PipeMapRed: R/W/S=3900000/0/0 in:216666=3900000/18 [rec/s] out:0=0/18 [rec/s]\n",
      "16/01/30 20:44:42 INFO streaming.PipeMapRed: R/W/S=4000000/0/0 in:222222=4000000/18 [rec/s] out:0=0/18 [rec/s]\n",
      "16/01/30 20:44:43 INFO streaming.PipeMapRed: R/W/S=4100000/0/0 in:215789=4100000/19 [rec/s] out:0=0/19 [rec/s]\n",
      "16/01/30 20:44:43 INFO streaming.PipeMapRed: R/W/S=4200000/0/0 in:210000=4200000/20 [rec/s] out:0=0/20 [rec/s]\n",
      "16/01/30 20:44:44 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:44:44 INFO mapreduce.Job:  map 100% reduce 81%\n",
      "16/01/30 20:44:44 INFO streaming.PipeMapRed: R/W/S=4300000/0/0 in:215000=4300000/20 [rec/s] out:0=0/20 [rec/s]\n",
      "16/01/30 20:44:44 INFO streaming.PipeMapRed: R/W/S=4400000/0/0 in:209523=4400000/21 [rec/s] out:0=0/21 [rec/s]\n",
      "16/01/30 20:44:45 INFO streaming.PipeMapRed: R/W/S=4500000/0/0 in:214285=4500000/21 [rec/s] out:0=0/21 [rec/s]\n",
      "16/01/30 20:44:45 INFO streaming.PipeMapRed: R/W/S=4600000/0/0 in:219047=4600000/21 [rec/s] out:0=0/21 [rec/s]\n",
      "16/01/30 20:44:45 INFO streaming.PipeMapRed: R/W/S=4700000/0/0 in:213636=4700000/22 [rec/s] out:0=0/22 [rec/s]\n",
      "16/01/30 20:44:46 INFO streaming.PipeMapRed: R/W/S=4800000/0/0 in:218181=4800000/22 [rec/s] out:0=0/22 [rec/s]\n",
      "16/01/30 20:44:46 INFO streaming.PipeMapRed: R/W/S=4900000/0/0 in:213043=4900000/23 [rec/s] out:0=0/23 [rec/s]\n",
      "16/01/30 20:44:47 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:44:47 INFO mapreduce.Job:  map 100% reduce 83%\n",
      "16/01/30 20:44:47 INFO streaming.PipeMapRed: R/W/S=5000000/0/0 in:217391=5000000/23 [rec/s] out:0=0/23 [rec/s]\n",
      "16/01/30 20:44:47 INFO streaming.PipeMapRed: R/W/S=5100000/0/0 in:221739=5100000/23 [rec/s] out:0=0/23 [rec/s]\n",
      "16/01/30 20:44:48 INFO streaming.PipeMapRed: R/W/S=5200000/0/0 in:216666=5200000/24 [rec/s] out:0=0/24 [rec/s]\n",
      "16/01/30 20:44:49 INFO streaming.PipeMapRed: R/W/S=5300000/0/0 in:212000=5300000/25 [rec/s] out:0=0/25 [rec/s]\n",
      "16/01/30 20:44:50 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:44:50 INFO mapreduce.Job:  map 100% reduce 84%\n",
      "16/01/30 20:44:50 INFO streaming.PipeMapRed: R/W/S=5400000/0/0 in:207692=5400000/26 [rec/s] out:0=0/26 [rec/s]\n",
      "16/01/30 20:44:51 INFO streaming.PipeMapRed: R/W/S=5500000/0/0 in:203703=5500000/27 [rec/s] out:0=0/27 [rec/s]\n",
      "16/01/30 20:44:53 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:44:53 INFO mapreduce.Job:  map 100% reduce 85%\n",
      "16/01/30 20:44:56 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:44:57 INFO streaming.PipeMapRed: R/W/S=5600000/0/0 in:169696=5600000/33 [rec/s] out:0=0/33 [rec/s]\n",
      "16/01/30 20:44:58 INFO streaming.PipeMapRed: R/W/S=5700000/0/0 in:167647=5700000/34 [rec/s] out:0=0/34 [rec/s]\n",
      "16/01/30 20:44:59 INFO streaming.PipeMapRed: R/W/S=5800000/0/0 in:165714=5800000/35 [rec/s] out:0=0/35 [rec/s]\n",
      "16/01/30 20:44:59 INFO streaming.PipeMapRed: R/W/S=5900000/0/0 in:163888=5900000/36 [rec/s] out:0=0/36 [rec/s]\n",
      "16/01/30 20:45:00 INFO streaming.PipeMapRed: R/W/S=6000000/0/0 in:162162=6000000/37 [rec/s] out:0=0/37 [rec/s]\n",
      "16/01/30 20:45:01 INFO streaming.PipeMapRed: R/W/S=6100000/0/0 in:164864=6100000/37 [rec/s] out:0=0/37 [rec/s]\n",
      "16/01/30 20:45:01 INFO streaming.PipeMapRed: R/W/S=6200000/0/0 in:163157=6200000/38 [rec/s] out:0=0/38 [rec/s]\n",
      "16/01/30 20:45:02 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:45:02 INFO mapreduce.Job:  map 100% reduce 87%\n",
      "16/01/30 20:45:02 INFO streaming.PipeMapRed: R/W/S=6300000/0/0 in:165789=6300000/38 [rec/s] out:0=0/38 [rec/s]\n",
      "16/01/30 20:45:03 INFO streaming.PipeMapRed: R/W/S=6400000/0/0 in:164102=6400000/39 [rec/s] out:0=0/39 [rec/s]\n",
      "16/01/30 20:45:03 INFO streaming.PipeMapRed: R/W/S=6500000/0/0 in:162500=6500000/40 [rec/s] out:0=0/40 [rec/s]\n",
      "16/01/30 20:45:04 INFO streaming.PipeMapRed: R/W/S=6600000/0/0 in:165000=6600000/40 [rec/s] out:0=0/40 [rec/s]\n",
      "16/01/30 20:45:04 INFO streaming.PipeMapRed: R/W/S=6700000/0/0 in:167500=6700000/40 [rec/s] out:0=0/40 [rec/s]\n",
      "16/01/30 20:45:04 INFO streaming.PipeMapRed: R/W/S=6800000/0/0 in:165853=6800000/41 [rec/s] out:0=0/41 [rec/s]\n",
      "16/01/30 20:45:05 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:45:05 INFO mapreduce.Job:  map 100% reduce 89%\n",
      "16/01/30 20:45:05 INFO streaming.PipeMapRed: R/W/S=6900000/0/0 in:168292=6900000/41 [rec/s] out:0=0/41 [rec/s]\n",
      "16/01/30 20:45:05 INFO streaming.PipeMapRed: R/W/S=7000000/0/0 in:170731=7000000/41 [rec/s] out:0=0/41 [rec/s]\n",
      "16/01/30 20:45:06 INFO streaming.PipeMapRed: R/W/S=7100000/0/0 in:169047=7100000/42 [rec/s] out:0=0/42 [rec/s]\n",
      "16/01/30 20:45:06 INFO streaming.PipeMapRed: R/W/S=7200000/0/0 in:167441=7200000/43 [rec/s] out:0=0/43 [rec/s]\n",
      "16/01/30 20:45:07 INFO streaming.PipeMapRed: R/W/S=7300000/0/0 in:169767=7300000/43 [rec/s] out:0=0/43 [rec/s]\n",
      "16/01/30 20:45:07 INFO streaming.PipeMapRed: R/W/S=7400000/0/0 in:172093=7400000/43 [rec/s] out:0=0/43 [rec/s]\n",
      "16/01/30 20:45:08 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:45:08 INFO mapreduce.Job:  map 100% reduce 91%\n",
      "16/01/30 20:45:08 INFO streaming.PipeMapRed: R/W/S=7500000/0/0 in:170454=7500000/44 [rec/s] out:0=0/44 [rec/s]\n",
      "16/01/30 20:45:08 INFO streaming.PipeMapRed: R/W/S=7600000/0/0 in:168888=7600000/45 [rec/s] out:0=0/45 [rec/s]\n",
      "16/01/30 20:45:09 INFO streaming.PipeMapRed: R/W/S=7700000/0/0 in:171111=7700000/45 [rec/s] out:0=0/45 [rec/s]\n",
      "16/01/30 20:45:10 INFO streaming.PipeMapRed: R/W/S=7800000/0/0 in:169565=7800000/46 [rec/s] out:0=0/46 [rec/s]\n",
      "16/01/30 20:45:10 INFO streaming.PipeMapRed: R/W/S=7900000/0/0 in:168085=7900000/47 [rec/s] out:0=0/47 [rec/s]\n",
      "16/01/30 20:45:11 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:45:11 INFO mapreduce.Job:  map 100% reduce 93%\n",
      "16/01/30 20:45:11 INFO streaming.PipeMapRed: R/W/S=8000000/0/0 in:170212=8000000/47 [rec/s] out:0=0/47 [rec/s]\n",
      "16/01/30 20:45:11 INFO streaming.PipeMapRed: R/W/S=8100000/0/0 in:168750=8100000/48 [rec/s] out:0=0/48 [rec/s]\n",
      "16/01/30 20:45:12 INFO streaming.PipeMapRed: R/W/S=8200000/0/0 in:170833=8200000/48 [rec/s] out:0=0/48 [rec/s]\n",
      "16/01/30 20:45:13 INFO streaming.PipeMapRed: R/W/S=8300000/0/0 in:169387=8300000/49 [rec/s] out:0=0/49 [rec/s]\n",
      "16/01/30 20:45:13 INFO streaming.PipeMapRed: R/W/S=8400000/0/0 in:168000=8400000/50 [rec/s] out:0=0/50 [rec/s]\n",
      "16/01/30 20:45:14 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:45:14 INFO mapreduce.Job:  map 100% reduce 94%\n",
      "16/01/30 20:45:14 INFO streaming.PipeMapRed: R/W/S=8500000/0/0 in:166666=8500000/51 [rec/s] out:0=0/51 [rec/s]\n",
      "16/01/30 20:45:15 INFO streaming.PipeMapRed: R/W/S=8600000/0/0 in:168627=8600000/51 [rec/s] out:0=0/51 [rec/s]\n",
      "16/01/30 20:45:15 INFO streaming.PipeMapRed: R/W/S=8700000/0/0 in:167307=8700000/52 [rec/s] out:0=0/52 [rec/s]\n",
      "16/01/30 20:45:16 INFO streaming.PipeMapRed: R/W/S=8800000/0/0 in:169230=8800000/52 [rec/s] out:0=0/52 [rec/s]\n",
      "16/01/30 20:45:17 INFO streaming.PipeMapRed: R/W/S=8900000/0/0 in:167924=8900000/53 [rec/s] out:0=0/53 [rec/s]\n",
      "16/01/30 20:45:17 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:45:17 INFO mapreduce.Job:  map 100% reduce 96%\n",
      "16/01/30 20:45:17 INFO streaming.PipeMapRed: R/W/S=9000000/0/0 in:166666=9000000/54 [rec/s] out:0=0/54 [rec/s]\n",
      "16/01/30 20:45:18 INFO streaming.PipeMapRed: R/W/S=9100000/0/0 in:168518=9100000/54 [rec/s] out:0=0/54 [rec/s]\n",
      "16/01/30 20:45:19 INFO streaming.PipeMapRed: R/W/S=9200000/0/0 in:167272=9200000/55 [rec/s] out:0=0/55 [rec/s]\n",
      "16/01/30 20:45:19 INFO streaming.PipeMapRed: R/W/S=9300000/0/0 in:166071=9300000/56 [rec/s] out:0=0/56 [rec/s]\n",
      "16/01/30 20:45:20 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:45:20 INFO mapreduce.Job:  map 100% reduce 98%\n",
      "16/01/30 20:45:20 INFO streaming.PipeMapRed: R/W/S=9400000/0/0 in:167857=9400000/56 [rec/s] out:0=0/56 [rec/s]\n",
      "16/01/30 20:45:21 INFO streaming.PipeMapRed: R/W/S=9500000/0/0 in:166666=9500000/57 [rec/s] out:0=0/57 [rec/s]\n",
      "16/01/30 20:45:22 INFO streaming.PipeMapRed: R/W/S=9600000/0/0 in:165517=9600000/58 [rec/s] out:0=0/58 [rec/s]\n",
      "16/01/30 20:45:22 INFO streaming.PipeMapRed: R/W/S=9700000/0/0 in:164406=9700000/59 [rec/s] out:0=0/59 [rec/s]\n",
      "16/01/30 20:45:23 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:45:23 INFO mapreduce.Job:  map 100% reduce 99%\n",
      "16/01/30 20:45:23 INFO streaming.PipeMapRed: R/W/S=9800000/0/0 in:166101=9800000/59 [rec/s] out:0=0/59 [rec/s]\n",
      "16/01/30 20:45:23 INFO streaming.PipeMapRed: R/W/S=9900000/0/0 in:165000=9900000/60 [rec/s] out:0=0/60 [rec/s]\n",
      "16/01/30 20:45:24 INFO streaming.PipeMapRed: R/W/S=10000000/0/0 in:166666=10000000/60 [rec/s] out:0=0/60 [rec/s]\n",
      "16/01/30 20:45:26 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:45:26 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/30 20:45:29 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:45:39 INFO streaming.PipeMapRed: Records R/W=10092961/1\n",
      "16/01/30 20:45:39 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 20:45:39 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 20:45:39 INFO mapred.Task: Task:attempt_local583153488_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/30 20:45:39 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:45:39 INFO mapred.Task: Task attempt_local583153488_0001_r_000000_0 is allowed to commit now\n",
      "16/01/30 20:45:39 INFO output.FileOutputCommitter: Saved output of task 'attempt_local583153488_0001_r_000000_0' to hdfs://localhost:9000/user/john/notebook/output/_temporary/0/task_local583153488_0001_r_000000\n",
      "16/01/30 20:45:39 INFO mapred.LocalJobRunner: Records R/W=10092961/1 > reduce\n",
      "16/01/30 20:45:39 INFO mapred.Task: Task 'attempt_local583153488_0001_r_000000_0' done.\n",
      "16/01/30 20:45:39 INFO mapred.LocalJobRunner: Finishing task: attempt_local583153488_0001_r_000000_0\n",
      "16/01/30 20:45:39 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/30 20:45:40 INFO mapreduce.Job: Job job_local583153488_0001 completed successfully\n",
      "16/01/30 20:45:40 INFO mapreduce.Job: Counters: 36\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1762727936\n",
      "\t\tFILE: Number of bytes written=2166684757\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6917034\n",
      "\t\tHDFS: Number of bytes written=8666\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=14728700\n",
      "\t\tMap output bytes=544010616\n",
      "\t\tMap output materialized bytes=403369371\n",
      "\t\tInput split bytes=122\n",
      "\t\tCombine input records=26799040\n",
      "\t\tCombine output records=22163301\n",
      "\t\tReduce input groups=10092961\n",
      "\t\tReduce shuffle bytes=403369371\n",
      "\t\tReduce input records=10092961\n",
      "\t\tReduce output records=108\n",
      "\t\tSpilled Records=32256262\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=52\n",
      "\t\tTotal committed heap usage (bytes)=526385152\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tUser-Defined\n",
      "\t\tNumber of Combiners=11\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=8666\n",
      "16/01/30 20:45:40 INFO streaming.StreamJob: Output directory: /user/john/notebook/output\n",
      "16/01/30 20:45:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\t\n",
      "========== Most Frequent Doubles ==========\t\n",
      "\t\n",
      "PAIR                          |SUPPORT COUNT       |SUPPORT             \t\n",
      "('DAI62779', 'ELE17451')      |                1592|                 100\t\n",
      "('FRO40251', 'SNA80324')      |                1412|                 100\t\n",
      "('DAI75645', 'FRO40251')      |                1254|                 100\t\n",
      "('FRO40251', 'GRO85051')      |                1213|                 100\t\n",
      "('DAI62779', 'GRO73461')      |                1139|                 100\t\n",
      "('DAI75645', 'SNA80324')      |                1130|                 100\t\n",
      "('DAI62779', 'FRO40251')      |                1070|                 100\t\n",
      "('DAI62779', 'SNA80324')      |                 923|                 100\t\n",
      "('DAI62779', 'DAI85309')      |                 918|                 100\t\n",
      "('ELE32164', 'GRO59710')      |                 911|                 100\t\n",
      "('DAI62779', 'DAI75645')      |                 882|                 100\t\n",
      "('FRO40251', 'GRO73461')      |                 882|                 100\t\n",
      "('DAI62779', 'ELE92920')      |                 877|                 100\t\n",
      "('FRO40251', 'FRO92469')      |                 835|                 100\t\n",
      "('DAI62779', 'ELE32164')      |                 832|                 100\t\n",
      "('DAI75645', 'GRO73461')      |                 712|                 100\t\n",
      "('DAI43223', 'ELE32164')      |                 711|                 100\t\n",
      "('DAI62779', 'GRO30386')      |                 709|                 100\t\n",
      "('ELE17451', 'FRO40251')      |                 697|                 100\t\n",
      "('DAI85309', 'ELE99737')      |                 659|                 100\t\n",
      "('DAI62779', 'ELE26917')      |                 650|                 100\t\n",
      "('GRO21487', 'GRO73461')      |                 631|                 100\t\n",
      "('DAI62779', 'SNA45677')      |                 604|                 100\t\n",
      "('ELE17451', 'SNA80324')      |                 597|                 100\t\n",
      "('DAI62779', 'GRO71621')      |                 595|                 100\t\n",
      "('DAI62779', 'SNA55762')      |                 593|                 100\t\n",
      "('DAI62779', 'DAI83733')      |                 586|                 100\t\n",
      "('ELE17451', 'GRO73461')      |                 580|                 100\t\n",
      "('GRO73461', 'SNA80324')      |                 562|                 100\t\n",
      "('DAI62779', 'GRO59710')      |                 561|                 100\t\n",
      "('DAI62779', 'FRO80039')      |                 550|                 100\t\n",
      "('DAI75645', 'ELE17451')      |                 547|                 100\t\n",
      "('DAI62779', 'SNA93860')      |                 537|                 100\t\n",
      "('DAI55148', 'DAI62779')      |                 526|                 100\t\n",
      "('DAI43223', 'GRO59710')      |                 512|                 100\t\n",
      "('ELE17451', 'ELE32164')      |                 511|                 100\t\n",
      "('DAI62779', 'SNA18336')      |                 506|                 100\t\n",
      "('ELE32164', 'GRO73461')      |                 486|                 100\t\n",
      "('DAI85309', 'ELE17451')      |                 482|                 100\t\n",
      "('DAI62779', 'FRO78087')      |                 482|                 100\t\n",
      "('DAI62779', 'GRO94758')      |                 479|                 100\t\n",
      "('GRO85051', 'SNA80324')      |                 471|                 100\t\n",
      "('DAI62779', 'GRO21487')      |                 471|                 100\t\n",
      "('ELE17451', 'GRO30386')      |                 468|                 100\t\n",
      "('FRO85978', 'SNA95666')      |                 463|                 100\t\n",
      "('DAI62779', 'FRO19221')      |                 462|                 100\t\n",
      "('DAI62779', 'GRO46854')      |                 461|                 100\t\n",
      "('DAI43223', 'DAI62779')      |                 459|                 100\t\n",
      "('ELE92920', 'SNA18336')      |                 455|                 100\t\n",
      "('DAI88079', 'FRO40251')      |                 446|                 100\t\n",
      "\t\n",
      "========== Most Frequent Triples ==========\t\n",
      "\t\n",
      "PAIR                                              |SUPPORT COUNT       |SUPPORT             \t\n",
      "('DAI75645', 'FRO40251', 'SNA80324')              |                 550|                 100\t\n",
      "('DAI62779', 'FRO40251', 'SNA80324')              |                 476|                 100\t\n",
      "('FRO40251', 'GRO85051', 'SNA80324')              |                 471|                 100\t\n",
      "('DAI62779', 'ELE92920', 'SNA18336')              |                 432|                 100\t\n",
      "('DAI62779', 'DAI75645', 'SNA80324')              |                 421|                 100\t\n",
      "('DAI62779', 'ELE17451', 'SNA80324')              |                 417|                 100\t\n",
      "('DAI62779', 'DAI75645', 'FRO40251')              |                 412|                 100\t\n",
      "('DAI62779', 'ELE17451', 'FRO40251')              |                 406|                 100\t\n",
      "('DAI75645', 'FRO40251', 'GRO85051')              |                 395|                 100\t\n",
      "('DAI62779', 'FRO40251', 'GRO85051')              |                 381|                 100\t\n",
      "('ELE17451', 'FRO40251', 'SNA80324')              |                 353|                 100\t\n",
      "('DAI62779', 'ELE17451', 'ELE92920')              |                 345|                 100\t\n",
      "('FRO40251', 'FRO92469', 'SNA80324')              |                 343|                 100\t\n",
      "('DAI62779', 'DAI85309', 'ELE17451')              |                 339|                 100\t\n",
      "('DAI62779', 'DAI75645', 'ELE17451')              |                 328|                 100\t\n",
      "('DAI62779', 'FRO40251', 'GRO73461')              |                 315|                 100\t\n",
      "('DAI62779', 'ELE32164', 'GRO59710')              |                 301|                 100\t\n",
      "('DAI75645', 'ELE17451', 'SNA80324')              |                 300|                 100\t\n",
      "('DAI75645', 'FRO40251', 'GRO73461')              |                 293|                 100\t\n",
      "('DAI75645', 'ELE17451', 'FRO40251')              |                 292|                 100\t\n",
      "('DAI43223', 'DAI62779', 'ELE32164')              |                 287|                 100\t\n",
      "('DAI43223', 'ELE32164', 'GRO59710')              |                 287|                 100\t\n",
      "('DAI62779', 'ELE17451', 'ELE32164')              |                 277|                 100\t\n",
      "('DAI62779', 'DAI85309', 'ELE99737')              |                 272|                 100\t\n",
      "('DAI62779', 'DAI75645', 'GRO73461')              |                 261|                 100\t\n",
      "('DAI75645', 'FRO40251', 'FRO92469')              |                 251|                 100\t\n",
      "('DAI62779', 'ELE17451', 'GRO73461')              |                 245|                 100\t\n",
      "('DAI62779', 'ELE17451', 'SNA18336')              |                 244|                 100\t\n",
      "('DAI62779', 'FRO40251', 'FRO92469')              |                 238|                 100\t\n",
      "('ELE20847', 'FRO40251', 'SNA80324')              |                 232|                 100\t\n",
      "('FRO40251', 'GRO73461', 'SNA80324')              |                 232|                 100\t\n",
      "('DAI75645', 'GRO73461', 'SNA80324')              |                 230|                 100\t\n",
      "('ELE17451', 'ELE92920', 'SNA18336')              |                 228|                 100\t\n",
      "('DAI43223', 'DAI62779', 'ELE17451')              |                 227|                 100\t\n",
      "('DAI62779', 'ELE17451', 'GRO30386')              |                 218|                 100\t\n",
      "('ELE17451', 'FRO40251', 'GRO85051')              |                 217|                 100\t\n",
      "('DAI62779', 'ELE17451', 'GRO59710')              |                 213|                 100\t\n",
      "('FRO40251', 'FRO92469', 'GRO73461')              |                 211|                 100\t\n",
      "('DAI43223', 'ELE17451', 'ELE32164')              |                 206|                 100\t\n",
      "('DAI62779', 'GRO85051', 'SNA80324')              |                 205|                 100\t\n",
      "('DAI43223', 'DAI62779', 'GRO59710')              |                 205|                 100\t\n",
      "('ELE17451', 'ELE32164', 'GRO59710')              |                 202|                 100\t\n",
      "('DAI62779', 'ELE17451', 'SNA59903')              |                 202|                 100\t\n",
      "('DAI62779', 'GRO73461', 'SNA80324')              |                 198|                 100\t\n",
      "('DAI75645', 'GRO85051', 'SNA80324')              |                 192|                 100\t\n",
      "('DAI62779', 'DAI85309', 'ELE92920')              |                 191|                 100\t\n",
      "('DAI55148', 'DAI62779', 'FRO40251')              |                 189|                 100\t\n",
      "('DAI55148', 'DAI62779', 'SNA80324')              |                 188|                 100\t\n",
      "('DAI62779', 'GRO30386', 'GRO73461')              |                 186|                 100\t\n",
      "('FRO40251', 'GRO21487', 'GRO73461')              |                 182|                 100\t\n"
     ]
    }
   ],
   "source": [
    "!bash wrapperQ34.sh ProductPurchaseData.txt mapperQ37.py reducerQ37.py combinerQ34.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Question 3.8 (Optional)\n",
    "**\n",
    "\n",
    "#### Solution: \n",
    "\n",
    "Here's some text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
