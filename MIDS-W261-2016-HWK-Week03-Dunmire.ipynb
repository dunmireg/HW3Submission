{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 3.0\n",
    "__What is a merge sort? Where is it used in Hadoop?__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The merge sort is a common sorting algorithm. In this design, one defines three references at the front of each array. So one reference at the front of each array with existing values and a reference at the front of the new fully sorted array. Then the algorithm picks the smallest element from the two existing lists and adds it to the temporary array, moving the appropriate references. \n",
    "Hadoop uses merge sort in the shuffling phase. The shuffle phase is divided in three parts: partition, sort, combine. At the shuffle, records are sorted within each partition (more on this in a later question) and merge sort is used to combine the sorted records. This occurs after the mapper has produced its output but before the output gets to the reducer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How is  a combiner function in the context of Hadoop? \n",
    "Give an example where it can be used and justify why it should be used in the context of this problem.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A combiner function is essentially a function that runs in between the mapper and reducer to reduce writing to disk and reduce network traffic. A good example is in a word count with multiple mappers and reducers. Each mapper outputs some tokens and associated counts as key-value pairs. In the reducer, these tokens will be used as keys and the counts added up to produce totals. In this context, a combiner would be a second version of the reducer running before the real reducer. What this will do is aggregate the counts for tokens that exit the mapper but before they are fully output at the reducer. The point here is it means less of the partition is saved to disk by reducing duplication from the mapper as well as there is less communication between the mapper and reducer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What is the Hadoop shuffle?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hadoop shuffle is all the steps taken between the mapper output and the reducer input in a MapReduce job. This includes, partitioning output, sorting the output, combining output (all from the mapper), merging the sorted outputs, sending this to the reducer, mergesort on the partition files, and streaming to the reducer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.1 Use Counters to do EDA (exploratory data analysis and to monitor progress)\n",
    "\n",
    "Counters are lightweight objects in Hadoop that allow you to keep track of system progress in both the map and reduce stages of processing. By default, Hadoop defines a number of standard counters in \"groups\"; these show up in the jobtracker webapp, giving you information such as \"Map input records\", \"Map output records\", etc. \n",
    "\n",
    "While processing information/data using MapReduce job, it is a challenge to monitor the progress of parallel threads running across nodes of distributed clusters. Moreover, it is also complicated to distinguish between the data that has been processed and the data which is yet to be processed. The MapReduce Framework offers a provision of user-defined Counters, which can be effectively utilized to monitor the progress of data across nodes of distributed clusters.\n",
    "\n",
    "Use the Consumer Complaints  Dataset provide here to complete this question:\n",
    "\n",
    "     https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0\n",
    "\n",
    "The consumer complaints dataset consists of diverse consumer complaints, which have been reported across the United States regarding various types of loans. The dataset consists of records of the form:\n",
    "\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    "Here’s is the first few lines of the  of the Consumer Complaints  Dataset:\n",
    "\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    "1114245,Debt collection,Medical,Disclosure verification of debt,Not given enough info to verify debt,FL,32219,Web,11/13/2014,11/13/2014,\"Choice Recovery, Inc.\",Closed with explanation,Yes,\n",
    "\n",
    "1114488,Debt collection,Medical,Disclosure verification of debt,Right to dispute notice not received,TX,75006,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "\n",
    "1114255,Bank account or service,Checking account,Deposits and withdrawals,,NY,11102,Web,11/13/2014,11/13/2014,\"FNIS (Fidelity National Information Services, Inc.)\",In progress,Yes,\n",
    "\n",
    "1115106,Debt collection,\"Other (phone, health club, etc.)\",Communication tactics,Frequent or repeated calls,GA,31721,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "\n",
    "User-defined Counters\n",
    "\n",
    "Now, let’s use Hadoop Counters to identify the number of complaints pertaining to debt collection, mortgage and other categories (all other categories get lumped into this one) in the consumer complaints dataset. Basically produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).\n",
    "\n",
    "Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your  job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "line_num = 0\n",
    "for line in sys.stdin: \n",
    "    if line_num == 0:\n",
    "        line_num += 1\n",
    "        continue\n",
    "    else:\n",
    "        line = line.strip()\n",
    "        line = line.rstrip()\n",
    "        line = line.split(',')\n",
    "        if line[1] == \"Debt collection\":\n",
    "            sys.stderr.write('reporter:counter:Debt-Counter,Total,1\\n')\n",
    "        elif line[1] == 'Mortgage':\n",
    "            sys.stderr.write('reporter:counter:Mortgage-Counter,Total,1\\n')\n",
    "        else:\n",
    "            sys.stderr.write('reporter:counter:Other-Counter,Total,1\\n')\n",
    "        sys.stderr.write(\"reporter:counter:Tokens,Total,1\\n\")\n",
    "        print line[1] + '\\t' + '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "debt_counter = 0\n",
    "mortgage_counter = 0\n",
    "other_counter = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().split('\\t')\n",
    "    if line[0] == \"Debt collection\":\n",
    "        debt_counter +=1 \n",
    "    elif line[0] == 'Mortgage':\n",
    "        mortgage_counter += 1\n",
    "    else:\n",
    "        other_counter += 1\n",
    "print \"Debt collection: \" + str(debt_counter)\n",
    "print \"Mortgage: \" + str(mortgage_counter)\n",
    "print \"Other: \" + str(other_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!cat Consumer_Complaints.csv | python mapper.py | python reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-resourcemanager-Glenns-Air.home.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-nodemanager-Glenns-Air.home.out\n",
      "16/01/29 21:04:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-namenode-Glenns-Air.home.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-datanode-Glenns-Air.home.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-secondarynamenode-Glenns-Air.home.out\n",
      "16/01/29 21:05:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start hadoop yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/29 21:05:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#add input to hdfs\n",
    "!hdfs dfs -put Consumer_Complaints.csv /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/29 21:05:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/29 21:05:34 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/29 21:05:34 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/29 21:05:34 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/29 21:05:35 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/29 21:05:35 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/29 21:05:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1206054309_0001\n",
      "16/01/29 21:05:35 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/29 21:05:35 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/29 21:05:35 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/29 21:05:35 INFO mapreduce.Job: Running job: job_local1206054309_0001\n",
      "16/01/29 21:05:35 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/29 21:05:35 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/29 21:05:35 INFO mapred.LocalJobRunner: Starting task: attempt_local1206054309_0001_m_000000_0\n",
      "16/01/29 21:05:36 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/29 21:05:36 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/29 21:05:36 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/29 21:05:36 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/Consumer_Complaints.csv:0+50906486\n",
      "16/01/29 21:05:36 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/29 21:05:36 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/29 21:05:36 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/29 21:05:36 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/29 21:05:36 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/29 21:05:36 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/29 21:05:36 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/29 21:05:36 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./mapper.py]\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/29 21:05:36 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:36 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:36 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:36 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:36 INFO streaming.PipeMapRed: Records R/W=2032/1\n",
      "16/01/29 21:05:36 INFO streaming.PipeMapRed: R/W/S=10000/7896/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:36 INFO mapreduce.Job: Job job_local1206054309_0001 running in uber mode : false\n",
      "16/01/29 21:05:36 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/29 21:05:37 INFO streaming.PipeMapRed: R/W/S=100000/98646/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:37 INFO streaming.PipeMapRed: R/W/S=200000/198558/0 in:200000=200000/1 [rec/s] out:198558=198558/1 [rec/s]\n",
      "16/01/29 21:05:38 INFO streaming.PipeMapRed: R/W/S=300000/298306/0 in:150000=300000/2 [rec/s] out:149153=298306/2 [rec/s]\n",
      "16/01/29 21:05:38 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/29 21:05:38 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/29 21:05:38 INFO mapred.LocalJobRunner: \n",
      "16/01/29 21:05:38 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/29 21:05:38 INFO mapred.MapTask: Spilling map output\n",
      "16/01/29 21:05:38 INFO mapred.MapTask: bufstart = 0; bufend = 4878322; bufvoid = 104857600\n",
      "16/01/29 21:05:38 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24962748(99850992); length = 1251649/6553600\n",
      "16/01/29 21:05:38 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/29 21:05:38 INFO mapred.Task: Task:attempt_local1206054309_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/29 21:05:38 INFO mapred.LocalJobRunner: Records R/W=2032/1\n",
      "16/01/29 21:05:38 INFO mapred.Task: Task 'attempt_local1206054309_0001_m_000000_0' done.\n",
      "16/01/29 21:05:38 INFO mapred.LocalJobRunner: Finishing task: attempt_local1206054309_0001_m_000000_0\n",
      "16/01/29 21:05:38 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/29 21:05:38 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/29 21:05:38 INFO mapred.LocalJobRunner: Starting task: attempt_local1206054309_0001_r_000000_0\n",
      "16/01/29 21:05:38 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/29 21:05:38 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/29 21:05:38 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/29 21:05:38 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2102fea2\n",
      "16/01/29 21:05:38 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/29 21:05:38 INFO reduce.EventFetcher: attempt_local1206054309_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/29 21:05:38 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1206054309_0001_m_000000_0 decomp: 5504150 len: 5504154 to MEMORY\n",
      "16/01/29 21:05:38 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/29 21:05:38 INFO reduce.InMemoryMapOutput: Read 5504150 bytes from map-output for attempt_local1206054309_0001_m_000000_0\n",
      "16/01/29 21:05:38 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 5504150, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->5504150\n",
      "16/01/29 21:05:38 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/29 21:05:38 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/29 21:05:38 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/29 21:05:38 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/29 21:05:38 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 5504124 bytes\n",
      "16/01/29 21:05:39 INFO reduce.MergeManagerImpl: Merged 1 segments, 5504150 bytes to disk to satisfy reduce memory limit\n",
      "16/01/29 21:05:39 INFO reduce.MergeManagerImpl: Merging 1 files, 5504154 bytes from disk\n",
      "16/01/29 21:05:39 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/29 21:05:39 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/29 21:05:39 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 5504124 bytes\n",
      "16/01/29 21:05:39 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./reducer.py]\n",
      "16/01/29 21:05:39 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/29 21:05:39 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: Records R/W=312913/1\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/29 21:05:39 INFO mapred.Task: Task:attempt_local1206054309_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/29 21:05:39 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/29 21:05:39 INFO mapred.Task: Task attempt_local1206054309_0001_r_000000_0 is allowed to commit now\n",
      "16/01/29 21:05:39 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1206054309_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/consumer_counters/_temporary/0/task_local1206054309_0001_r_000000\n",
      "16/01/29 21:05:39 INFO mapred.LocalJobRunner: Records R/W=312913/1 > reduce\n",
      "16/01/29 21:05:39 INFO mapred.Task: Task 'attempt_local1206054309_0001_r_000000_0' done.\n",
      "16/01/29 21:05:39 INFO mapred.LocalJobRunner: Finishing task: attempt_local1206054309_0001_r_000000_0\n",
      "16/01/29 21:05:39 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/29 21:05:39 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/29 21:05:39 INFO mapreduce.Job: Job job_local1206054309_0001 completed successfully\n",
      "16/01/29 21:05:39 INFO mapreduce.Job: Counters: 39\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=11220438\n",
      "\t\tFILE: Number of bytes written=17314316\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=101812972\n",
      "\t\tHDFS: Number of bytes written=57\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=312913\n",
      "\t\tMap output bytes=4878322\n",
      "\t\tMap output materialized bytes=5504154\n",
      "\t\tInput split bytes=111\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10\n",
      "\t\tReduce shuffle bytes=5504154\n",
      "\t\tReduce input records=312913\n",
      "\t\tReduce output records=3\n",
      "\t\tSpilled Records=625826\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=9\n",
      "\t\tTotal committed heap usage (bytes)=671088640\n",
      "\tDebt-Counter\n",
      "\t\tTotal=44372\n",
      "\tMortgage-Counter\n",
      "\t\tTotal=125752\n",
      "\tOther-Counter\n",
      "\t\tTotal=142789\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tTokens\n",
      "\t\tTotal=312913\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=57\n",
      "16/01/29 21:05:39 INFO streaming.StreamJob: Output directory: consumer_counters\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input Consumer_Complaints.csv \\\n",
    "-output consumer_counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/29 21:06:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Debt collection: 44372\t\n",
      "Mortgage: 125752\t\n",
      "Other: 142789\t\n"
     ]
    }
   ],
   "source": [
    "#show results\n",
    "!hdfs dfs -cat /user/dunmireg/consumer_counters/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting historyserver, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/mapred-dunmireg-historyserver-Glenns-Air.home.out\r\n"
     ]
    }
   ],
   "source": [
    "#Start history server to check counter\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/mr-jobhistory-daemon.sh --config /usr/local/Cellar/hadoop/2.7.1/libexec/etc/hadoop start historyserver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping historyserver\r\n"
     ]
    }
   ],
   "source": [
    "#Stop history server\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/mr-jobhistory-daemon.sh --config /usr/local/Cellar/hadoop/2.7.1/libexec/etc/hadoop stop historyserver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this link to view job tracker\n",
    "\n",
    "http://192.168.0.10:19888/jobhistory/app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/29 18:22:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/29 18:22:50 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
      "16/01/29 18:22:50 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "#move output to local directory\n",
    "#!hadoop fs -copyToLocal /user/dunmireg/consumer_counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/29 21:09:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/29 21:09:45 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/Consumer_Complaints.csv\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/29 21:09:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/29 21:09:47 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/consumer_counters\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "localhost: nodemanager did not stop gracefully after 5 seconds: killing with kill -9\n",
      "no proxyserver to stop\n",
      "16/01/29 21:09:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/29 21:10:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Remove output directory and stop yarn and hdfs\n",
    "!hadoop fs -rmr /user/dunmireg/Consumer_Complaints.csv\n",
    "!hadoop fs -rmr /user/dunmireg/consumer_counters\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### HW 3.2 Analyze the performance of your Mappers, Combiners and Reducers using Counters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Part 1__\n",
    "\n",
    "For this brief study the Input file will be one record (the next line only): \n",
    "foo foo quux labs foo bar quux\n",
    "\n",
    "\n",
    "Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively. Please explain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('input_text.txt', 'w') as myfile:\n",
    "    myfile.write('foo foo quux labs foo bar quux')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "sys.stderr.write('reporter:counter:Map-Count,Total,1\\n')\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    line = line.rstrip()\n",
    "    line = line.split()\n",
    "    for word in line:\n",
    "        print word + '\\t' + '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = None\n",
    "word = None\n",
    "sys.stderr.write('reporter:counter:Reduce-Counter,Total,1\\n')\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    line = line.rstrip()\n",
    "    word, count = line.split('\\t')\n",
    "    \n",
    "    if current_word == word:\n",
    "        current_count += int(count)\n",
    "    else:\n",
    "        if current_word:\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_word = word\n",
    "        current_count = int(count)\n",
    "\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!cat input.txt | python mapper.py | sort | python reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-resourcemanager-Glenns-Air.home.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-nodemanager-Glenns-Air.home.out\n",
      "16/02/01 22:31:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-namenode-Glenns-Air.home.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-datanode-Glenns-Air.home.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-secondarynamenode-Glenns-Air.home.out\n",
      "16/02/01 22:31:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start hadoop yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 22:31:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#add input to hdfs\n",
    "!hdfs dfs -put input_text.txt /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 22:33:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 22:33:49 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 22:33:49 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 22:33:49 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 22:33:49 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 22:33:49 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 22:33:49 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 22:33:49 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/01 22:33:49 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local927319060_0001\n",
      "16/02/01 22:33:49 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 22:33:49 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 22:33:49 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 22:33:49 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 22:33:49 INFO mapreduce.Job: Running job: job_local927319060_0001\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Starting task: attempt_local927319060_0001_m_000000_0\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 22:33:50 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 22:33:50 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 22:33:50 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 22:33:50 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/input_text.txt:0+30\n",
      "16/02/01 22:33:50 INFO mapred.MapTask: numReduceTasks: 4\n",
      "16/02/01 22:33:50 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 22:33:50 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 22:33:50 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 22:33:50 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 22:33:50 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 22:33:50 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./mapper.py]\n",
      "16/02/01 22:33:50 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 22:33:50 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 22:33:50 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 22:33:50 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 22:33:50 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 22:33:50 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 22:33:50 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 22:33:50 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 22:33:50 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 22:33:50 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 22:33:50 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 22:33:50 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: \n",
      "16/02/01 22:33:50 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 22:33:50 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 22:33:50 INFO mapred.MapTask: bufstart = 0; bufend = 45; bufvoid = 104857600\n",
      "16/02/01 22:33:50 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214372(104857488); length = 25/6553600\n",
      "16/02/01 22:33:50 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 22:33:50 INFO mapred.Task: Task:attempt_local927319060_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
      "16/02/01 22:33:50 INFO mapred.Task: Task 'attempt_local927319060_0001_m_000000_0' done.\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Finishing task: attempt_local927319060_0001_m_000000_0\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Starting task: attempt_local927319060_0001_r_000000_0\n",
      "16/02/01 22:33:50 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 22:33:50 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 22:33:50 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 22:33:50 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@347fc133\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 22:33:50 INFO reduce.EventFetcher: attempt_local927319060_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 22:33:50 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local927319060_0001_m_000000_0 decomp: 20 len: 24 to MEMORY\n",
      "16/02/01 22:33:50 INFO reduce.InMemoryMapOutput: Read 20 bytes from map-output for attempt_local927319060_0001_m_000000_0\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 20, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->20\n",
      "16/02/01 22:33:50 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 13 bytes\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: Merged 1 segments, 20 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: Merging 1 files, 24 bytes from disk\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 13 bytes\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./reducer.py]\n",
      "16/02/01 22:33:50 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 22:33:50 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: Records R/W=2/1\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 22:33:50 INFO mapred.Task: Task:attempt_local927319060_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:33:50 INFO mapred.Task: Task attempt_local927319060_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 22:33:50 INFO output.FileOutputCommitter: Saved output of task 'attempt_local927319060_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/word_count/_temporary/0/task_local927319060_0001_r_000000\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Records R/W=2/1 > reduce\n",
      "16/02/01 22:33:50 INFO mapred.Task: Task 'attempt_local927319060_0001_r_000000_0' done.\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Finishing task: attempt_local927319060_0001_r_000000_0\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Starting task: attempt_local927319060_0001_r_000001_0\n",
      "16/02/01 22:33:50 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 22:33:50 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 22:33:50 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 22:33:50 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@34751e75\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 22:33:50 INFO reduce.EventFetcher: attempt_local927319060_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 22:33:50 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local927319060_0001_m_000000_0 decomp: 26 len: 30 to MEMORY\n",
      "16/02/01 22:33:50 INFO reduce.InMemoryMapOutput: Read 26 bytes from map-output for attempt_local927319060_0001_m_000000_0\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 26, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->26\n",
      "16/02/01 22:33:50 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 20 bytes\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: Merged 1 segments, 26 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: Merging 1 files, 30 bytes from disk\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 20 bytes\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./reducer.py]\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: Records R/W=3/1\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 22:33:50 INFO mapred.Task: Task:attempt_local927319060_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:33:50 INFO mapred.Task: Task attempt_local927319060_0001_r_000001_0 is allowed to commit now\n",
      "16/02/01 22:33:50 INFO output.FileOutputCommitter: Saved output of task 'attempt_local927319060_0001_r_000001_0' to hdfs://localhost:9000/user/dunmireg/word_count/_temporary/0/task_local927319060_0001_r_000001\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Records R/W=3/1 > reduce\n",
      "16/02/01 22:33:50 INFO mapred.Task: Task 'attempt_local927319060_0001_r_000001_0' done.\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Finishing task: attempt_local927319060_0001_r_000001_0\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Starting task: attempt_local927319060_0001_r_000002_0\n",
      "16/02/01 22:33:50 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 22:33:50 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 22:33:50 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 22:33:50 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4e432ccb\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 22:33:50 INFO reduce.EventFetcher: attempt_local927319060_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 22:33:50 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local927319060_0001_m_000000_0 decomp: 10 len: 14 to MEMORY\n",
      "16/02/01 22:33:50 INFO reduce.InMemoryMapOutput: Read 10 bytes from map-output for attempt_local927319060_0001_m_000000_0\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->10\n",
      "16/02/01 22:33:50 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4 bytes\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: Merged 1 segments, 10 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: Merging 1 files, 14 bytes from disk\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4 bytes\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./reducer.py]\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 22:33:50 INFO mapred.Task: Task:attempt_local927319060_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:33:50 INFO mapred.Task: Task attempt_local927319060_0001_r_000002_0 is allowed to commit now\n",
      "16/02/01 22:33:50 INFO output.FileOutputCommitter: Saved output of task 'attempt_local927319060_0001_r_000002_0' to hdfs://localhost:9000/user/dunmireg/word_count/_temporary/0/task_local927319060_0001_r_000002\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Records R/W=1/1 > reduce\n",
      "16/02/01 22:33:50 INFO mapred.Task: Task 'attempt_local927319060_0001_r_000002_0' done.\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Finishing task: attempt_local927319060_0001_r_000002_0\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Starting task: attempt_local927319060_0001_r_000003_0\n",
      "16/02/01 22:33:50 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 22:33:50 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 22:33:50 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 22:33:50 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@15760f0b\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 22:33:50 INFO reduce.EventFetcher: attempt_local927319060_0001_r_000003_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 22:33:50 INFO reduce.LocalFetcher: localfetcher#4 about to shuffle output of map attempt_local927319060_0001_m_000000_0 decomp: 11 len: 15 to MEMORY\n",
      "16/02/01 22:33:50 INFO reduce.InMemoryMapOutput: Read 11 bytes from map-output for attempt_local927319060_0001_m_000000_0\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 11, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->11\n",
      "16/02/01 22:33:50 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4 bytes\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: Merged 1 segments, 11 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: Merging 1 files, 15 bytes from disk\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4 bytes\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./reducer.py]\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 22:33:50 INFO mapred.Task: Task:attempt_local927319060_0001_r_000003_0 is done. And is in the process of committing\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:33:50 INFO mapred.Task: Task attempt_local927319060_0001_r_000003_0 is allowed to commit now\n",
      "16/02/01 22:33:50 INFO output.FileOutputCommitter: Saved output of task 'attempt_local927319060_0001_r_000003_0' to hdfs://localhost:9000/user/dunmireg/word_count/_temporary/0/task_local927319060_0001_r_000003\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Records R/W=1/1 > reduce\n",
      "16/02/01 22:33:50 INFO mapred.Task: Task 'attempt_local927319060_0001_r_000003_0' done.\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Finishing task: attempt_local927319060_0001_r_000003_0\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 22:33:50 INFO mapreduce.Job: Job job_local927319060_0001 running in uber mode : false\n",
      "16/02/01 22:33:50 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 22:33:50 INFO mapreduce.Job: Job job_local927319060_0001 completed successfully\n",
      "16/02/01 22:33:51 INFO mapreduce.Job: Counters: 37\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=532031\n",
      "\t\tFILE: Number of bytes written=1998924\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=150\n",
      "\t\tHDFS: Number of bytes written=65\n",
      "\t\tHDFS: Number of read operations=55\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=25\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=7\n",
      "\t\tMap output bytes=45\n",
      "\t\tMap output materialized bytes=83\n",
      "\t\tInput split bytes=102\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=83\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=7\n",
      "\t\tTotal committed heap usage (bytes)=1551892480\n",
      "\tMap-Count\n",
      "\t\tTotal=1\n",
      "\tReduce-Counter\n",
      "\t\tTotal=4\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=30\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=26\n",
      "16/02/01 22:33:51 INFO streaming.StreamJob: Output directory: word_count\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.map.tasks=1 \\\n",
    "-D mapred.reduce.tasks=4 \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input input_text.txt \\\n",
    "-output word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 22:34:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 22:34:08 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/input_text.txt\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 22:34:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 22:34:10 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/word_count\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/02/01 22:34:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/02/01 22:34:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Remove output directory and stop yarn and hdfs\n",
    "!hadoop fs -rmr /user/dunmireg/input_text.txt\n",
    "!hadoop fs -rmr /user/dunmireg/word_count\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Part 2__\n",
    "\n",
    "Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "from csv import reader\n",
    "\n",
    "#Structure of complaints\n",
    "#Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,\n",
    "#Company,Company response,Timely response?,Consumer disputed?\n",
    "line_num = 0\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "sys.stderr.write('reporter:counter:Map-Count,Total,1\\n')\n",
    "for line in reader(sys.stdin):\n",
    "    if line_num == 0:\n",
    "        line_num += 1\n",
    "        continue\n",
    "    else:\n",
    "        issue = line[3]\n",
    "        if issue == '':\n",
    "            issue = 'Blank'\n",
    "        words = re.findall(WORD_RE, issue)\n",
    "        for word in words:\n",
    "            print word.lower() + '\\t' + str(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = None\n",
    "word = None\n",
    "sys.stderr.write('reporter:counter:Reduce-Counter,Total,1\\n')\n",
    "for line in sys.stdin:\n",
    "    #issue, count = line.split('\\t')\n",
    "    line = line.split('\\t')\n",
    "    word = line[0]\n",
    "    count = int(line[1])\n",
    "    \n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_word = word\n",
    "        current_count = count\n",
    "\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!cat Consumer_Complaints.csv | python mapper.py | sort | python reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-resourcemanager-Glenns-Air.home.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-nodemanager-Glenns-Air.home.out\n",
      "16/02/01 17:40:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-namenode-Glenns-Air.home.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-datanode-Glenns-Air.home.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-secondarynamenode-Glenns-Air.home.out\n",
      "16/02/01 17:40:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start hadoop yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 17:40:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#add input to hdfs\n",
    "!hdfs dfs -put Consumer_Complaints.csv /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 17:40:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 17:40:31 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 17:40:31 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 17:40:31 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 17:40:31 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 17:40:31 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 17:40:32 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 17:40:32 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/01 17:40:32 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local161210426_0001\n",
      "16/02/01 17:40:32 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 17:40:32 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 17:40:32 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 17:40:32 INFO mapreduce.Job: Running job: job_local161210426_0001\n",
      "16/02/01 17:40:32 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 17:40:32 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 17:40:32 INFO mapred.LocalJobRunner: Starting task: attempt_local161210426_0001_m_000000_0\n",
      "16/02/01 17:40:32 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 17:40:32 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 17:40:32 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 17:40:32 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/Consumer_Complaints.csv:0+50906486\n",
      "16/02/01 17:40:32 INFO mapred.MapTask: numReduceTasks: 2\n",
      "16/02/01 17:40:32 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 17:40:32 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 17:40:32 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 17:40:32 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 17:40:32 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 17:40:32 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 17:40:32 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./mapper.py]\n",
      "16/02/01 17:40:32 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 17:40:32 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 17:40:32 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 17:40:32 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 17:40:32 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 17:40:32 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 17:40:32 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 17:40:32 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 17:40:32 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 17:40:32 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 17:40:32 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 17:40:32 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 17:40:32 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:40:32 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:40:32 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:40:33 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:40:33 INFO streaming.PipeMapRed: Records R/W=1400/1\n",
      "16/02/01 17:40:33 INFO streaming.PipeMapRed: R/W/S=10000/40775/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:40:33 INFO mapreduce.Job: Job job_local161210426_0001 running in uber mode : false\n",
      "16/02/01 17:40:33 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/01 17:40:34 INFO streaming.PipeMapRed: R/W/S=100000/448120/0 in:100000=100000/1 [rec/s] out:448120=448120/1 [rec/s]\n",
      "16/02/01 17:40:35 INFO streaming.PipeMapRed: R/W/S=200000/884534/0 in:100000=200000/2 [rec/s] out:442267=884534/2 [rec/s]\n",
      "16/02/01 17:40:36 INFO streaming.PipeMapRed: R/W/S=300000/1296452/0 in:100000=300000/3 [rec/s] out:432150=1296452/3 [rec/s]\n",
      "16/02/01 17:40:36 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 17:40:36 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 17:40:36 INFO mapred.LocalJobRunner: \n",
      "16/02/01 17:40:36 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 17:40:36 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 17:40:36 INFO mapred.MapTask: bufstart = 0; bufend = 13424739; bufvoid = 104857600\n",
      "16/02/01 17:40:36 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 20821152(83284608); length = 5393245/6553600\n",
      "16/02/01 17:40:37 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 17:40:37 INFO mapred.Task: Task:attempt_local161210426_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 17:40:37 INFO mapred.LocalJobRunner: Records R/W=1400/1\n",
      "16/02/01 17:40:37 INFO mapred.Task: Task 'attempt_local161210426_0001_m_000000_0' done.\n",
      "16/02/01 17:40:37 INFO mapred.LocalJobRunner: Finishing task: attempt_local161210426_0001_m_000000_0\n",
      "16/02/01 17:40:37 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 17:40:37 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 17:40:37 INFO mapred.LocalJobRunner: Starting task: attempt_local161210426_0001_r_000000_0\n",
      "16/02/01 17:40:37 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 17:40:37 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 17:40:37 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 17:40:37 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1472699b\n",
      "16/02/01 17:40:38 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 17:40:38 INFO reduce.EventFetcher: attempt_local161210426_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 17:40:38 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local161210426_0001_m_000000_0 decomp: 7798408 len: 7798412 to MEMORY\n",
      "16/02/01 17:40:38 INFO reduce.InMemoryMapOutput: Read 7798408 bytes from map-output for attempt_local161210426_0001_m_000000_0\n",
      "16/02/01 17:40:38 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 7798408, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->7798408\n",
      "16/02/01 17:40:38 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 17:40:38 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 17:40:38 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 17:40:38 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 17:40:38 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 7798404 bytes\n",
      "16/02/01 17:40:38 INFO reduce.MergeManagerImpl: Merged 1 segments, 7798408 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 17:40:38 INFO reduce.MergeManagerImpl: Merging 1 files, 7798412 bytes from disk\n",
      "16/02/01 17:40:38 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 17:40:38 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 17:40:38 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 7798404 bytes\n",
      "16/02/01 17:40:38 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 17:40:38 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./reducer.py]\n",
      "16/02/01 17:40:38 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 17:40:38 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 17:40:38 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:40:38 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:40:38 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:40:38 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:40:38 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:40:38 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/01 17:40:38 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:40:38 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:40:38 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:40:39 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:40:39 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:40:39 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:600000=600000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 17:40:39 INFO streaming.PipeMapRed: Records R/W=687448/1\n",
      "16/02/01 17:40:39 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 17:40:39 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 17:40:39 INFO mapred.Task: Task:attempt_local161210426_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 17:40:39 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 17:40:39 INFO mapred.Task: Task attempt_local161210426_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 17:40:39 INFO output.FileOutputCommitter: Saved output of task 'attempt_local161210426_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/word_count/_temporary/0/task_local161210426_0001_r_000000\n",
      "16/02/01 17:40:39 INFO mapred.LocalJobRunner: Records R/W=687448/1 > reduce\n",
      "16/02/01 17:40:39 INFO mapred.Task: Task 'attempt_local161210426_0001_r_000000_0' done.\n",
      "16/02/01 17:40:39 INFO mapred.LocalJobRunner: Finishing task: attempt_local161210426_0001_r_000000_0\n",
      "16/02/01 17:40:39 INFO mapred.LocalJobRunner: Starting task: attempt_local161210426_0001_r_000001_0\n",
      "16/02/01 17:40:39 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 17:40:39 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 17:40:39 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 17:40:39 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@365b4cb3\n",
      "16/02/01 17:40:39 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 17:40:39 INFO reduce.EventFetcher: attempt_local161210426_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 17:40:39 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local161210426_0001_m_000000_0 decomp: 8322959 len: 8322963 to MEMORY\n",
      "16/02/01 17:40:39 INFO reduce.InMemoryMapOutput: Read 8322959 bytes from map-output for attempt_local161210426_0001_m_000000_0\n",
      "16/02/01 17:40:39 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 8322959, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->8322959\n",
      "16/02/01 17:40:39 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 17:40:39 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 17:40:39 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 17:40:39 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 17:40:39 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 8322950 bytes\n",
      "16/02/01 17:40:39 INFO reduce.MergeManagerImpl: Merged 1 segments, 8322959 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 17:40:39 INFO reduce.MergeManagerImpl: Merging 1 files, 8322963 bytes from disk\n",
      "16/02/01 17:40:39 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 17:40:39 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 17:40:39 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 8322950 bytes\n",
      "16/02/01 17:40:39 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 17:40:39 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./reducer.py]\n",
      "16/02/01 17:40:40 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:40:40 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:40:40 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:40:40 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:40:40 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:40:40 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:40:40 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:40:40 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:40:40 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/02/01 17:40:40 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:40:40 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:40:40 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:40:41 INFO streaming.PipeMapRed: Records R/W=660864/1\n",
      "16/02/01 17:40:41 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 17:40:41 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 17:40:41 INFO mapred.Task: Task:attempt_local161210426_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/01 17:40:41 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 17:40:41 INFO mapred.Task: Task attempt_local161210426_0001_r_000001_0 is allowed to commit now\n",
      "16/02/01 17:40:41 INFO output.FileOutputCommitter: Saved output of task 'attempt_local161210426_0001_r_000001_0' to hdfs://localhost:9000/user/dunmireg/word_count/_temporary/0/task_local161210426_0001_r_000001\n",
      "16/02/01 17:40:41 INFO mapred.LocalJobRunner: Records R/W=660864/1 > reduce\n",
      "16/02/01 17:40:41 INFO mapred.Task: Task 'attempt_local161210426_0001_r_000001_0' done.\n",
      "16/02/01 17:40:41 INFO mapred.LocalJobRunner: Finishing task: attempt_local161210426_0001_r_000001_0\n",
      "16/02/01 17:40:41 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 17:40:41 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 17:40:41 INFO mapreduce.Job: Job job_local161210426_0001 completed successfully\n",
      "16/02/01 17:40:41 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=48166073\n",
      "\t\tFILE: Number of bytes written=73482829\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=152719458\n",
      "\t\tHDFS: Number of bytes written=3213\n",
      "\t\tHDFS: Number of read operations=24\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=1348312\n",
      "\t\tMap output bytes=13424739\n",
      "\t\tMap output materialized bytes=16121375\n",
      "\t\tInput split bytes=111\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=175\n",
      "\t\tReduce shuffle bytes=16121375\n",
      "\t\tReduce input records=1348312\n",
      "\t\tReduce output records=175\n",
      "\t\tSpilled Records=2696624\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=4\n",
      "\t\tTotal committed heap usage (bytes)=819462144\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2182\n",
      "16/02/01 17:40:41 INFO streaming.StreamJob: Output directory: word_count\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.reduce.tasks=2 \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input Consumer_Complaints.csv \\\n",
    "-output word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show results\n",
    "#!hdfs dfs -cat /user/dunmireg/word_count/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 17:41:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 17:41:11 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
      "16/02/01 17:41:11 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
      "16/02/01 17:41:11 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "#move output to local directory\n",
    "!hadoop fs -copyToLocal /user/dunmireg/word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 17:41:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 17:41:14 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/Consumer_Complaints.csv\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 17:41:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 17:41:16 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/word_count\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/02/01 17:41:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/02/01 17:41:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Remove output directory and stop yarn and hdfs\n",
    "!hadoop fs -rmr /user/dunmireg/Consumer_Complaints.csv\n",
    "!hadoop fs -rmr /user/dunmireg/word_count\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Part 3__\n",
    "\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = None\n",
    "word = None\n",
    "sys.stderr.write('reporter:counter:Combiner-Counter,Total,1\\n')\n",
    "for line in sys.stdin:\n",
    "    #issue, count = line.split('\\t')\n",
    "    line = line.split('\\t')\n",
    "    word = line[0]\n",
    "    count = int(line[1])\n",
    "    \n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_word = word\n",
    "        current_count = count\n",
    "\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-resourcemanager-Glenns-Air.home.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-nodemanager-Glenns-Air.home.out\n",
      "16/02/01 22:35:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-namenode-Glenns-Air.home.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-datanode-Glenns-Air.home.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-secondarynamenode-Glenns-Air.home.out\n",
      "16/02/01 22:36:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start hadoop yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 22:36:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#add input to hdfs\n",
    "!hdfs dfs -put Consumer_Complaints.csv /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 22:36:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 22:36:51 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 22:36:51 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 22:36:51 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 22:36:51 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 22:36:51 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 22:36:52 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 22:36:52 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/01 22:36:52 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1588555268_0001\n",
      "16/02/01 22:36:52 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 22:36:52 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 22:36:52 INFO mapreduce.Job: Running job: job_local1588555268_0001\n",
      "16/02/01 22:36:52 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 22:36:52 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 22:36:52 INFO mapred.LocalJobRunner: Starting task: attempt_local1588555268_0001_m_000000_0\n",
      "16/02/01 22:36:52 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 22:36:52 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 22:36:52 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 22:36:52 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 22:36:52 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/Consumer_Complaints.csv:0+50906486\n",
      "16/02/01 22:36:52 INFO mapred.MapTask: numReduceTasks: 2\n",
      "16/02/01 22:36:52 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 22:36:52 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 22:36:52 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 22:36:52 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 22:36:52 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 22:36:52 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 22:36:52 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./mapper.py]\n",
      "16/02/01 22:36:52 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 22:36:52 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 22:36:52 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 22:36:52 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 22:36:52 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 22:36:52 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 22:36:52 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 22:36:52 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 22:36:52 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 22:36:52 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 22:36:52 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 22:36:52 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 22:36:52 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:52 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:52 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:52 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:52 INFO streaming.PipeMapRed: Records R/W=1043/1\n",
      "16/02/01 22:36:52 INFO streaming.PipeMapRed: R/W/S=10000/39415/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:53 INFO mapreduce.Job: Job job_local1588555268_0001 running in uber mode : false\n",
      "16/02/01 22:36:53 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/01 22:36:54 INFO streaming.PipeMapRed: R/W/S=100000/448120/0 in:100000=100000/1 [rec/s] out:448120=448120/1 [rec/s]\n",
      "16/02/01 22:36:55 INFO streaming.PipeMapRed: R/W/S=200000/884534/0 in:100000=200000/2 [rec/s] out:442267=884534/2 [rec/s]\n",
      "16/02/01 22:36:56 INFO streaming.PipeMapRed: R/W/S=300000/1296452/0 in:100000=300000/3 [rec/s] out:432150=1296452/3 [rec/s]\n",
      "16/02/01 22:36:56 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 22:36:56 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 22:36:56 INFO mapred.LocalJobRunner: \n",
      "16/02/01 22:36:56 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 22:36:56 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 22:36:56 INFO mapred.MapTask: bufstart = 0; bufend = 13424739; bufvoid = 104857600\n",
      "16/02/01 22:36:56 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 20821152(83284608); length = 5393245/6553600\n",
      "16/02/01 22:36:57 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./combiner.py]\n",
      "16/02/01 22:36:57 INFO Configuration.deprecation: mapred.skip.map.auto.incr.proc.count is deprecated. Instead, use mapreduce.map.skip.proc-count.auto-incr\n",
      "16/02/01 22:36:57 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:57 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:57 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:57 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:57 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:57 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:57 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:57 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:57 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:57 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:57 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:57 INFO streaming.PipeMapRed: Records R/W=687448/1\n",
      "16/02/01 22:36:57 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 22:36:57 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 22:36:58 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./combiner.py]\n",
      "16/02/01 22:36:58 INFO Configuration.deprecation: mapred.skip.reduce.auto.incr.proc.count is deprecated. Instead, use mapreduce.reduce.skip.proc-count.auto-incr\n",
      "16/02/01 22:36:58 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:58 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:58 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:58 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:58 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:58 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:58 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:58 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:58 INFO mapred.LocalJobRunner: Records R/W=687448/1 > sort\n",
      "16/02/01 22:36:58 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:58 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:600000=600000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 22:36:59 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: Records R/W=660864/1\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 22:36:59 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 22:36:59 INFO mapred.Task: Task:attempt_local1588555268_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: Records R/W=660864/1\n",
      "16/02/01 22:36:59 INFO mapred.Task: Task 'attempt_local1588555268_0001_m_000000_0' done.\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: Finishing task: attempt_local1588555268_0001_m_000000_0\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: Starting task: attempt_local1588555268_0001_r_000000_0\n",
      "16/02/01 22:36:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 22:36:59 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 22:36:59 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 22:36:59 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@448f7373\n",
      "16/02/01 22:36:59 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 22:36:59 INFO reduce.EventFetcher: attempt_local1588555268_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 22:36:59 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1588555268_0001_m_000000_0 decomp: 1201 len: 1205 to MEMORY\n",
      "16/02/01 22:36:59 INFO reduce.InMemoryMapOutput: Read 1201 bytes from map-output for attempt_local1588555268_0001_m_000000_0\n",
      "16/02/01 22:36:59 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1201, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1201\n",
      "16/02/01 22:36:59 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:36:59 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 22:36:59 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 22:36:59 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1197 bytes\n",
      "16/02/01 22:36:59 INFO reduce.MergeManagerImpl: Merged 1 segments, 1201 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 22:36:59 INFO reduce.MergeManagerImpl: Merging 1 files, 1205 bytes from disk\n",
      "16/02/01 22:36:59 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 22:36:59 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 22:36:59 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1197 bytes\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./reducer.py]\n",
      "16/02/01 22:36:59 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 22:36:59 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: Records R/W=84/1\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 22:36:59 INFO mapred.Task: Task:attempt_local1588555268_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:36:59 INFO mapred.Task: Task attempt_local1588555268_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 22:36:59 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1588555268_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/word_count/_temporary/0/task_local1588555268_0001_r_000000\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: Records R/W=84/1 > reduce\n",
      "16/02/01 22:36:59 INFO mapred.Task: Task 'attempt_local1588555268_0001_r_000000_0' done.\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: Finishing task: attempt_local1588555268_0001_r_000000_0\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: Starting task: attempt_local1588555268_0001_r_000001_0\n",
      "16/02/01 22:36:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 22:36:59 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 22:36:59 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 22:36:59 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@71770ed3\n",
      "16/02/01 22:36:59 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 22:36:59 INFO reduce.EventFetcher: attempt_local1588555268_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 22:36:59 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local1588555268_0001_m_000000_0 decomp: 1335 len: 1339 to MEMORY\n",
      "16/02/01 22:36:59 INFO reduce.InMemoryMapOutput: Read 1335 bytes from map-output for attempt_local1588555268_0001_m_000000_0\n",
      "16/02/01 22:36:59 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1335, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1335\n",
      "16/02/01 22:36:59 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:36:59 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 22:36:59 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 22:36:59 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1326 bytes\n",
      "16/02/01 22:36:59 INFO reduce.MergeManagerImpl: Merged 1 segments, 1335 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 22:36:59 INFO reduce.MergeManagerImpl: Merging 1 files, 1339 bytes from disk\n",
      "16/02/01 22:36:59 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 22:36:59 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 22:36:59 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1326 bytes\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./reducer.py]\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: Records R/W=91/1\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 22:36:59 INFO mapred.Task: Task:attempt_local1588555268_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:36:59 INFO mapred.Task: Task attempt_local1588555268_0001_r_000001_0 is allowed to commit now\n",
      "16/02/01 22:36:59 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1588555268_0001_r_000001_0' to hdfs://localhost:9000/user/dunmireg/word_count/_temporary/0/task_local1588555268_0001_r_000001\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: Records R/W=91/1 > reduce\n",
      "16/02/01 22:36:59 INFO mapred.Task: Task 'attempt_local1588555268_0001_r_000001_0' done.\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: Finishing task: attempt_local1588555268_0001_r_000001_0\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 22:37:00 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 22:37:00 INFO mapreduce.Job: Job job_local1588555268_0001 completed successfully\n",
      "16/02/01 22:37:00 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=328491\n",
      "\t\tFILE: Number of bytes written=1217204\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=152719458\n",
      "\t\tHDFS: Number of bytes written=3213\n",
      "\t\tHDFS: Number of read operations=24\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=1348312\n",
      "\t\tMap output bytes=13424739\n",
      "\t\tMap output materialized bytes=2544\n",
      "\t\tInput split bytes=111\n",
      "\t\tCombine input records=1348312\n",
      "\t\tCombine output records=175\n",
      "\t\tReduce input groups=175\n",
      "\t\tReduce shuffle bytes=2544\n",
      "\t\tReduce input records=175\n",
      "\t\tReduce output records=175\n",
      "\t\tSpilled Records=350\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=5\n",
      "\t\tTotal committed heap usage (bytes)=813170688\n",
      "\tCombiner-Counter\n",
      "\t\tTotal=2\n",
      "\tMap-Count\n",
      "\t\tTotal=1\n",
      "\tReduce-Counter\n",
      "\t\tTotal=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2182\n",
      "16/02/01 22:37:00 INFO streaming.StreamJob: Output directory: word_count\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.reduce.tasks=2 \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-combiner combiner.py \\\n",
    "-input Consumer_Complaints.csv \\\n",
    "-output word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show results\n",
    "#!hdfs dfs -cat /user/dunmireg/word_count/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 22:37:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 22:37:12 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/Consumer_Complaints.csv\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 22:37:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 22:37:13 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/word_count\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/02/01 22:37:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/02/01 22:37:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Remove output directory and stop yarn and hdfs\n",
    "!hadoop fs -rmr /user/dunmireg/Consumer_Complaints.csv\n",
    "!hadoop fs -rmr /user/dunmireg/word_count\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Part 4__\n",
    "\n",
    "Using a single reducer: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper-3-2-4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper-3-2-4.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "from csv import reader\n",
    "\n",
    "#Structure of complaints\n",
    "#Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,\n",
    "#Company,Company response,Timely response?,Consumer disputed?\n",
    "line_num = 0\n",
    "total = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "for line in reader(sys.stdin):\n",
    "    if line_num == 0:\n",
    "        line_num += 1\n",
    "        continue\n",
    "    else:\n",
    "        issue = line[3]\n",
    "        if issue == '':\n",
    "            issue = 'Blank'\n",
    "        words = re.findall(WORD_RE, issue)\n",
    "        for word in words:\n",
    "            total += 1\n",
    "            print word.lower() + '\\t' + str(1)\n",
    "print '*' + '\\t' + str(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer-3-2-4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer-3-2-4.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import operator\n",
    "\n",
    "current_word = None\n",
    "current_count = None\n",
    "word = None\n",
    "total = 0\n",
    "wordcount = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.split('\\t')\n",
    "    word = line[0]\n",
    "    count = int(line[1])\n",
    "    if word == '*':\n",
    "        total = count\n",
    "    else:\n",
    "        if current_word == word:\n",
    "            current_count += count\n",
    "        else:\n",
    "            if current_word:\n",
    "                #sys.stderr.write('reporter:counter:Reduce-Counter,Total,1\\n')\n",
    "                #wordcount[current_word] = current_count\n",
    "                print current_word + '\\t' + str(current_count) + '\\t' + str(float(current_count)/total)\n",
    "            current_word = word\n",
    "            current_count = count\n",
    "\n",
    "if current_word == word:\n",
    "    #sys.stderr.write('reporter:counter:Reduce-Counter,Total,1\\n')\n",
    "    #wordcount[current_word] = current_count\n",
    "    print current_word + '\\t' + str(current_count) + '\\t' + str(float(current_count)/total)\n",
    "    \n",
    "# largest = 50\n",
    "# smallest = 10\n",
    "# sortedWordCount = sorted(wordcount.items(), key = operator.itemgetter(1))\n",
    "\n",
    "# print \"The Top 50 terms are\"\n",
    "# for i in range(largest):\n",
    "#     print str(sortedWordCount[-i-1][0]) + '\\t' + str(sortedWordCount[-i-1][1]) + '\\t' + str(float(sortedWordCount[-i-1][1])/total)\n",
    "    \n",
    "# print '\\n'\n",
    "\n",
    "# print \"The bottom 10 terms are\"\n",
    "# for i in range(smallest):\n",
    "#     print str(sortedWordCount[i][0]) + '\\t' + str(sortedWordCount[i][1]) + '\\t' + str(float(sortedWordCount[i][1])/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper-3-2-4-2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper-3-2-4-2.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "#Structure is word + \\t + count + \\t + relative count\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    line = line.split('\\t')\n",
    "    print line[1] + '\\t'+ line[0] + '\\t' + line[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer-3-2-4-2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer-3-2-4-2.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "print \"Issue\" + '\\t' + \"Count\" + '\\t' + 'Relative Count'\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    line = line.split('\\t')\n",
    "    #structure currently is count + word + relative count\n",
    "    print line[1] + '\\t' + line[0] + '\\t'+ line[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!cat Consumer_Complaints.csv | python mapper-3-2-4.py | sort | python reducer-3-2-4.py | python mapper-3-2-4-2.py | sort -n | python reducer-3-2-4-2.py> output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-resourcemanager-Glenns-Air.home.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-nodemanager-Glenns-Air.home.out\n",
      "16/01/31 23:00:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-namenode-Glenns-Air.home.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-datanode-Glenns-Air.home.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-secondarynamenode-Glenns-Air.home.out\n",
      "16/01/31 23:01:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start hadoop yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 23:01:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#add input to hdfs\n",
    "!hdfs dfs -put Consumer_Complaints.csv /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 23:01:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 23:01:12 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/31 23:01:12 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/31 23:01:12 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/31 23:01:13 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/31 23:01:13 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/31 23:01:13 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1964160718_0001\n",
      "16/01/31 23:01:13 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/31 23:01:13 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/31 23:01:13 INFO mapreduce.Job: Running job: job_local1964160718_0001\n",
      "16/01/31 23:01:13 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/31 23:01:13 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 23:01:13 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/31 23:01:13 INFO mapred.LocalJobRunner: Starting task: attempt_local1964160718_0001_m_000000_0\n",
      "16/01/31 23:01:13 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 23:01:13 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/31 23:01:13 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/31 23:01:13 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/Consumer_Complaints.csv:0+50906486\n",
      "16/01/31 23:01:13 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/31 23:01:14 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/31 23:01:14 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/31 23:01:14 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/31 23:01:14 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/31 23:01:14 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/31 23:01:14 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/31 23:01:14 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./mapper-3-2-4.py]\n",
      "16/01/31 23:01:14 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/31 23:01:14 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/31 23:01:14 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/31 23:01:14 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/31 23:01:14 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/31 23:01:14 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/31 23:01:14 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/31 23:01:14 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/31 23:01:14 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/31 23:01:14 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/31 23:01:14 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/31 23:01:14 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/31 23:01:14 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:14 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:14 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:14 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:14 INFO streaming.PipeMapRed: Records R/W=1513/1\n",
      "16/01/31 23:01:14 INFO streaming.PipeMapRed: R/W/S=10000/41100/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:14 INFO mapreduce.Job: Job job_local1964160718_0001 running in uber mode : false\n",
      "16/01/31 23:01:14 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/31 23:01:15 INFO streaming.PipeMapRed: R/W/S=100000/448120/0 in:100000=100000/1 [rec/s] out:448120=448120/1 [rec/s]\n",
      "16/01/31 23:01:16 INFO streaming.PipeMapRed: R/W/S=200000/884534/0 in:100000=200000/2 [rec/s] out:442267=884534/2 [rec/s]\n",
      "16/01/31 23:01:17 INFO streaming.PipeMapRed: R/W/S=300000/1296452/0 in:100000=300000/3 [rec/s] out:432150=1296452/3 [rec/s]\n",
      "16/01/31 23:01:18 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 23:01:18 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 23:01:18 INFO mapred.LocalJobRunner: \n",
      "16/01/31 23:01:18 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/31 23:01:18 INFO mapred.MapTask: Spilling map output\n",
      "16/01/31 23:01:18 INFO mapred.MapTask: bufstart = 0; bufend = 13424749; bufvoid = 104857600\n",
      "16/01/31 23:01:18 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 20821148(83284592); length = 5393249/6553600\n",
      "16/01/31 23:01:19 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/31 23:01:19 INFO mapred.Task: Task:attempt_local1964160718_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/31 23:01:19 INFO mapred.LocalJobRunner: Records R/W=1513/1\n",
      "16/01/31 23:01:19 INFO mapred.Task: Task 'attempt_local1964160718_0001_m_000000_0' done.\n",
      "16/01/31 23:01:19 INFO mapred.LocalJobRunner: Finishing task: attempt_local1964160718_0001_m_000000_0\n",
      "16/01/31 23:01:19 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/31 23:01:19 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/31 23:01:19 INFO mapred.LocalJobRunner: Starting task: attempt_local1964160718_0001_r_000000_0\n",
      "16/01/31 23:01:19 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 23:01:19 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/31 23:01:19 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/31 23:01:19 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@79e9b608\n",
      "16/01/31 23:01:19 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/31 23:01:19 INFO reduce.EventFetcher: attempt_local1964160718_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/31 23:01:19 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1964160718_0001_m_000000_0 decomp: 16121377 len: 16121381 to MEMORY\n",
      "16/01/31 23:01:19 INFO reduce.InMemoryMapOutput: Read 16121377 bytes from map-output for attempt_local1964160718_0001_m_000000_0\n",
      "16/01/31 23:01:19 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 16121377, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->16121377\n",
      "16/01/31 23:01:19 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/31 23:01:19 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 23:01:19 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/31 23:01:19 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 23:01:19 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16121373 bytes\n",
      "16/01/31 23:01:19 INFO reduce.MergeManagerImpl: Merged 1 segments, 16121377 bytes to disk to satisfy reduce memory limit\n",
      "16/01/31 23:01:19 INFO reduce.MergeManagerImpl: Merging 1 files, 16121381 bytes from disk\n",
      "16/01/31 23:01:19 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/31 23:01:19 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 23:01:19 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16121373 bytes\n",
      "16/01/31 23:01:19 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 23:01:19 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./reducer-3-2-4.py]\n",
      "16/01/31 23:01:19 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/31 23:01:19 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/31 23:01:19 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:19 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:19 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:19 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/31 23:01:19 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:19 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:20 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:20 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:20 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:20 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:20 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:20 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:20 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:700000=700000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/31 23:01:20 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:800000=800000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/31 23:01:21 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:900000=900000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/31 23:01:21 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:1000000=1000000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/31 23:01:21 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:1100000=1100000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/31 23:01:21 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:1200000=1200000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/31 23:01:21 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:1300000=1300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/31 23:01:21 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 23:01:21 INFO streaming.PipeMapRed: Records R/W=1348313/1\n",
      "16/01/31 23:01:21 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 23:01:21 INFO mapred.Task: Task:attempt_local1964160718_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/31 23:01:21 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 23:01:21 INFO mapred.Task: Task attempt_local1964160718_0001_r_000000_0 is allowed to commit now\n",
      "16/01/31 23:01:21 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1964160718_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/word_count/_temporary/0/task_local1964160718_0001_r_000000\n",
      "16/01/31 23:01:21 INFO mapred.LocalJobRunner: Records R/W=1348313/1 > reduce\n",
      "16/01/31 23:01:21 INFO mapred.Task: Task 'attempt_local1964160718_0001_r_000000_0' done.\n",
      "16/01/31 23:01:21 INFO mapred.LocalJobRunner: Finishing task: attempt_local1964160718_0001_r_000000_0\n",
      "16/01/31 23:01:21 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/31 23:01:21 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/31 23:01:21 INFO mapreduce.Job: Job job_local1964160718_0001 completed successfully\n",
      "16/01/31 23:01:21 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=32454892\n",
      "\t\tFILE: Number of bytes written=49166017\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=101812972\n",
      "\t\tHDFS: Number of bytes written=5182\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=1348313\n",
      "\t\tMap output bytes=13424749\n",
      "\t\tMap output materialized bytes=16121381\n",
      "\t\tInput split bytes=111\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=176\n",
      "\t\tReduce shuffle bytes=16121381\n",
      "\t\tReduce input records=1348313\n",
      "\t\tReduce output records=175\n",
      "\t\tSpilled Records=2696626\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=4\n",
      "\t\tTotal committed heap usage (bytes)=541065216\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5182\n",
      "16/01/31 23:01:21 INFO streaming.StreamJob: Output directory: word_count\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper-3-2-4.py \\\n",
    "-reducer reducer-3-2-4.py \\\n",
    "-input Consumer_Complaints.csv \\\n",
    "-output word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 23:01:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 23:01:45 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/31 23:01:45 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/31 23:01:45 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/31 23:01:45 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/31 23:01:45 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/01/31 23:01:46 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1842258814_0001\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/31 23:01:46 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/31 23:01:46 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 23:01:46 INFO mapreduce.Job: Running job: job_local1842258814_0001\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: Starting task: attempt_local1842258814_0001_m_000000_0\n",
      "16/01/31 23:01:46 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 23:01:46 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/31 23:01:46 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/31 23:01:46 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/word_count/part-00000:0+5182\n",
      "16/01/31 23:01:46 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/31 23:01:46 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/31 23:01:46 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/31 23:01:46 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/31 23:01:46 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/31 23:01:46 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/31 23:01:46 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/31 23:01:46 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./mapper-3-2-4-2.py]\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/31 23:01:46 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:46 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:46 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:46 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 23:01:46 INFO streaming.PipeMapRed: Records R/W=175/1\n",
      "16/01/31 23:01:46 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: \n",
      "16/01/31 23:01:46 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/31 23:01:46 INFO mapred.MapTask: Spilling map output\n",
      "16/01/31 23:01:46 INFO mapred.MapTask: bufstart = 0; bufend = 5182; bufvoid = 104857600\n",
      "16/01/31 23:01:46 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213700(104854800); length = 697/6553600\n",
      "16/01/31 23:01:46 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/31 23:01:46 INFO mapred.Task: Task:attempt_local1842258814_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: Records R/W=175/1\n",
      "16/01/31 23:01:46 INFO mapred.Task: Task 'attempt_local1842258814_0001_m_000000_0' done.\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: Finishing task: attempt_local1842258814_0001_m_000000_0\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: Starting task: attempt_local1842258814_0001_r_000000_0\n",
      "16/01/31 23:01:46 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 23:01:46 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/31 23:01:46 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/31 23:01:46 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@562f392a\n",
      "16/01/31 23:01:46 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/31 23:01:46 INFO reduce.EventFetcher: attempt_local1842258814_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/31 23:01:46 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1842258814_0001_m_000000_0 decomp: 5534 len: 5538 to MEMORY\n",
      "16/01/31 23:01:46 INFO reduce.InMemoryMapOutput: Read 5534 bytes from map-output for attempt_local1842258814_0001_m_000000_0\n",
      "16/01/31 23:01:46 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 5534, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->5534\n",
      "16/01/31 23:01:46 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 23:01:46 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/31 23:01:46 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 23:01:46 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 5530 bytes\n",
      "16/01/31 23:01:46 INFO reduce.MergeManagerImpl: Merged 1 segments, 5534 bytes to disk to satisfy reduce memory limit\n",
      "16/01/31 23:01:46 INFO reduce.MergeManagerImpl: Merging 1 files, 5538 bytes from disk\n",
      "16/01/31 23:01:46 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/31 23:01:46 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 23:01:46 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 5530 bytes\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 23:01:46 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./reducer-3-2-4-2.py]\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/31 23:01:46 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:46 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:46 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:46 INFO streaming.PipeMapRed: Records R/W=175/1\n",
      "16/01/31 23:01:46 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 23:01:46 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 23:01:46 INFO mapred.Task: Task:attempt_local1842258814_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 23:01:46 INFO mapred.Task: Task attempt_local1842258814_0001_r_000000_0 is allowed to commit now\n",
      "16/01/31 23:01:46 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1842258814_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/sortedWordCount/_temporary/0/task_local1842258814_0001_r_000000\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: Records R/W=175/1 > reduce\n",
      "16/01/31 23:01:46 INFO mapred.Task: Task 'attempt_local1842258814_0001_r_000000_0' done.\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: Finishing task: attempt_local1842258814_0001_r_000000_0\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/31 23:01:47 INFO mapreduce.Job: Job job_local1842258814_0001 running in uber mode : false\n",
      "16/01/31 23:01:47 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/31 23:01:47 INFO mapreduce.Job: Job job_local1842258814_0001 completed successfully\n",
      "16/01/31 23:01:47 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=223198\n",
      "\t\tFILE: Number of bytes written=820648\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=10364\n",
      "\t\tHDFS: Number of bytes written=5209\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=175\n",
      "\t\tMap output records=175\n",
      "\t\tMap output bytes=5182\n",
      "\t\tMap output materialized bytes=5538\n",
      "\t\tInput split bytes=109\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=116\n",
      "\t\tReduce shuffle bytes=5538\n",
      "\t\tReduce input records=175\n",
      "\t\tReduce output records=176\n",
      "\t\tSpilled Records=350\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=5\n",
      "\t\tTotal committed heap usage (bytes)=511705088\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5182\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5209\n",
      "16/01/31 23:01:47 INFO streaming.StreamJob: Output directory: sortedWordCount\n"
     ]
    }
   ],
   "source": [
    "#run second job to properly sort by counts\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D  mapred.text.key.comparator.options=-n \\\n",
    "-mapper mapper-3-2-4-2.py \\\n",
    "-reducer reducer-3-2-4-2.py \\\n",
    "-input /user/dunmireg/word_count/part-00000 \\\n",
    "-output sortedWordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 23:01:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Issue\tCount\tRelative Count\n",
      "blank\t4\t2.96667240223e-06\n",
      "disclosures\t64\t4.74667584357e-05\n",
      "missing\t64\t4.74667584357e-05\n",
      "day\t71\t5.26584351396e-05\n",
      "amt\t71\t5.26584351396e-05\n",
      "checks\t75\t5.56251075419e-05\n",
      "convenience\t75\t5.56251075419e-05\n",
      "credited\t92\t6.82334652514e-05\n",
      "payment\t92\t6.82334652514e-05\n",
      "amount\t98\t7.26834738547e-05\n",
      "apply\t118\t8.75168358659e-05\n",
      "overlimit\t127\t9.41918487709e-05\n",
      "stop\t131\t9.71585211731e-05\n",
      "charges\t131\t9.71585211731e-05\n",
      "sale\t139\t0.000103091865978\n",
      "did\t139\t0.000103091865978\n",
      "applied\t139\t0.000103091865978\n",
      "receive\t139\t0.000103091865978\n",
      "acct\t163\t0.000120891900391\n",
      "arbitration\t168\t0.000124600240894\n",
      "wrong\t169\t0.000125341908994\n",
      "bank\t202\t0.000149816956313\n",
      "received\t216\t0.000160200309721\n",
      "bankruptcy\t222\t0.000164650318324\n",
      "advance\t240\t0.000178000344134\n",
      "cash\t240\t0.000178000344134\n",
      "privacy\t240\t0.000178000344134\n",
      "delay\t243\t0.000180225348436\n",
      "processing\t243\t0.000180225348436\n",
      "available\t274\t0.000203217059553\n",
      "promised\t274\t0.000203217059553\n",
      "was\t274\t0.000203217059553\n",
      "getting\t291\t0.000215825417262\n",
      "workout\t350\t0.000259583835195\n",
      "forbearance\t350\t0.000259583835195\n",
      "terms\t350\t0.000259583835195\n",
      "plans\t350\t0.000259583835195\n",
      "changes\t350\t0.000259583835195\n",
      "issues\t538\t0.0003990174381\n",
      "scam\t566\t0.000419784144916\n",
      "transfer\t597\t0.000442775856033\n",
      "balance\t597\t0.000442775856033\n",
      "unsolicited\t640\t0.000474667584357\n",
      "issuance\t640\t0.000474667584357\n",
      "shopping\t672\t0.000498400963575\n",
      "fees\t807\t0.000598526157151\n",
      "expect\t807\t0.000598526157151\n",
      "dispute\t904\t0.000670467962905\n",
      "i\t925\t0.000686042993016\n",
      "didn't\t925\t0.000686042993016\n",
      "for\t929\t0.000689009665419\n",
      "charged\t976\t0.000723868066145\n",
      "rewards\t1002\t0.000743151436759\n",
      "practices\t1003\t0.00074389310486\n",
      "delinquent\t1061\t0.000786909854692\n",
      "issue\t1098\t0.000814351574413\n",
      "decrease\t1149\t0.000852176647542\n",
      "increase\t1149\t0.000852176647542\n",
      "payoff\t1155\t0.000856626656145\n",
      "marketing\t1193\t0.000884810043966\n",
      "advertising\t1193\t0.000884810043966\n",
      "statement\t1220\t0.000904835082681\n",
      "out\t1242\t0.000921151780893\n",
      "relations\t1367\t0.00101386029346\n",
      "monitoring\t1453\t0.00107764375011\n",
      "use\t1477\t0.00109544378452\n",
      "transaction\t1485\t0.00110137712933\n",
      "determination\t1490\t0.00110508546983\n",
      "service\t1518\t0.00112585217665\n",
      "repay\t1647\t0.00122152736162\n",
      "line\t1732\t0.00128456915017\n",
      "late\t1797\t0.0013327775767\n",
      "servicer\t1944\t0.00144180278749\n",
      "dealing\t1944\t0.00144180278749\n",
      "with\t1944\t0.00144180278749\n",
      "can't\t1999\t0.00148259453302\n",
      "lender\t2165\t0.00160571143771\n",
      "using\t2422\t0.00179632013955\n",
      "debit\t2422\t0.00179632013955\n",
      "atm\t2422\t0.00179632013955\n",
      "customer\t2734\t0.00202772058693\n",
      "decision\t2774\t0.00205738731095\n",
      "underwriting\t2774\t0.00205738731095\n",
      "cancelling\t2795\t0.00207296234106\n",
      "action\t2964\t0.00219830425005\n",
      "threatening\t2964\t0.00219830425005\n",
      "illegal\t2964\t0.00219830425005\n",
      "an\t2964\t0.00219830425005\n",
      "fee\t3198\t0.00237185458559\n",
      "making\t3226\t0.0023926212924\n",
      "receiving\t3226\t0.0023926212924\n",
      "sending\t3226\t0.0023926212924\n",
      "theft\t3276\t0.00242970469743\n",
      "embezzlement\t3276\t0.00242970469743\n",
      "rate\t3431\t0.00254466325302\n",
      "apr\t3431\t0.00254466325302\n",
      "sharing\t3489\t0.00258768000285\n",
      "a\t3503\t0.00259806335626\n",
      "info\t3553\t0.00263514676128\n",
      "representation\t3621\t0.00268558019212\n",
      "statements\t3621\t0.00268558019212\n",
      "false\t3621\t0.00268558019212\n",
      "money\t3639\t0.00269893021793\n",
      "contact\t3710\t0.00275158865307\n",
      "are\t3821\t0.00283391381223\n",
      "you\t3821\t0.00283391381223\n",
      "pay\t3821\t0.00283391381223\n",
      "fraud\t3842\t0.00284948884235\n",
      "repaying\t3844\t0.00285097217855\n",
      "your\t3844\t0.00285097217855\n",
      "when\t4095\t0.00303713087179\n",
      "protection\t4139\t0.00306976426821\n",
      "taking\t4206\t0.00311945603095\n",
      "interest\t4238\t0.00314318941017\n",
      "settlement\t4350\t0.00322625623743\n",
      "costs\t4350\t0.00322625623743\n",
      "score\t4357\t0.00323144791413\n",
      "get\t4357\t0.00323144791413\n",
      "card\t4405\t0.00326704798296\n",
      "identity\t4729\t0.00350734844754\n",
      "investigation\t4858\t0.00360302363251\n",
      "company's\t4858\t0.00360302363251\n",
      "improper\t4966\t0.00368312378737\n",
      "managing\t5006\t0.00371279051139\n",
      "process\t5505\t0.00408288289357\n",
      "funds\t5663\t0.00420006645346\n",
      "low\t5663\t0.00420006645346\n",
      "caused\t5663\t0.00420006645346\n",
      "by\t5663\t0.00420006645346\n",
      "being\t5663\t0.00420006645346\n",
      "the\t6248\t0.00463394229229\n",
      "lease\t6337\t0.00469995075324\n",
      "reporting\t6559\t0.00486460107156\n",
      "disputes\t6938\t0.00514569328167\n",
      "verification\t7655\t0.00567746930977\n",
      "disclosure\t7655\t0.00567746930977\n",
      "other\t7886\t0.005848794641\n",
      "billing\t8158\t0.00605052836435\n",
      "unable\t8178\t0.00606536172637\n",
      "to\t8401\t0.00623075371279\n",
      "broker\t8625\t0.00639688736732\n",
      "originator\t8625\t0.00639688736732\n",
      "mortgage\t8625\t0.00639688736732\n",
      "communication\t8671\t0.00643100409994\n",
      "tactics\t8671\t0.00643100409994\n",
      "application\t8868\t0.00657711271575\n",
      "problems\t9484\t0.0070339802657\n",
      "deposits\t10555\t0.00782830680139\n",
      "withdrawals\t10555\t0.00782830680139\n",
      "my\t10731\t0.00795884038709\n",
      "of\t13983\t0.0103707450501\n",
      "opening\t16205\t0.0120187315695\n",
      "management\t16205\t0.0120187315695\n",
      "and\t16448\t0.012198956918\n",
      "collect\t17972\t0.0133292591032\n",
      "cont'd\t17972\t0.0133292591032\n",
      "owed\t17972\t0.0133292591032\n",
      "attempts\t17972\t0.0133292591032\n",
      "not\t18477\t0.013703801494\n",
      "closing\t19000\t0.0140916939106\n",
      "debt\t27874\t0.020673256635\n",
      "on\t29069\t0.0215595500151\n",
      "information\t29069\t0.0215595500151\n",
      "incorrect\t29133\t0.0216070167736\n",
      "report\t34903\t0.0258864417138\n",
      "servicing\t36767\t0.0272689110532\n",
      "escrow\t36767\t0.0272689110532\n",
      "payments\t39993\t0.0296615323456\n",
      "or\t40508\t0.0300434914174\n",
      "credit\t55251\t0.0409779042239\n",
      "account\t57448\t0.0426073490409\n",
      "foreclosure\t70487\t0.0522779594041\n",
      "modification\t70487\t0.0522779594041\n",
      "collection\t72394\t0.0536923204718\n",
      "loan\t119630\t0.0887257548698\n"
     ]
    }
   ],
   "source": [
    "#show results\n",
    "!hdfs dfs -cat /user/dunmireg/sortedWordCount/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#move output to local directory\n",
    "#!hadoop fs -copyToLocal /user/dunmireg/word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/31 23:02:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 23:02:10 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/Consumer_Complaints.csv\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/31 23:02:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 23:02:11 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/word_count\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/31 23:02:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 23:02:13 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/sortedWordCount\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/01/31 23:02:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/31 23:02:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Remove output directory and stop yarn and hdfs\n",
    "!hadoop fs -rmr /user/dunmireg/Consumer_Complaints.csv\n",
    "!hadoop fs -rmr /user/dunmireg/word_count\n",
    "!hadoop fs -rmr /user/dunmireg/sortedWordCount\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### HW3.3. Shopping Cart Analysis\n",
    "\n",
    "Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "\n",
    "For this homework use the online browsing behavior dataset located at: \n",
    "\n",
    "       https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "Each line in this dataset represents a browsing session of a customer. \n",
    "On each line, each string of 8 characters represents the id of an item browsed during that session. \n",
    "The items are separated by spaces.\n",
    "\n",
    "Here are the first few lines of the ProductPurchaseData \n",
    "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \n",
    "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \n",
    "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 \n",
    "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 \n",
    "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444 \n",
    "\n",
    "\n",
    "Do some exploratory data analysis of this dataset. \n",
    "\n",
    "How many unique items are available from this supplier?\n",
    "\n",
    "Using a single reducer: Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper-3-3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper-3-3.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "total = 0\n",
    "line_num = 1\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    items = line.split()\n",
    "    for item in items:\n",
    "        total += 1\n",
    "        print item + '\\t' + str(1)\n",
    "    print '*' + '\\t' + str(line_num) + '\\t' + str(len(items))\n",
    "    line_num += 1\n",
    "print '*' + '\\t' + str(0) + '\\t' + str(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer-3-3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer-3-3.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import operator\n",
    "\n",
    "total = 0\n",
    "unique_items = set()\n",
    "largest_basket = {}\n",
    "\n",
    "current_item = None\n",
    "current_count = None\n",
    "item = None\n",
    "\n",
    "wordcount = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    line = line.split('\\t')\n",
    "    \n",
    "    if line[0] == '*':\n",
    "        if line[1] == '0':\n",
    "            total = int(line[2])\n",
    "        else:\n",
    "            largest_basket[line[1]] = int(line[2])\n",
    "    else:\n",
    "        item = line[0]\n",
    "        count = int(line[1])\n",
    "        unique_items.add(item)\n",
    "        \n",
    "        if current_item == item:\n",
    "            current_count += count\n",
    "        else:\n",
    "            if current_item:\n",
    "                wordcount[current_item] = current_count\n",
    "            current_item = item\n",
    "            current_count = count\n",
    "\n",
    "if current_item == item:\n",
    "    wordcount[current_item] = current_count\n",
    "\n",
    "largest = 50\n",
    "smallest = 10\n",
    "#sorted_x = sorted(x.items(), key=operator.itemgetter(1))\n",
    "sortedWordCount = sorted(wordcount.items(), key = lambda x: (-x[1], x[0]))\n",
    "\n",
    "print \"The Top 50 items are:\"\n",
    "for i in range(largest):\n",
    "    print sortedWordCount[i][0] + '\\t' + str(sortedWordCount[i][1]) + '\\t' + str(float(sortedWordCount[i][1])/total)\n",
    "\n",
    "print '\\n'\n",
    "print \"The 10 smalleset items are\"\n",
    "for i in range(smallest):\n",
    "    print sortedWordCount[-i-1][0] + '\\t' + str(sortedWordCount[-i-1][1]) + '\\t' + str(float(sortedWordCount[-i-1][1])/total)\n",
    "\n",
    "print '\\n'\n",
    "print \"Number of unique items from this supplier: \" + str(len(unique_items))\n",
    "print \"Largest basket is \" + max(largest_basket.iteritems(), key = operator.itemgetter(1))[0] + \" with a basket size of \" + str(max(largest_basket.iteritems(), key = operator.itemgetter(1))[1]) \n",
    "\n",
    "#result = max(your_dict.iteritems(), key=operator.itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Top 50 items are:\r\n",
      "DAI62779\t6667\t0.0175067747831\r\n",
      "FRO40251\t3881\t0.010191059387\r\n",
      "ELE17451\t3875\t0.0101753040775\r\n",
      "GRO73461\t3602\t0.00945843749344\r\n",
      "SNA80324\t3044\t0.00799319370628\r\n",
      "ELE32164\t2851\t0.0074863979161\r\n",
      "DAI75645\t2736\t0.00718442114993\r\n",
      "SNA45677\t2455\t0.0064465474865\r\n",
      "FRO31317\t2330\t0.0061183118711\r\n",
      "DAI85309\t2293\t0.00602115412894\r\n",
      "ELE26917\t2292\t0.00601852824402\r\n",
      "FRO80039\t2233\t0.00586360103355\r\n",
      "GRO21487\t2115\t0.00555374661261\r\n",
      "SNA99873\t2083\t0.00546971829507\r\n",
      "GRO59710\t2004\t0.00526227338613\r\n",
      "GRO71621\t1920\t0.00504169905258\r\n",
      "FRO85978\t1918\t0.00503644728273\r\n",
      "GRO30386\t1840\t0.00483162825872\r\n",
      "ELE74009\t1816\t0.00476860702057\r\n",
      "GRO56726\t1784\t0.00468457870302\r\n",
      "DAI63921\t1773\t0.00465569396887\r\n",
      "GRO46854\t1756\t0.00461105392517\r\n",
      "ELE66600\t1713\t0.00449814087347\r\n",
      "DAI83733\t1712\t0.00449551498855\r\n",
      "FRO32293\t1702\t0.00446925613932\r\n",
      "ELE66810\t1697\t0.0044561267147\r\n",
      "SNA55762\t1646\t0.00432220658362\r\n",
      "DAI22177\t1627\t0.00427231477008\r\n",
      "FRO78087\t1531\t0.00402022981745\r\n",
      "ELE99737\t1516\t0.0039808415436\r\n",
      "ELE34057\t1489\t0.00390994265067\r\n",
      "GRO94758\t1489\t0.00390994265067\r\n",
      "FRO35904\t1436\t0.00377077074974\r\n",
      "FRO53271\t1420\t0.00372875659097\r\n",
      "SNA93860\t1407\t0.00369462008697\r\n",
      "SNA90094\t1390\t0.00364998004327\r\n",
      "GRO38814\t1352\t0.00355019641619\r\n",
      "ELE56788\t1345\t0.00353181522173\r\n",
      "GRO61133\t1321\t0.00346879398357\r\n",
      "DAI88807\t1316\t0.00345566455896\r\n",
      "ELE74482\t1316\t0.00345566455896\r\n",
      "ELE59935\t1311\t0.00344253513434\r\n",
      "SNA96271\t1295\t0.00340052097557\r\n",
      "DAI43223\t1290\t0.00338739155095\r\n",
      "ELE91337\t1289\t0.00338476566603\r\n",
      "GRO15017\t1275\t0.0033480032771\r\n",
      "DAI31081\t1261\t0.00331124088818\r\n",
      "GRO81087\t1220\t0.00320357960633\r\n",
      "DAI22896\t1219\t0.0032009537214\r\n",
      "GRO85051\t1214\t0.00318782429679\r\n",
      "\r\n",
      "\r\n",
      "The 10 smalleset items are\r\n",
      "SNA99941\t1\t2.62588492322e-06\r\n",
      "SNA99814\t1\t2.62588492322e-06\r\n",
      "SNA99752\t1\t2.62588492322e-06\r\n",
      "SNA99635\t1\t2.62588492322e-06\r\n",
      "SNA99634\t1\t2.62588492322e-06\r\n",
      "SNA99515\t1\t2.62588492322e-06\r\n",
      "SNA99413\t1\t2.62588492322e-06\r\n",
      "SNA99404\t1\t2.62588492322e-06\r\n",
      "SNA99393\t1\t2.62588492322e-06\r\n",
      "SNA99310\t1\t2.62588492322e-06\r\n",
      "\r\n",
      "\r\n",
      "Number of unique items from this supplier: 12592\r\n",
      "Largest basket is 7034 with a basket size of 37\r\n"
     ]
    }
   ],
   "source": [
    "!cat ProductPurchaseData.txt | python mapper-3-3.py | sort | python reducer-3-3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-resourcemanager-Glenns-Air.home.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-nodemanager-Glenns-Air.home.out\n",
      "16/02/01 17:53:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-namenode-Glenns-Air.home.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-datanode-Glenns-Air.home.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-secondarynamenode-Glenns-Air.home.out\n",
      "16/02/01 17:54:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start hadoop yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 17:54:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#add input to hdfs\n",
    "!hdfs dfs -put ProductPurchaseData.txt /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 17:54:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 17:54:10 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 17:54:10 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 17:54:10 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 17:54:10 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 17:54:10 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 17:54:11 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local672224401_0001\n",
      "16/02/01 17:54:11 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 17:54:11 INFO mapreduce.Job: Running job: job_local672224401_0001\n",
      "16/02/01 17:54:11 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 17:54:11 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 17:54:11 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 17:54:11 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 17:54:11 INFO mapred.LocalJobRunner: Starting task: attempt_local672224401_0001_m_000000_0\n",
      "16/02/01 17:54:11 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 17:54:11 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 17:54:11 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 17:54:11 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/ProductPurchaseData.txt:0+3458517\n",
      "16/02/01 17:54:11 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/01 17:54:11 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 17:54:11 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 17:54:11 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 17:54:11 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 17:54:11 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 17:54:11 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 17:54:11 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./mapper-3-3.py]\n",
      "16/02/01 17:54:11 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 17:54:11 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 17:54:11 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 17:54:11 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 17:54:11 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 17:54:11 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 17:54:11 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 17:54:11 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 17:54:11 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 17:54:11 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 17:54:11 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 17:54:11 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 17:54:11 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:54:11 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:54:11 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:54:11 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:54:11 INFO streaming.PipeMapRed: Records R/W=1216/1\n",
      "16/02/01 17:54:11 INFO streaming.PipeMapRed: R/W/S=10000/124513/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:54:12 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 17:54:12 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 17:54:12 INFO mapred.LocalJobRunner: \n",
      "16/02/01 17:54:12 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 17:54:12 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 17:54:12 INFO mapred.MapTask: bufstart = 0; bufend = 4509483; bufvoid = 104857600\n",
      "16/02/01 17:54:12 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24566696(98266784); length = 1647701/6553600\n",
      "16/02/01 17:54:12 INFO mapreduce.Job: Job job_local672224401_0001 running in uber mode : false\n",
      "16/02/01 17:54:12 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/01 17:54:12 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 17:54:12 INFO mapred.Task: Task:attempt_local672224401_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 17:54:12 INFO mapred.LocalJobRunner: Records R/W=1216/1\n",
      "16/02/01 17:54:12 INFO mapred.Task: Task 'attempt_local672224401_0001_m_000000_0' done.\n",
      "16/02/01 17:54:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local672224401_0001_m_000000_0\n",
      "16/02/01 17:54:12 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 17:54:12 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 17:54:12 INFO mapred.LocalJobRunner: Starting task: attempt_local672224401_0001_r_000000_0\n",
      "16/02/01 17:54:12 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 17:54:12 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 17:54:12 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 17:54:12 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@46dbb3eb\n",
      "16/02/01 17:54:12 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 17:54:12 INFO reduce.EventFetcher: attempt_local672224401_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 17:54:12 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local672224401_0001_m_000000_0 decomp: 5333337 len: 5333341 to MEMORY\n",
      "16/02/01 17:54:12 INFO reduce.InMemoryMapOutput: Read 5333337 bytes from map-output for attempt_local672224401_0001_m_000000_0\n",
      "16/02/01 17:54:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 5333337, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->5333337\n",
      "16/02/01 17:54:12 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 17:54:12 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 17:54:12 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 17:54:12 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 17:54:12 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 5333333 bytes\n",
      "16/02/01 17:54:12 INFO reduce.MergeManagerImpl: Merged 1 segments, 5333337 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 17:54:12 INFO reduce.MergeManagerImpl: Merging 1 files, 5333341 bytes from disk\n",
      "16/02/01 17:54:12 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 17:54:12 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 17:54:12 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 5333333 bytes\n",
      "16/02/01 17:54:12 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 17:54:12 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./reducer-3-3.py]\n",
      "16/02/01 17:54:12 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 17:54:12 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 17:54:12 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:54:12 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:54:12 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:54:12 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:54:13 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:54:13 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/01 17:54:13 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:54:13 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:54:13 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:54:13 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:54:13 INFO streaming.PipeMapRed: Records R/W=411926/1\n",
      "16/02/01 17:54:13 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 17:54:13 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 17:54:13 INFO mapred.Task: Task:attempt_local672224401_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 17:54:13 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 17:54:13 INFO mapred.Task: Task attempt_local672224401_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 17:54:13 INFO output.FileOutputCommitter: Saved output of task 'attempt_local672224401_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/sortedWordCount/_temporary/0/task_local672224401_0001_r_000000\n",
      "16/02/01 17:54:13 INFO mapred.LocalJobRunner: Records R/W=411926/1 > reduce\n",
      "16/02/01 17:54:13 INFO mapred.Task: Task 'attempt_local672224401_0001_r_000000_0' done.\n",
      "16/02/01 17:54:13 INFO mapred.LocalJobRunner: Finishing task: attempt_local672224401_0001_r_000000_0\n",
      "16/02/01 17:54:13 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 17:54:14 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 17:54:14 INFO mapreduce.Job: Job job_local672224401_0001 completed successfully\n",
      "16/02/01 17:54:14 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=10878810\n",
      "\t\tFILE: Number of bytes written=16798891\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6917034\n",
      "\t\tHDFS: Number of bytes written=1987\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=411926\n",
      "\t\tMap output bytes=4509483\n",
      "\t\tMap output materialized bytes=5333341\n",
      "\t\tInput split bytes=111\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12593\n",
      "\t\tReduce shuffle bytes=5333341\n",
      "\t\tReduce input records=411926\n",
      "\t\tReduce output records=68\n",
      "\t\tSpilled Records=823852\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=620756992\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1987\n",
      "16/02/01 17:54:14 INFO streaming.StreamJob: Output directory: sortedWordCount\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper-3-3.py \\\n",
    "-reducer reducer-3-3.py \\\n",
    "-input ProductPurchaseData.txt \\\n",
    "-output sortedWordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 17:54:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "The Top 50 items are:\t\n",
      "DAI62779\t6667\t0.0175067747831\n",
      "FRO40251\t3881\t0.010191059387\n",
      "ELE17451\t3875\t0.0101753040775\n",
      "GRO73461\t3602\t0.00945843749344\n",
      "SNA80324\t3044\t0.00799319370628\n",
      "ELE32164\t2851\t0.0074863979161\n",
      "DAI75645\t2736\t0.00718442114993\n",
      "SNA45677\t2455\t0.0064465474865\n",
      "FRO31317\t2330\t0.0061183118711\n",
      "DAI85309\t2293\t0.00602115412894\n",
      "ELE26917\t2292\t0.00601852824402\n",
      "FRO80039\t2233\t0.00586360103355\n",
      "GRO21487\t2115\t0.00555374661261\n",
      "SNA99873\t2083\t0.00546971829507\n",
      "GRO59710\t2004\t0.00526227338613\n",
      "GRO71621\t1920\t0.00504169905258\n",
      "FRO85978\t1918\t0.00503644728273\n",
      "GRO30386\t1840\t0.00483162825872\n",
      "ELE74009\t1816\t0.00476860702057\n",
      "GRO56726\t1784\t0.00468457870302\n",
      "DAI63921\t1773\t0.00465569396887\n",
      "GRO46854\t1756\t0.00461105392517\n",
      "ELE66600\t1713\t0.00449814087347\n",
      "DAI83733\t1712\t0.00449551498855\n",
      "FRO32293\t1702\t0.00446925613932\n",
      "ELE66810\t1697\t0.0044561267147\n",
      "SNA55762\t1646\t0.00432220658362\n",
      "DAI22177\t1627\t0.00427231477008\n",
      "FRO78087\t1531\t0.00402022981745\n",
      "ELE99737\t1516\t0.0039808415436\n",
      "ELE34057\t1489\t0.00390994265067\n",
      "GRO94758\t1489\t0.00390994265067\n",
      "FRO35904\t1436\t0.00377077074974\n",
      "FRO53271\t1420\t0.00372875659097\n",
      "SNA93860\t1407\t0.00369462008697\n",
      "SNA90094\t1390\t0.00364998004327\n",
      "GRO38814\t1352\t0.00355019641619\n",
      "ELE56788\t1345\t0.00353181522173\n",
      "GRO61133\t1321\t0.00346879398357\n",
      "DAI88807\t1316\t0.00345566455896\n",
      "ELE74482\t1316\t0.00345566455896\n",
      "ELE59935\t1311\t0.00344253513434\n",
      "SNA96271\t1295\t0.00340052097557\n",
      "DAI43223\t1290\t0.00338739155095\n",
      "ELE91337\t1289\t0.00338476566603\n",
      "GRO15017\t1275\t0.0033480032771\n",
      "DAI31081\t1261\t0.00331124088818\n",
      "GRO81087\t1220\t0.00320357960633\n",
      "DAI22896\t1219\t0.0032009537214\n",
      "GRO85051\t1214\t0.00318782429679\n",
      "\t\n",
      "\t\n",
      "The 10 smalleset items are\t\n",
      "SNA99941\t1\t2.62588492322e-06\n",
      "SNA99814\t1\t2.62588492322e-06\n",
      "SNA99752\t1\t2.62588492322e-06\n",
      "SNA99635\t1\t2.62588492322e-06\n",
      "SNA99634\t1\t2.62588492322e-06\n",
      "SNA99515\t1\t2.62588492322e-06\n",
      "SNA99413\t1\t2.62588492322e-06\n",
      "SNA99404\t1\t2.62588492322e-06\n",
      "SNA99393\t1\t2.62588492322e-06\n",
      "SNA99310\t1\t2.62588492322e-06\n",
      "\t\n",
      "\t\n",
      "Number of unique items from this supplier: 12592\t\n",
      "Largest basket is 7034 with a basket size of 37\t\n"
     ]
    }
   ],
   "source": [
    "#show results\n",
    "!hdfs dfs -cat /user/dunmireg/sortedWordCount/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 17:54:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 17:54:17 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/ProductPurchaseData.txt\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 17:54:18 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 17:54:19 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/sortedWordCount\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/02/01 17:54:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/02/01 17:54:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Remove output directory and stop yarn and hdfs\n",
    "!hadoop fs -rmr /user/dunmireg/ProductPurchaseData.txt\n",
    "!hadoop fs -rmr /user/dunmireg/sortedWordCount\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### HW3.4.\n",
    "\n",
    "(Computationally prohibitive but then again Hadoop can handle this) Pairs\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a map-reduce program \n",
    "to find products which are frequently browsed together. Fix the support count (cooccurence count) to s = 100 \n",
    "(i.e. product pairs need to occur together at least 100 times to be considered frequent) \n",
    "and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more.\n",
    "\n",
    "List the top 50 product pairs with corresponding support count (aka frequency), and relative frequency or support (number of records where they coccur, the number of records where they coccur/the number of baskets in the dataset)  in decreasing order of support  for frequent (100>count) itemsets of size 2. \n",
    "\n",
    "Use the Pairs pattern (lecture 3)  to  extract these frequent itemsets of size 2. Free free to use combiners if they bring value. Instrument your code with counters for count the number of times your mapper, combiner and reducers are called.  \n",
    "\n",
    "Please output records of the following form for the top 50 pairs (itemsets of size 2): \n",
    "\n",
    "      item1, item2, support count, support\n",
    "\n",
    "\n",
    "\n",
    "Fix the ordering of the pairs lexicographically (left to right), \n",
    "and break ties in support (between pairs, if any exist) \n",
    "by taking the first ones in lexicographically increasing order. \n",
    "\n",
    "Report  the compute time for the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper-3-4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper-3-4.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "from itertools import combinations\n",
    "\n",
    "totalBaskets = 0\n",
    "for line in sys.stdin:\n",
    "    totalBaskets += 1\n",
    "    line = line.strip()\n",
    "    line = line.split()\n",
    "    \n",
    "    pairs = list(combinations(line, 2))\n",
    "    for pair in pairs:\n",
    "        pair = sorted(list(pair))\n",
    "        print pair[0] + ' ' + pair[1] + '\\t' + str(1)\n",
    "print '*' + '\\t' + str(totalBaskets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer-3-4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer-3-4.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "\n",
    "support = 100\n",
    "totalBaskets = 0\n",
    "pairs = defaultdict(int)\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.split('\\t')\n",
    "    if line[0] == '*':\n",
    "        totalBaskets = int(line[1])\n",
    "    else:\n",
    "        pairs[line[0]] += int(line[1])\n",
    "\n",
    "freqDict = {}\n",
    "for pair, count in pairs.iteritems():\n",
    "    if count > 3:\n",
    "        freqDict[pair] = count\n",
    "\n",
    "print \"Top 50 item pairs:\"\n",
    "print '\\n'\n",
    "print 'Item Pair' + '\\t' + 'Support Count' + '\\t' + 'Relative Support Count'\n",
    "print '\\n'\n",
    "        \n",
    "sortedFreqDict = sorted(freqDict.items(), key = lambda x: (-x[1], x[0]))\n",
    "for i in range(50):\n",
    "    print sortedFreqDict[i][0] + '\\t' + str(sortedFreqDict[i][1]) + '\\t' + str(float(sortedFreqDict[i][1])/totalBaskets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!cat ProductPurchaseData.txt | python mapper-3-4.py | sort | python reducer-3-4.py > output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-resourcemanager-Glenns-Air.home.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-nodemanager-Glenns-Air.home.out\n",
      "16/02/01 17:56:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-namenode-Glenns-Air.home.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-datanode-Glenns-Air.home.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-secondarynamenode-Glenns-Air.home.out\n",
      "16/02/01 17:56:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start hadoop yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 17:56:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#add input to hdfs\n",
    "!hdfs dfs -put ProductPurchaseData.txt /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 17:56:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 17:56:36 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 17:56:36 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 17:56:36 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 17:56:37 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 17:56:37 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 17:56:37 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local350127535_0001\n",
      "16/02/01 17:56:37 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 17:56:37 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 17:56:37 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 17:56:37 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 17:56:37 INFO mapreduce.Job: Running job: job_local350127535_0001\n",
      "16/02/01 17:56:37 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 17:56:37 INFO mapred.LocalJobRunner: Starting task: attempt_local350127535_0001_m_000000_0\n",
      "16/02/01 17:56:37 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 17:56:37 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 17:56:37 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 17:56:37 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/ProductPurchaseData.txt:0+3458517\n",
      "16/02/01 17:56:37 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/01 17:56:37 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 17:56:37 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 17:56:37 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 17:56:37 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 17:56:37 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 17:56:37 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 17:56:37 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./mapper-3-4.py]\n",
      "16/02/01 17:56:37 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 17:56:37 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 17:56:37 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 17:56:37 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 17:56:37 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 17:56:37 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 17:56:37 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 17:56:37 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 17:56:37 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 17:56:37 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 17:56:37 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 17:56:37 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 17:56:37 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:56:37 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:56:37 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:56:37 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:56:37 INFO streaming.PipeMapRed: Records R/W=1216/1\n",
      "16/02/01 17:56:38 INFO mapreduce.Job: Job job_local350127535_0001 running in uber mode : false\n",
      "16/02/01 17:56:38 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/01 17:56:39 INFO streaming.PipeMapRed: R/W/S=10000/854425/0 in:10000=10000/1 [rec/s] out:854425=854425/1 [rec/s]\n",
      "16/02/01 17:56:41 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 17:56:41 INFO mapred.MapTask: bufstart = 0; bufend = 46603380; bufvoid = 104857600\n",
      "16/02/01 17:56:41 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 16893724(67574896); length = 9320673/6553600\n",
      "16/02/01 17:56:41 INFO mapred.MapTask: (EQUATOR) 55924036 kvi 13981004(55924016)\n",
      "16/02/01 17:56:42 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 17:56:42 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 17:56:42 INFO mapred.LocalJobRunner: \n",
      "16/02/01 17:56:42 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 17:56:43 INFO mapred.LocalJobRunner: Records R/W=1216/1 > sort\n",
      "16/02/01 17:56:44 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/02/01 17:56:46 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 17:56:46 INFO mapred.MapTask: (RESET) equator 55924036 kv 13981004(55924016) kvi 13165448(52661792)\n",
      "16/02/01 17:56:46 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 17:56:46 INFO mapred.MapTask: bufstart = 55924036; bufend = 60001804; bufvoid = 104857600\n",
      "16/02/01 17:56:46 INFO mapred.MapTask: kvstart = 13981004(55924016); kvend = 13165452(52661808); length = 815553/6553600\n",
      "16/02/01 17:56:46 INFO mapred.MapTask: Finished spill 1\n",
      "16/02/01 17:56:46 INFO mapred.Merger: Merging 2 sorted segments\n",
      "16/02/01 17:56:46 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 55749252 bytes\n",
      "16/02/01 17:56:46 INFO mapred.LocalJobRunner: Records R/W=1216/1 > sort > \n",
      "16/02/01 17:56:47 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "16/02/01 17:56:47 INFO mapred.Task: Task:attempt_local350127535_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 17:56:47 INFO mapred.LocalJobRunner: Records R/W=1216/1 > sort\n",
      "16/02/01 17:56:47 INFO mapred.Task: Task 'attempt_local350127535_0001_m_000000_0' done.\n",
      "16/02/01 17:56:47 INFO mapred.LocalJobRunner: Finishing task: attempt_local350127535_0001_m_000000_0\n",
      "16/02/01 17:56:47 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 17:56:47 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 17:56:47 INFO mapred.LocalJobRunner: Starting task: attempt_local350127535_0001_r_000000_0\n",
      "16/02/01 17:56:47 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 17:56:47 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 17:56:47 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 17:56:47 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1a05e6dd\n",
      "16/02/01 17:56:47 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 17:56:47 INFO reduce.EventFetcher: attempt_local350127535_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 17:56:47 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local350127535_0001_m_000000_0 decomp: 55749266 len: 55749270 to MEMORY\n",
      "16/02/01 17:56:47 INFO reduce.InMemoryMapOutput: Read 55749266 bytes from map-output for attempt_local350127535_0001_m_000000_0\n",
      "16/02/01 17:56:47 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 55749266, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->55749266\n",
      "16/02/01 17:56:47 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 17:56:47 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 17:56:47 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 17:56:47 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 17:56:47 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 55749262 bytes\n",
      "16/02/01 17:56:48 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/01 17:56:48 INFO reduce.MergeManagerImpl: Merged 1 segments, 55749266 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 17:56:48 INFO reduce.MergeManagerImpl: Merging 1 files, 55749270 bytes from disk\n",
      "16/02/01 17:56:48 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 17:56:48 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 17:56:48 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 55749262 bytes\n",
      "16/02/01 17:56:48 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 17:56:48 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./reducer-3-4.py]\n",
      "16/02/01 17:56:48 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 17:56:48 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 17:56:48 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:56:48 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:56:48 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:56:48 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:56:48 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:56:49 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:56:49 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:56:49 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:56:49 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:56:49 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:56:49 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:600000=600000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 17:56:50 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:700000=700000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 17:56:50 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:800000=800000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 17:56:50 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:900000=900000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 17:56:50 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:1000000=1000000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 17:56:50 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:1100000=1100000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 17:56:50 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:600000=1200000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/01 17:56:51 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:650000=1300000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/01 17:56:51 INFO streaming.PipeMapRed: R/W/S=1400000/0/0 in:700000=1400000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/01 17:56:51 INFO streaming.PipeMapRed: R/W/S=1500000/0/0 in:750000=1500000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/01 17:56:51 INFO streaming.PipeMapRed: R/W/S=1600000/0/0 in:800000=1600000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/01 17:56:51 INFO streaming.PipeMapRed: R/W/S=1700000/0/0 in:850000=1700000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/01 17:56:51 INFO streaming.PipeMapRed: R/W/S=1800000/0/0 in:600000=1800000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/02/01 17:56:52 INFO streaming.PipeMapRed: R/W/S=1900000/0/0 in:633333=1900000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/02/01 17:56:52 INFO streaming.PipeMapRed: R/W/S=2000000/0/0 in:666666=2000000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/02/01 17:56:52 INFO streaming.PipeMapRed: R/W/S=2100000/0/0 in:700000=2100000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/02/01 17:56:52 INFO streaming.PipeMapRed: R/W/S=2200000/0/0 in:733333=2200000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/02/01 17:56:52 INFO streaming.PipeMapRed: R/W/S=2300000/0/0 in:766666=2300000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/02/01 17:56:53 INFO streaming.PipeMapRed: R/W/S=2400000/0/0 in:600000=2400000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/02/01 17:56:53 INFO streaming.PipeMapRed: R/W/S=2500000/0/0 in:625000=2500000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/02/01 17:56:53 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 17:56:54 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 17:56:54 INFO streaming.PipeMapRed: Records R/W=2534058/1\n",
      "16/02/01 17:56:54 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 17:56:54 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 17:56:55 INFO mapred.Task: Task:attempt_local350127535_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 17:56:55 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 17:56:55 INFO mapred.Task: Task attempt_local350127535_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 17:56:55 INFO output.FileOutputCommitter: Saved output of task 'attempt_local350127535_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/sortedProducts/_temporary/0/task_local350127535_0001_r_000000\n",
      "16/02/01 17:56:55 INFO mapred.LocalJobRunner: Records R/W=2534058/1 > reduce\n",
      "16/02/01 17:56:55 INFO mapred.Task: Task 'attempt_local350127535_0001_r_000000_0' done.\n",
      "16/02/01 17:56:55 INFO mapred.LocalJobRunner: Finishing task: attempt_local350127535_0001_r_000000_0\n",
      "16/02/01 17:56:55 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 17:56:55 INFO mapreduce.Job: Job job_local350127535_0001 completed successfully\n",
      "16/02/01 17:56:55 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=223209220\n",
      "\t\tFILE: Number of bytes written=279545226\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6917034\n",
      "\t\tHDFS: Number of bytes written=1973\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=2534058\n",
      "\t\tMap output bytes=50681148\n",
      "\t\tMap output materialized bytes=55749270\n",
      "\t\tInput split bytes=111\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=877099\n",
      "\t\tReduce shuffle bytes=55749270\n",
      "\t\tReduce input records=2534058\n",
      "\t\tReduce output records=56\n",
      "\t\tSpilled Records=7602174\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=6\n",
      "\t\tTotal committed heap usage (bytes)=620756992\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1973\n",
      "16/02/01 17:56:55 INFO streaming.StreamJob: Output directory: sortedProducts\n",
      "CPU times: user 107 ms, sys: 28.8 ms, total: 136 ms\n",
      "Wall time: 20.6 s\n"
     ]
    }
   ],
   "source": [
    "time !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper-3-4.py \\\n",
    "-reducer reducer-3-4.py \\\n",
    "-input ProductPurchaseData.txt \\\n",
    "-output sortedProducts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 17:57:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Top 50 item pairs:\t\n",
      "\t\n",
      "\t\n",
      "Item Pair\tSupport Count\tRelative Support Count\n",
      "\t\n",
      "\t\n",
      "DAI62779 ELE17451\t1592\t0.0511880646925\n",
      "FRO40251 SNA80324\t1412\t0.0454004694383\n",
      "DAI75645 FRO40251\t1254\t0.0403202469374\n",
      "FRO40251 GRO85051\t1213\t0.0390019613517\n",
      "DAI62779 GRO73461\t1139\t0.0366226166361\n",
      "DAI75645 SNA80324\t1130\t0.0363332368734\n",
      "DAI62779 FRO40251\t1070\t0.0344040384554\n",
      "DAI62779 SNA80324\t923\t0.0296775023311\n",
      "DAI62779 DAI85309\t918\t0.0295167357963\n",
      "ELE32164 GRO59710\t911\t0.0292916626475\n",
      "DAI62779 DAI75645\t882\t0.0283592167454\n",
      "FRO40251 GRO73461\t882\t0.0283592167454\n",
      "DAI62779 ELE92920\t877\t0.0281984502106\n",
      "FRO40251 FRO92469\t835\t0.026848011318\n",
      "DAI62779 ELE32164\t832\t0.0267515513971\n",
      "DAI75645 GRO73461\t712\t0.0228931545609\n",
      "DAI43223 ELE32164\t711\t0.022861001254\n",
      "DAI62779 GRO30386\t709\t0.02279669464\n",
      "ELE17451 FRO40251\t697\t0.0224108549564\n",
      "DAI85309 ELE99737\t659\t0.0211890292917\n",
      "DAI62779 ELE26917\t650\t0.020899649529\n",
      "GRO21487 GRO73461\t631\t0.0202887366966\n",
      "DAI62779 SNA45677\t604\t0.0194205974084\n",
      "ELE17451 SNA80324\t597\t0.0191955242597\n",
      "DAI62779 GRO71621\t595\t0.0191312176457\n",
      "DAI62779 SNA55762\t593\t0.0190669110318\n",
      "DAI62779 DAI83733\t586\t0.018841837883\n",
      "ELE17451 GRO73461\t580\t0.0186489180412\n",
      "GRO73461 SNA80324\t562\t0.0180701585158\n",
      "DAI62779 GRO59710\t561\t0.0180380052088\n",
      "DAI62779 FRO80039\t550\t0.0176843188322\n",
      "DAI75645 ELE17451\t547\t0.0175878589113\n",
      "DAI62779 SNA93860\t537\t0.0172663258416\n",
      "DAI55148 DAI62779\t526\t0.016912639465\n",
      "DAI43223 GRO59710\t512\t0.0164624931674\n",
      "ELE17451 ELE32164\t511\t0.0164303398605\n",
      "DAI62779 SNA18336\t506\t0.0162695733256\n",
      "ELE32164 GRO73461\t486\t0.0156265071863\n",
      "DAI62779 FRO78087\t482\t0.0154978939584\n",
      "DAI85309 ELE17451\t482\t0.0154978939584\n",
      "DAI62779 GRO94758\t479\t0.0154014340375\n",
      "DAI62779 GRO21487\t471\t0.0151442075817\n",
      "GRO85051 SNA80324\t471\t0.0151442075817\n",
      "ELE17451 GRO30386\t468\t0.0150477476608\n",
      "FRO85978 SNA95666\t463\t0.014886981126\n",
      "DAI62779 FRO19221\t462\t0.014854827819\n",
      "DAI62779 GRO46854\t461\t0.0148226745121\n",
      "DAI43223 DAI62779\t459\t0.0147583678981\n",
      "ELE92920 SNA18336\t455\t0.0146297546703\n",
      "DAI88079 FRO40251\t446\t0.0143403749076\n"
     ]
    }
   ],
   "source": [
    "#show results\n",
    "!hdfs dfs -cat /user/dunmireg/sortedProducts/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 17:57:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 17:57:12 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/ProductPurchaseData.txt\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 17:57:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 17:57:13 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/sortedProducts\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/02/01 17:57:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/02/01 17:57:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Remove output directory and stop yarn and hdfs\n",
    "!hadoop fs -rmr /user/dunmireg/ProductPurchaseData.txt\n",
    "!hadoop fs -rmr /user/dunmireg/sortedProducts\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### HW3.5: Stripes\n",
    "Repeat 3.4 using the stripes design pattern for finding cooccuring pairs.\n",
    "\n",
    "Report  the compute times for stripes job versus the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts. Discuss the differences in these counts between the Pairs and Stripes jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper-3-5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper-3-5.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "totalBaskets = 0\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    line = line.split()\n",
    "    totalBaskets += 1\n",
    "    \n",
    "    for item in line:\n",
    "        counts = {}\n",
    "        for othItem in line:\n",
    "            if othItem != item:\n",
    "                if othItem in counts.keys():\n",
    "                    counts[othItem] += 1\n",
    "                else:\n",
    "                    counts[othItem] = 1\n",
    "        print item + '\\t' + str(counts)\n",
    "print \"*\" + '\\t' + str(totalBaskets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer-3-5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer-3-5.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import ast\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "support = 3\n",
    "totalBaskets = 0\n",
    "current_item = None\n",
    "current_item_dict = Counter({})\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().split('\\t')\n",
    "    \n",
    "    if line[0] == '*':\n",
    "        totalBaskets = int(line[1])\n",
    "    else:\n",
    "        item = line[0]\n",
    "        item_dict = line[1]\n",
    "        item_dict = Counter(eval(item_dict))\n",
    "        if current_item == item:\n",
    "            current_item_dict += Counter(item_dict)\n",
    "        else:\n",
    "            if current_item:\n",
    "                for i in OrderedDict(sorted(current_item_dict.items())):\n",
    "                    if current_item_dict[i] >= support:\n",
    "                        print current_item + ' ' + i + '\\t' + str(current_item_dict[i]) + '\\t' + str(float(current_item_dict[i])/totalBaskets)\n",
    "            current_item = item\n",
    "            current_item_dict = item_dict\n",
    "            \n",
    "for i in OrderedDict(sorted(current_item_dict.items())):\n",
    "    if current_item_dict[i] >= support:\n",
    "        print current_item + ' ' + i + '\\t' + str(current_item_dict[i]) + '\\t' + str(float(current_item_dict[i])/totalBaskets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper-3-5-2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper-3-5-2.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "#Structure is word + \\t + count + \\t + relative count\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.split('\\t')\n",
    "    print line[1] + '\\t'+ line[0] + '\\t' + line[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer-3-5-2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer-3-5-2.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "print \"Pair\" + '\\t' + \"Count\" + '\\t' + 'Relative Count'\n",
    "for line in sys.stdin:\n",
    "    line = line.split('\\t')\n",
    "    print line\n",
    "    #structure currently is count + word + relative count\n",
    "    print line[1] + '\\t' + line[0] + '\\t'+ line[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Problem where sort -n is outputting a blank file. Unclear why__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat ProductPurchaseData2.txt | python mapper-3-5.py | sort | python reducer-3-5.py | python mapper-3-5-2.py > output.txt #python reducer-3-5-2.py > output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-resourcemanager-Glenns-Air.home.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-nodemanager-Glenns-Air.home.out\n",
      "16/02/01 22:11:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-namenode-Glenns-Air.home.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-datanode-Glenns-Air.home.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-secondarynamenode-Glenns-Air.home.out\n",
      "16/02/01 22:12:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start hadoop yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 22:12:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#add input to hdfs\n",
    "!hdfs dfs -put ProductPurchaseData.txt /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 22:12:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 22:12:31 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 22:12:31 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 22:12:31 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 22:12:32 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 22:12:32 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 22:12:32 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local500015672_0001\n",
      "16/02/01 22:12:32 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 22:12:32 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 22:12:32 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 22:12:32 INFO mapreduce.Job: Running job: job_local500015672_0001\n",
      "16/02/01 22:12:32 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 22:12:32 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 22:12:32 INFO mapred.LocalJobRunner: Starting task: attempt_local500015672_0001_m_000000_0\n",
      "16/02/01 22:12:32 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 22:12:32 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 22:12:32 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 22:12:32 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/ProductPurchaseData.txt:0+3458517\n",
      "16/02/01 22:12:32 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/01 22:12:32 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 22:12:32 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 22:12:32 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 22:12:32 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 22:12:32 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 22:12:32 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 22:12:32 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./mapper-3-5.py]\n",
      "16/02/01 22:12:32 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 22:12:32 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 22:12:32 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 22:12:32 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 22:12:32 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 22:12:32 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 22:12:32 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 22:12:33 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 22:12:33 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 22:12:33 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 22:12:33 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 22:12:33 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 22:12:33 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:12:33 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:12:33 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:12:33 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:12:33 INFO streaming.PipeMapRed: Records R/W=1216/1\n",
      "16/02/01 22:12:33 INFO mapreduce.Job: Job job_local500015672_0001 running in uber mode : false\n",
      "16/02/01 22:12:33 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/01 22:12:34 INFO streaming.PipeMapRed: R/W/S=10000/120238/0 in:10000=10000/1 [rec/s] out:120238=120238/1 [rec/s]\n",
      "16/02/01 22:12:37 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 22:12:37 INFO mapred.MapTask: bufstart = 0; bufend = 77965406; bufvoid = 104857600\n",
      "16/02/01 22:12:37 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24734212(98936848); length = 1480185/6553600\n",
      "16/02/01 22:12:37 INFO mapred.MapTask: (EQUATOR) 79450094 kvi 19862516(79450064)\n",
      "16/02/01 22:12:38 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 22:12:38 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 22:12:38 INFO mapred.LocalJobRunner: \n",
      "16/02/01 22:12:38 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 22:12:38 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 22:12:38 INFO mapred.MapTask: (RESET) equator 79450094 kv 19862516(79450064) kvi 19819404(79277616)\n",
      "16/02/01 22:12:38 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 22:12:38 INFO mapred.MapTask: bufstart = 79450094; bufend = 81699127; bufvoid = 104857600\n",
      "16/02/01 22:12:38 INFO mapred.MapTask: kvstart = 19862516(79450064); kvend = 19819408(79277632); length = 43109/6553600\n",
      "16/02/01 22:12:38 INFO mapred.MapTask: Finished spill 1\n",
      "16/02/01 22:12:38 INFO mapred.Merger: Merging 2 sorted segments\n",
      "16/02/01 22:12:38 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 81380768 bytes\n",
      "16/02/01 22:12:38 INFO mapred.LocalJobRunner: Records R/W=1216/1 > sort > \n",
      "16/02/01 22:12:39 INFO mapred.Task: Task:attempt_local500015672_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 22:12:39 INFO mapred.LocalJobRunner: Records R/W=1216/1 > sort\n",
      "16/02/01 22:12:39 INFO mapred.Task: Task 'attempt_local500015672_0001_m_000000_0' done.\n",
      "16/02/01 22:12:39 INFO mapred.LocalJobRunner: Finishing task: attempt_local500015672_0001_m_000000_0\n",
      "16/02/01 22:12:39 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 22:12:39 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 22:12:39 INFO mapred.LocalJobRunner: Starting task: attempt_local500015672_0001_r_000000_0\n",
      "16/02/01 22:12:39 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 22:12:39 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 22:12:39 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 22:12:39 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7784e5ce\n",
      "16/02/01 22:12:39 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 22:12:39 INFO reduce.EventFetcher: attempt_local500015672_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 22:12:39 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local500015672_0001_m_000000_0 decomp: 81380774 len: 81380778 to MEMORY\n",
      "16/02/01 22:12:39 INFO reduce.InMemoryMapOutput: Read 81380774 bytes from map-output for attempt_local500015672_0001_m_000000_0\n",
      "16/02/01 22:12:39 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 81380774, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->81380774\n",
      "16/02/01 22:12:39 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 22:12:39 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:12:39 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 22:12:39 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 22:12:39 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 81380770 bytes\n",
      "16/02/01 22:12:39 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/01 22:12:39 INFO reduce.MergeManagerImpl: Merged 1 segments, 81380774 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 22:12:39 INFO reduce.MergeManagerImpl: Merging 1 files, 81380778 bytes from disk\n",
      "16/02/01 22:12:39 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 22:12:39 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 22:12:39 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 81380770 bytes\n",
      "16/02/01 22:12:39 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:12:39 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./reducer-3-5.py]\n",
      "16/02/01 22:12:39 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 22:12:39 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 22:12:39 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:12:39 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:12:39 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:12:40 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:12:40 INFO streaming.PipeMapRed: Records R/W=1285/1\n",
      "16/02/01 22:12:45 INFO mapred.LocalJobRunner: Records R/W=1285/1 > reduce\n",
      "16/02/01 22:12:45 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "16/02/01 22:12:46 INFO streaming.PipeMapRed: R/W/S=10000/8629/0 in:1666=10000/6 [rec/s] out:1438=8629/6 [rec/s]\n",
      "16/02/01 22:12:48 INFO mapred.LocalJobRunner: Records R/W=1285/1 > reduce\n",
      "16/02/01 22:12:48 INFO mapreduce.Job:  map 100% reduce 68%\n",
      "16/02/01 22:12:50 INFO streaming.PipeMapRed: Records R/W=15767/14240\n",
      "16/02/01 22:12:51 INFO mapred.LocalJobRunner: Records R/W=15767/14240 > reduce\n",
      "16/02/01 22:12:54 INFO mapred.LocalJobRunner: Records R/W=15767/14240 > reduce\n",
      "16/02/01 22:12:54 INFO mapreduce.Job:  map 100% reduce 69%\n",
      "16/02/01 22:12:57 INFO mapred.LocalJobRunner: Records R/W=15767/14240 > reduce\n",
      "16/02/01 22:13:00 INFO streaming.PipeMapRed: Records R/W=30208/28476\n",
      "16/02/01 22:13:00 INFO mapred.LocalJobRunner: Records R/W=30208/28476 > reduce\n",
      "16/02/01 22:13:03 INFO mapred.LocalJobRunner: Records R/W=30208/28476 > reduce\n",
      "16/02/01 22:13:03 INFO mapreduce.Job:  map 100% reduce 70%\n",
      "16/02/01 22:13:06 INFO mapred.LocalJobRunner: Records R/W=30208/28476 > reduce\n",
      "16/02/01 22:13:09 INFO mapred.LocalJobRunner: Records R/W=30208/28476 > reduce\n",
      "16/02/01 22:13:12 INFO mapred.LocalJobRunner: Records R/W=30208/28476 > reduce\n",
      "16/02/01 22:13:15 INFO mapred.LocalJobRunner: Records R/W=30208/28476 > reduce\n",
      "16/02/01 22:13:18 INFO mapred.LocalJobRunner: Records R/W=30208/28476 > reduce\n",
      "16/02/01 22:13:21 INFO mapred.LocalJobRunner: Records R/W=30208/28476 > reduce\n",
      "16/02/01 22:13:24 INFO mapred.LocalJobRunner: Records R/W=30208/28476 > reduce\n",
      "16/02/01 22:13:24 INFO mapreduce.Job:  map 100% reduce 71%\n",
      "16/02/01 22:13:26 INFO streaming.PipeMapRed: Records R/W=47902/39259\n",
      "16/02/01 22:13:27 INFO mapred.LocalJobRunner: Records R/W=47902/39259 > reduce\n",
      "16/02/01 22:13:30 INFO mapred.LocalJobRunner: Records R/W=47902/39259 > reduce\n",
      "16/02/01 22:13:33 INFO mapred.LocalJobRunner: Records R/W=47902/39259 > reduce\n",
      "16/02/01 22:13:33 INFO mapreduce.Job:  map 100% reduce 72%\n",
      "16/02/01 22:13:36 INFO mapred.LocalJobRunner: Records R/W=47902/39259 > reduce\n",
      "16/02/01 22:13:39 INFO streaming.PipeMapRed: Records R/W=59014/50463\n",
      "16/02/01 22:13:39 INFO mapred.LocalJobRunner: Records R/W=59014/50463 > reduce\n",
      "16/02/01 22:13:42 INFO mapred.LocalJobRunner: Records R/W=59014/50463 > reduce\n",
      "16/02/01 22:13:45 INFO mapred.LocalJobRunner: Records R/W=59014/50463 > reduce\n",
      "16/02/01 22:13:48 INFO mapred.LocalJobRunner: Records R/W=59014/50463 > reduce\n",
      "16/02/01 22:13:49 INFO mapreduce.Job:  map 100% reduce 73%\n",
      "16/02/01 22:13:49 INFO streaming.PipeMapRed: Records R/W=67632/57791\n",
      "16/02/01 22:13:51 INFO mapred.LocalJobRunner: Records R/W=67632/57791 > reduce\n",
      "16/02/01 22:13:54 INFO mapred.LocalJobRunner: Records R/W=67632/57791 > reduce\n",
      "16/02/01 22:13:57 INFO mapred.LocalJobRunner: Records R/W=67632/57791 > reduce\n",
      "16/02/01 22:13:59 INFO streaming.PipeMapRed: Records R/W=80250/70291\n",
      "16/02/01 22:14:00 INFO mapred.LocalJobRunner: Records R/W=80250/70291 > reduce\n",
      "16/02/01 22:14:01 INFO mapreduce.Job:  map 100% reduce 74%\n",
      "16/02/01 22:14:03 INFO mapred.LocalJobRunner: Records R/W=80250/70291 > reduce\n",
      "16/02/01 22:14:06 INFO mapred.LocalJobRunner: Records R/W=80250/70291 > reduce\n",
      "16/02/01 22:14:09 INFO mapred.LocalJobRunner: Records R/W=80250/70291 > reduce\n",
      "16/02/01 22:14:12 INFO mapred.LocalJobRunner: Records R/W=80250/70291 > reduce\n",
      "16/02/01 22:14:13 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "16/02/01 22:14:15 INFO streaming.PipeMapRed: Records R/W=92631/79778\n",
      "16/02/01 22:14:15 INFO mapred.LocalJobRunner: Records R/W=92631/79778 > reduce\n",
      "16/02/01 22:14:18 INFO mapred.LocalJobRunner: Records R/W=92631/79778 > reduce\n",
      "16/02/01 22:14:19 INFO streaming.PipeMapRed: R/W/S=100000/88829/0 in:1010=100000/99 [rec/s] out:897=88829/99 [rec/s]\n",
      "16/02/01 22:14:21 INFO mapred.LocalJobRunner: Records R/W=92631/79778 > reduce\n",
      "16/02/01 22:14:24 INFO mapred.LocalJobRunner: Records R/W=92631/79778 > reduce\n",
      "16/02/01 22:14:25 INFO streaming.PipeMapRed: Records R/W=103709/92276\n",
      "16/02/01 22:14:25 INFO mapreduce.Job:  map 100% reduce 76%\n",
      "16/02/01 22:14:27 INFO mapred.LocalJobRunner: Records R/W=103709/92276 > reduce\n",
      "16/02/01 22:14:30 INFO mapred.LocalJobRunner: Records R/W=103709/92276 > reduce\n",
      "16/02/01 22:14:33 INFO mapred.LocalJobRunner: Records R/W=103709/92276 > reduce\n",
      "16/02/01 22:14:35 INFO streaming.PipeMapRed: Records R/W=112513/100034\n",
      "16/02/01 22:14:36 INFO mapred.LocalJobRunner: Records R/W=112513/100034 > reduce\n",
      "16/02/01 22:14:37 INFO mapreduce.Job:  map 100% reduce 77%\n",
      "16/02/01 22:14:39 INFO mapred.LocalJobRunner: Records R/W=112513/100034 > reduce\n",
      "16/02/01 22:14:42 INFO mapred.LocalJobRunner: Records R/W=112513/100034 > reduce\n",
      "16/02/01 22:14:43 INFO mapreduce.Job:  map 100% reduce 78%\n",
      "16/02/01 22:14:45 INFO mapred.LocalJobRunner: Records R/W=112513/100034 > reduce\n",
      "16/02/01 22:14:46 INFO streaming.PipeMapRed: Records R/W=129481/116000\n",
      "16/02/01 22:14:48 INFO mapred.LocalJobRunner: Records R/W=129481/116000 > reduce\n",
      "16/02/01 22:14:51 INFO mapred.LocalJobRunner: Records R/W=129481/116000 > reduce\n",
      "16/02/01 22:14:52 INFO mapreduce.Job:  map 100% reduce 79%\n",
      "16/02/01 22:14:54 INFO mapred.LocalJobRunner: Records R/W=129481/116000 > reduce\n",
      "16/02/01 22:14:56 INFO streaming.PipeMapRed: Records R/W=139375/125055\n",
      "16/02/01 22:14:57 INFO mapred.LocalJobRunner: Records R/W=139375/125055 > reduce\n",
      "16/02/01 22:15:00 INFO mapred.LocalJobRunner: Records R/W=139375/125055 > reduce\n",
      "16/02/01 22:15:03 INFO mapred.LocalJobRunner: Records R/W=139375/125055 > reduce\n",
      "16/02/01 22:15:04 INFO mapreduce.Job:  map 100% reduce 80%\n",
      "16/02/01 22:15:06 INFO streaming.PipeMapRed: Records R/W=153671/139288\n",
      "16/02/01 22:15:06 INFO mapred.LocalJobRunner: Records R/W=153671/139288 > reduce\n",
      "16/02/01 22:15:09 INFO mapred.LocalJobRunner: Records R/W=153671/139288 > reduce\n",
      "16/02/01 22:15:12 INFO mapred.LocalJobRunner: Records R/W=153671/139288 > reduce\n",
      "16/02/01 22:15:13 INFO mapreduce.Job:  map 100% reduce 81%\n",
      "16/02/01 22:15:15 INFO mapred.LocalJobRunner: Records R/W=153671/139288 > reduce\n",
      "16/02/01 22:15:17 INFO streaming.PipeMapRed: Records R/W=166391/151797\n",
      "16/02/01 22:15:18 INFO mapred.LocalJobRunner: Records R/W=166391/151797 > reduce\n",
      "16/02/01 22:15:19 INFO mapreduce.Job:  map 100% reduce 82%\n",
      "16/02/01 22:15:21 INFO mapred.LocalJobRunner: Records R/W=166391/151797 > reduce\n",
      "16/02/01 22:15:24 INFO mapred.LocalJobRunner: Records R/W=166391/151797 > reduce\n",
      "16/02/01 22:15:27 INFO mapred.LocalJobRunner: Records R/W=166391/151797 > reduce\n",
      "16/02/01 22:15:30 INFO streaming.PipeMapRed: Records R/W=180911/164309\n",
      "16/02/01 22:15:30 INFO mapred.LocalJobRunner: Records R/W=180911/164309 > reduce\n",
      "16/02/01 22:15:31 INFO mapreduce.Job:  map 100% reduce 83%\n",
      "16/02/01 22:15:33 INFO mapred.LocalJobRunner: Records R/W=180911/164309 > reduce\n",
      "16/02/01 22:15:36 INFO mapred.LocalJobRunner: Records R/W=180911/164309 > reduce\n",
      "16/02/01 22:15:39 INFO mapred.LocalJobRunner: Records R/W=180911/164309 > reduce\n",
      "16/02/01 22:15:42 INFO streaming.PipeMapRed: Records R/W=190054/170348\n",
      "16/02/01 22:15:42 INFO mapred.LocalJobRunner: Records R/W=190054/170348 > reduce\n",
      "16/02/01 22:15:46 INFO mapred.LocalJobRunner: Records R/W=190054/170348 > reduce\n",
      "16/02/01 22:15:46 INFO mapreduce.Job:  map 100% reduce 84%\n",
      "16/02/01 22:15:48 INFO streaming.PipeMapRed: R/W/S=200000/182421/0 in:1063=200000/188 [rec/s] out:970=182421/188 [rec/s]\n",
      "16/02/01 22:15:49 INFO mapred.LocalJobRunner: Records R/W=190054/170348 > reduce\n",
      "16/02/01 22:15:52 INFO mapred.LocalJobRunner: Records R/W=190054/170348 > reduce\n",
      "16/02/01 22:15:52 INFO mapreduce.Job:  map 100% reduce 85%\n",
      "16/02/01 22:15:53 INFO streaming.PipeMapRed: Records R/W=208991/191057\n",
      "16/02/01 22:15:55 INFO mapred.LocalJobRunner: Records R/W=208991/191057 > reduce\n",
      "16/02/01 22:15:58 INFO mapred.LocalJobRunner: Records R/W=208991/191057 > reduce\n",
      "16/02/01 22:16:01 INFO mapred.LocalJobRunner: Records R/W=208991/191057 > reduce\n",
      "16/02/01 22:16:01 INFO mapreduce.Job:  map 100% reduce 86%\n",
      "16/02/01 22:16:03 INFO streaming.PipeMapRed: Records R/W=218190/199676\n",
      "16/02/01 22:16:04 INFO mapred.LocalJobRunner: Records R/W=218190/199676 > reduce\n",
      "16/02/01 22:16:07 INFO mapred.LocalJobRunner: Records R/W=218190/199676 > reduce\n",
      "16/02/01 22:16:10 INFO mapred.LocalJobRunner: Records R/W=218190/199676 > reduce\n",
      "16/02/01 22:16:13 INFO mapred.LocalJobRunner: Records R/W=218190/199676 > reduce\n",
      "16/02/01 22:16:13 INFO streaming.PipeMapRed: Records R/W=230109/210892\n",
      "16/02/01 22:16:13 INFO mapreduce.Job:  map 100% reduce 87%\n",
      "16/02/01 22:16:16 INFO mapred.LocalJobRunner: Records R/W=230109/210892 > reduce\n",
      "16/02/01 22:16:19 INFO mapred.LocalJobRunner: Records R/W=230109/210892 > reduce\n",
      "16/02/01 22:16:19 INFO mapreduce.Job:  map 100% reduce 88%\n",
      "16/02/01 22:16:22 INFO mapred.LocalJobRunner: Records R/W=230109/210892 > reduce\n",
      "16/02/01 22:16:23 INFO streaming.PipeMapRed: Records R/W=241847/220815\n",
      "16/02/01 22:16:25 INFO mapred.LocalJobRunner: Records R/W=241847/220815 > reduce\n",
      "16/02/01 22:16:28 INFO mapred.LocalJobRunner: Records R/W=241847/220815 > reduce\n",
      "16/02/01 22:16:31 INFO mapred.LocalJobRunner: Records R/W=241847/220815 > reduce\n",
      "16/02/01 22:16:31 INFO mapreduce.Job:  map 100% reduce 89%\n",
      "16/02/01 22:16:33 INFO streaming.PipeMapRed: Records R/W=255543/233751\n",
      "16/02/01 22:16:34 INFO mapred.LocalJobRunner: Records R/W=255543/233751 > reduce\n",
      "16/02/01 22:16:37 INFO mapred.LocalJobRunner: Records R/W=255543/233751 > reduce\n",
      "16/02/01 22:16:40 INFO mapred.LocalJobRunner: Records R/W=255543/233751 > reduce\n",
      "16/02/01 22:16:41 INFO mapreduce.Job:  map 100% reduce 90%\n",
      "16/02/01 22:16:43 INFO mapred.LocalJobRunner: Records R/W=255543/233751 > reduce\n",
      "16/02/01 22:16:44 INFO streaming.PipeMapRed: Records R/W=268706/245829\n",
      "16/02/01 22:16:46 INFO mapred.LocalJobRunner: Records R/W=268706/245829 > reduce\n",
      "16/02/01 22:16:49 INFO mapred.LocalJobRunner: Records R/W=268706/245829 > reduce\n",
      "16/02/01 22:16:50 INFO mapreduce.Job:  map 100% reduce 91%\n",
      "16/02/01 22:16:52 INFO mapred.LocalJobRunner: Records R/W=268706/245829 > reduce\n",
      "16/02/01 22:16:54 INFO streaming.PipeMapRed: Records R/W=279951/256604\n",
      "16/02/01 22:16:55 INFO mapred.LocalJobRunner: Records R/W=279951/256604 > reduce\n",
      "16/02/01 22:16:58 INFO mapred.LocalJobRunner: Records R/W=279951/256604 > reduce\n",
      "16/02/01 22:17:01 INFO mapred.LocalJobRunner: Records R/W=279951/256604 > reduce\n",
      "16/02/01 22:17:02 INFO mapreduce.Job:  map 100% reduce 92%\n",
      "16/02/01 22:17:04 INFO mapred.LocalJobRunner: Records R/W=279951/256604 > reduce\n",
      "16/02/01 22:17:07 INFO mapred.LocalJobRunner: Records R/W=279951/256604 > reduce\n",
      "16/02/01 22:17:07 INFO streaming.PipeMapRed: Records R/W=288502/261774\n",
      "16/02/01 22:17:10 INFO mapred.LocalJobRunner: Records R/W=288502/261774 > reduce\n",
      "16/02/01 22:17:13 INFO mapred.LocalJobRunner: Records R/W=288502/261774 > reduce\n",
      "16/02/01 22:17:14 INFO mapreduce.Job:  map 100% reduce 93%\n",
      "16/02/01 22:17:15 INFO streaming.PipeMapRed: R/W/S=300000/274275/0 in:1090=300000/275 [rec/s] out:997=274275/275 [rec/s]\n",
      "16/02/01 22:17:16 INFO mapred.LocalJobRunner: Records R/W=288502/261774 > reduce\n",
      "16/02/01 22:17:19 INFO mapred.LocalJobRunner: Records R/W=288502/261774 > reduce\n",
      "16/02/01 22:17:19 INFO streaming.PipeMapRed: Records R/W=305190/278592\n",
      "16/02/01 22:17:22 INFO mapred.LocalJobRunner: Records R/W=305190/278592 > reduce\n",
      "16/02/01 22:17:22 INFO mapreduce.Job:  map 100% reduce 94%\n",
      "16/02/01 22:17:25 INFO mapred.LocalJobRunner: Records R/W=305190/278592 > reduce\n",
      "16/02/01 22:17:28 INFO mapred.LocalJobRunner: Records R/W=305190/278592 > reduce\n",
      "16/02/01 22:17:28 INFO mapreduce.Job:  map 100% reduce 95%\n",
      "16/02/01 22:17:29 INFO streaming.PipeMapRed: Records R/W=326810/301038\n",
      "16/02/01 22:17:31 INFO mapred.LocalJobRunner: Records R/W=326810/301038 > reduce\n",
      "16/02/01 22:17:34 INFO mapred.LocalJobRunner: Records R/W=326810/301038 > reduce\n",
      "16/02/01 22:17:34 INFO mapreduce.Job:  map 100% reduce 96%\n",
      "16/02/01 22:17:37 INFO mapred.LocalJobRunner: Records R/W=326810/301038 > reduce\n",
      "16/02/01 22:17:39 INFO streaming.PipeMapRed: Records R/W=338731/312685\n",
      "16/02/01 22:17:40 INFO mapred.LocalJobRunner: Records R/W=338731/312685 > reduce\n",
      "16/02/01 22:17:43 INFO mapred.LocalJobRunner: Records R/W=338731/312685 > reduce\n",
      "16/02/01 22:17:43 INFO mapreduce.Job:  map 100% reduce 97%\n",
      "16/02/01 22:17:46 INFO mapred.LocalJobRunner: Records R/W=338731/312685 > reduce\n",
      "16/02/01 22:17:49 INFO mapred.LocalJobRunner: Records R/W=338731/312685 > reduce\n",
      "16/02/01 22:17:50 INFO streaming.PipeMapRed: Records R/W=352647/325192\n",
      "16/02/01 22:17:52 INFO mapred.LocalJobRunner: Records R/W=352647/325192 > reduce\n",
      "16/02/01 22:17:52 INFO mapreduce.Job:  map 100% reduce 98%\n",
      "16/02/01 22:17:55 INFO mapred.LocalJobRunner: Records R/W=352647/325192 > reduce\n",
      "16/02/01 22:17:58 INFO mapred.LocalJobRunner: Records R/W=352647/325192 > reduce\n",
      "16/02/01 22:18:00 INFO streaming.PipeMapRed: Records R/W=362590/332960\n",
      "16/02/01 22:18:01 INFO mapred.LocalJobRunner: Records R/W=362590/332960 > reduce\n",
      "16/02/01 22:18:04 INFO mapred.LocalJobRunner: Records R/W=362590/332960 > reduce\n",
      "16/02/01 22:18:04 INFO mapreduce.Job:  map 100% reduce 99%\n",
      "16/02/01 22:18:07 INFO mapred.LocalJobRunner: Records R/W=362590/332960 > reduce\n",
      "16/02/01 22:18:10 INFO mapred.LocalJobRunner: Records R/W=362590/332960 > reduce\n",
      "16/02/01 22:18:10 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 22:18:10 INFO streaming.PipeMapRed: Records R/W=376284/347190\n",
      "16/02/01 22:18:13 INFO mapred.LocalJobRunner: Records R/W=376284/347190 > reduce\n",
      "16/02/01 22:18:16 INFO mapred.LocalJobRunner: Records R/W=376284/347190 > reduce\n",
      "16/02/01 22:18:19 INFO mapred.LocalJobRunner: Records R/W=376284/347190 > reduce\n",
      "16/02/01 22:18:19 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 22:18:19 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 22:18:19 INFO mapred.Task: Task:attempt_local500015672_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 22:18:19 INFO mapred.LocalJobRunner: Records R/W=376284/347190 > reduce\n",
      "16/02/01 22:18:19 INFO mapred.Task: Task attempt_local500015672_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 22:18:19 INFO output.FileOutputCommitter: Saved output of task 'attempt_local500015672_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/sortedProducts/_temporary/0/task_local500015672_0001_r_000000\n",
      "16/02/01 22:18:19 INFO mapred.LocalJobRunner: Records R/W=376284/347190 > reduce\n",
      "16/02/01 22:18:19 INFO mapred.Task: Task 'attempt_local500015672_0001_r_000000_0' done.\n",
      "16/02/01 22:18:19 INFO mapred.LocalJobRunner: Finishing task: attempt_local500015672_0001_r_000000_0\n",
      "16/02/01 22:18:19 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 22:18:20 INFO mapreduce.Job: Job job_local500015672_0001 completed successfully\n",
      "16/02/01 22:18:20 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=325735252\n",
      "\t\tFILE: Number of bytes written=407702766\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6917034\n",
      "\t\tHDFS: Number of bytes written=13399988\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=380825\n",
      "\t\tMap output bytes=80214439\n",
      "\t\tMap output materialized bytes=81380778\n",
      "\t\tInput split bytes=111\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12593\n",
      "\t\tReduce shuffle bytes=81380778\n",
      "\t\tReduce input records=380825\n",
      "\t\tReduce output records=352734\n",
      "\t\tSpilled Records=1142475\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=9\n",
      "\t\tTotal committed heap usage (bytes)=575668224\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=13399988\n",
      "16/02/01 22:18:20 INFO streaming.StreamJob: Output directory: sortedProducts\n",
      "CPU times: user 1.06 s, sys: 248 ms, total: 1.3 s\n",
      "Wall time: 5min 50s\n"
     ]
    }
   ],
   "source": [
    "time !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper-3-5.py \\\n",
    "-reducer reducer-3-5.py \\\n",
    "-input ProductPurchaseData.txt \\\n",
    "-output sortedProducts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 22:20:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 22:20:56 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 22:20:56 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 22:20:56 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 22:20:56 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 22:20:56 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 22:20:56 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/02/01 22:20:56 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/02/01 22:20:56 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local692637415_0001\n",
      "16/02/01 22:20:56 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 22:20:56 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 22:20:56 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 22:20:56 INFO mapreduce.Job: Running job: job_local692637415_0001\n",
      "16/02/01 22:20:56 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 22:20:56 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 22:20:56 INFO mapred.LocalJobRunner: Starting task: attempt_local692637415_0001_m_000000_0\n",
      "16/02/01 22:20:56 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 22:20:56 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 22:20:56 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 22:20:56 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/sortedProducts/part-00000:0+13399988\n",
      "16/02/01 22:20:56 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/01 22:20:56 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 22:20:56 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 22:20:56 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 22:20:56 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 22:20:56 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 22:20:56 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 22:20:56 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./mapper-3-5-2.py]\n",
      "16/02/01 22:20:56 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 22:20:56 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 22:20:56 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 22:20:56 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 22:20:56 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 22:20:56 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 22:20:56 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 22:20:56 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 22:20:56 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 22:20:56 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 22:20:56 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 22:20:56 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 22:20:56 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:20:56 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:20:56 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:20:56 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:20:57 INFO streaming.PipeMapRed: Records R/W=3455/1\n",
      "16/02/01 22:20:57 INFO streaming.PipeMapRed: R/W/S=10000/13458/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:20:57 INFO streaming.PipeMapRed: R/W/S=100000/191045/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:20:57 INFO streaming.PipeMapRed: R/W/S=200000/390742/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:20:57 INFO streaming.PipeMapRed: R/W/S=300000/590191/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:20:57 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 22:20:57 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 22:20:57 INFO mapred.LocalJobRunner: \n",
      "16/02/01 22:20:57 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 22:20:57 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 22:20:57 INFO mapred.MapTask: bufstart = 0; bufend = 14105456; bufvoid = 104857600\n",
      "16/02/01 22:20:57 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 23392528(93570112); length = 2821869/6553600\n",
      "16/02/01 22:20:57 INFO mapreduce.Job: Job job_local692637415_0001 running in uber mode : false\n",
      "16/02/01 22:20:57 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/01 22:20:59 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 22:20:59 INFO mapred.Task: Task:attempt_local692637415_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 22:20:59 INFO mapred.LocalJobRunner: Records R/W=3455/1\n",
      "16/02/01 22:20:59 INFO mapred.Task: Task 'attempt_local692637415_0001_m_000000_0' done.\n",
      "16/02/01 22:20:59 INFO mapred.LocalJobRunner: Finishing task: attempt_local692637415_0001_m_000000_0\n",
      "16/02/01 22:20:59 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 22:20:59 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 22:20:59 INFO mapred.LocalJobRunner: Starting task: attempt_local692637415_0001_r_000000_0\n",
      "16/02/01 22:20:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 22:20:59 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 22:20:59 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 22:20:59 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@65cd4c97\n",
      "16/02/01 22:20:59 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 22:20:59 INFO reduce.EventFetcher: attempt_local692637415_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 22:20:59 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local692637415_0001_m_000000_0 decomp: 15516394 len: 15516398 to MEMORY\n",
      "16/02/01 22:20:59 INFO reduce.InMemoryMapOutput: Read 15516394 bytes from map-output for attempt_local692637415_0001_m_000000_0\n",
      "16/02/01 22:20:59 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 15516394, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->15516394\n",
      "16/02/01 22:20:59 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 22:20:59 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:20:59 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 22:20:59 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 22:20:59 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 15516391 bytes\n",
      "16/02/01 22:21:00 INFO reduce.MergeManagerImpl: Merged 1 segments, 15516394 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 22:21:00 INFO reduce.MergeManagerImpl: Merging 1 files, 15516398 bytes from disk\n",
      "16/02/01 22:21:00 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 22:21:00 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 22:21:00 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 15516391 bytes\n",
      "16/02/01 22:21:00 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:21:00 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./reducer-3-5-2.py]\n",
      "16/02/01 22:21:00 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 22:21:00 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 22:21:00 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:21:00 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:21:00 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:21:00 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:21:00 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dunmireg/Documents/261HW/HW3/./reducer-3-5-2.py\", line 8, in <module>\n",
      "    print line[1] + '\\t' + line[0] + '\\t'+ line[2]\n",
      "IndexError: list index out of range\n",
      "16/02/01 22:21:00 INFO streaming.PipeMapRed: Records R/W=65537/1\n",
      "16/02/01 22:21:00 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 22:21:00 WARN streaming.PipeMapRed: java.io.IOException: Stream closed\n",
      "16/02/01 22:21:00 INFO streaming.PipeMapRed: PipeMapRed failed!\n",
      "java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:322)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:535)\n",
      "\tat org.apache.hadoop.streaming.PipeReducer.reduce(PipeReducer.java:128)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\n",
      "\tat org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "16/02/01 22:21:00 WARN streaming.PipeMapRed: java.io.IOException: Stream closed\n",
      "16/02/01 22:21:00 INFO streaming.PipeMapRed: PipeMapRed failed!\n",
      "java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:322)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:535)\n",
      "\tat org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)\n",
      "\tat org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:244)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\n",
      "\tat org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "16/02/01 22:21:00 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 22:21:00 WARN mapred.LocalJobRunner: job_local692637415_0001\n",
      "java.lang.Exception: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\n",
      "\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)\n",
      "Caused by: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:322)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:535)\n",
      "\tat org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)\n",
      "\tat org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:244)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\n",
      "\tat org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "16/02/01 22:21:00 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/01 22:21:00 INFO mapreduce.Job: Job job_local692637415_0001 failed with state FAILED due to: NA\n",
      "16/02/01 22:21:00 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=106050\n",
      "\t\tFILE: Number of bytes written=15916912\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=13399988\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=5\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=1\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=352734\n",
      "\t\tMap output records=705468\n",
      "\t\tMap output bytes=14105456\n",
      "\t\tMap output materialized bytes=15516398\n",
      "\t\tInput split bytes=113\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=0\n",
      "\t\tReduce shuffle bytes=15516398\n",
      "\t\tReduce input records=0\n",
      "\t\tReduce output records=0\n",
      "\t\tSpilled Records=705468\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=310902784\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=13399988\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n",
      "16/02/01 22:21:00 ERROR streaming.StreamJob: Job not successful!\n",
      "Streaming Command Failed!\n",
      "16/02/01 22:21:00 WARN hdfs.DFSClient: DataStreamer Exception\n",
      "org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dunmireg/numSortedProducts/_temporary/0/_temporary/attempt_local692637415_0001_r_000000_0/part-00000 (inode 16937): File does not exist. Holder DFSClient_NONMAPREDUCE_-883251745_1 does not have any open files.\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3431)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3236)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3074)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3034)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:723)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)\n",
      "\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1476)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1407)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n",
      "\tat com.sun.proxy.$Proxy9.addBlock(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:418)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:497)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n",
      "\tat com.sun.proxy.$Proxy10.addBlock(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1430)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1226)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:449)\n",
      "16/02/01 22:21:00 ERROR hdfs.DFSClient: Failed to close inode 16937\n",
      "org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dunmireg/numSortedProducts/_temporary/0/_temporary/attempt_local692637415_0001_r_000000_0/part-00000 (inode 16937): File does not exist. Holder DFSClient_NONMAPREDUCE_-883251745_1 does not have any open files.\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3431)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3236)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3074)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3034)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:723)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)\n",
      "\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1476)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1407)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n",
      "\tat com.sun.proxy.$Proxy9.addBlock(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:418)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:497)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n",
      "\tat com.sun.proxy.$Proxy10.addBlock(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1430)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1226)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:449)\n"
     ]
    }
   ],
   "source": [
    "#run second job to properly sort by counts\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D  mapred.text.key.comparator.options=-n \\\n",
    "-mapper mapper-3-5-2.py \\\n",
    "-reducer reducer-3-5-2.py \\\n",
    "-input /user/dunmireg/sortedProducts/part-00000 \\\n",
    "-output numSortedProducts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 22:27:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 22:27:23 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/ProductPurchaseData.txt\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 22:27:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 22:27:25 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/sortedProducts\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/02/01 22:27:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 22:27:27 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/numSortedProducts\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/02/01 22:27:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/02/01 22:27:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Remove output directory and stop yarn and hdfs\n",
    "!hadoop fs -rmr /user/dunmireg/ProductPurchaseData.txt\n",
    "!hadoop fs -rmr /user/dunmireg/sortedProducts\n",
    "!hadoop fs -rmr /user/dunmireg/numSortedProducts\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
