{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is MrJob? How is it different to Hadoop MapReduce? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MRJob is a Python framework to make running complex Map Reduce tasks much simpler. It is capable of running sequences of MapReduce or even iterative MapReduce jobs. The really nice thing about MRJob is the almost pseudo-code like way of expressing how to execute and combine MapReduce jobs.\n",
    "\n",
    "MRJob is not Hadoop but it can execute in a stand-alone mode to run your MapReduce jobs, useful for small scale testing. MRJob also can submit your job to Hadoop via the Streaming API, whether on a local or remote Hadoop cluster. In addition, MRJob has a very nice integration with Amazon AWS Elastic Map Reduce, allowing the researcher to focus on the MapReduce and analysis instead of the infrastructure on which to execute it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the mapper_init, mapper_final(), combiner_final(), reducer_final() methods? When are they called?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MRJob defines a base class that you as the developer must override to use MRJob. The base class executes the mapper, reducer, and combiner functions when you override them in the class. The MRJob base class also provides intializer and finalizer methods for each of the mapper, combiner and reducer functions. These methods are `mapper_init()`, `combiner_init()`, `reducer_init()`, `mapper_final()`, `combiner_final()`, and `reducer_final()` respectively. The init() methods are called before the corresponding `mapper()`, `combiner()`, `reducer()` methods, allowing setup of data or other things before the method is called. The final() methods are called immediately after the `mapper()`, `reducer()` or `combiner()` methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is serialization in the context of MrJob or Hadoop?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serialization is the process of converting a machine representation of an object to a format used for storage or transmission. In the context of Hadoop Streaming all input and output is treated as a character stream with keys and values separated by tabs (or another specified delimiter). In the case of MRJob, serialization consists of three types: raw, json, or pickle. Raw is text streams, json is json formatted text streams, and pickle is the Python binary serialization method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When it used in these frameworks?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MRJob uses serialization for input and output as well as internal transmission of objects. Each place serialization is used can be defined by the type of protocol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the default serialization mode for input and outputs for MrJob? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default serialization mode for MRJob inputs is `RAWValueProtocol` which reads lines of text with no key - it's just a stream of text. The default output protocol is `JSONprotocol` which outputs JSON formatted strings separated by a tab character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.2: \n",
    "\n",
    "Recall the Microsoft logfiles data from the async lecture. The logfiles are described are located at:\n",
    "\n",
    "https://kdd.ics.uci.edu/databases/msweb/msweb.html<br/>\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/<br/>\n",
    "\n",
    "This dataset records which areas (Vroots) of www.microsoft.com each user visited in a one-week timeframe in Feburary 1998.\n",
    "\n",
    "Here, you must preprocess the data on a single node (i.e., not on a cluster of nodes) from the format:\n",
    "\n",
    "    C,\"10001\",10001   #Visitor id 10001\n",
    "    V,1000,1          #Visit by Visitor 10001 to page id 1000\n",
    "    V,1001,1          #Visit by Visitor 10001 to page id 1001\n",
    "    V,1002,1          #Visit by Visitor 10001 to page id 1002\n",
    "    C,\"10002\",10002   #Visitor id 10001\n",
    "\n",
    "V\n",
    "Note: #denotes comments to the format:\n",
    "\n",
    "    V,1000,1,C, 10001\n",
    "    V,1001,1,C, 10001\n",
    "    V,1002,1,C, 10001\n",
    "\n",
    "Write the python code to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make input file\n",
    "import csv\n",
    "\n",
    "with open('anonymous-msweb.data', 'r') as inputFile, open('msweblog.csv', 'wb') as outputFile: #get appropriate files\n",
    "    writer = csv.writer(outputFile) #use csv.writer for writiing output\n",
    "    lines = inputFile.readlines() #get input files\n",
    "    currID = None #stores current ID\n",
    "    for line in lines:\n",
    "        line = line.split(',') #split on comma delimiter\n",
    "        if line[0] == 'C': #if the line starts with C, that means we're dealing with a new customer ID\n",
    "            currID = int(line[2]) #Set the customer ID\n",
    "        elif line[0] == 'V': #otherwise if V, that's the visiting behavior (we ignore all other lines)\n",
    "            newLine = ['V']\n",
    "            newLine.extend((line[1], '1', 'C', currID)) #construct a new line\n",
    "            writer.writerow(newLine) #write output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.3: \n",
    "\n",
    "Find the 5 most frequently visited pages using MrJob from the output of 4.2 (i.e., transfromed log file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile mostvisitedpage.py\n",
    "from mrjob.job import MRJob \n",
    "from mrjob.step import MRStep\n",
    "import heapq\n",
    "\n",
    "class MRMostVisitedPage(MRJob):\n",
    "    def mapper_get_visits(self, _, record):\n",
    "        self.increment_counter('Execution Counts', 'mapper calls', 1)\n",
    "        # yield each visit in the line\n",
    "        tokens = record.split(',')\n",
    "        if tokens[0] == 'V':\n",
    "            yield (tokens[1], 1)\n",
    "\n",
    "    def combiner_count_visits(self, page, counts): \n",
    "        self.increment_counter('Execution Counts', 'combiner calls', 1)\n",
    "        # sum the page visits we've seen so far\n",
    "        yield (page, sum(counts))\n",
    "        \n",
    "    def reducer_count_visits(self, page, counts):\n",
    "        self.increment_counter('Execution Counts', 'reducer_count calls', 1)\n",
    "        # send all (num_occurrences, word) pairs to the same reducer.\n",
    "        # num_occurrences is so we can easily use Python's max() function. yield None, (sum(counts), page)\n",
    "        # discard the key; it is just None\n",
    "        yield None, (sum(counts), page)\n",
    "        \n",
    "    def reducer_find_top5_visits(self, _, page_count_pairs):\n",
    "        self.increment_counter('Execution Counts', 'reducer_find_max calls', 1)\n",
    "        # each item of page_count_pairs is (count, page),\n",
    "        # so yielding one results in key=counts, value=page yield max(page_count_pairs)\n",
    "        return heapq.nlargest(5, page_count_pairs)\n",
    "\n",
    "        \n",
    "    def steps(self): return [\n",
    "            MRStep(mapper=self.mapper_get_visits,\n",
    "                   combiner=self.combiner_count_visits,\n",
    "                   reducer=self.reducer_count_visits),\n",
    "            MRStep(reducer=self.reducer_find_top5_visits)\n",
    "        ]\n",
    "    \n",
    "if __name__ == '__main__': \n",
    "    MRMostVisitedPage.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python mostvisitedpage.py anonymous-msweb-transformed.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: Copied from working version on local machine. Emphasis added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no configs found; falling back on auto-configuration\n",
    "no configs found; falling back on auto-configuration\n",
    "creating tmp directory /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostvisitedpage.rcordell.20160208.015734.738794\n",
    "writing to /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostvisitedpage.rcordell.20160208.015734.738794/step-0-mapper_part-00000\n",
    "Counters from step 1:\n",
    "  Execution Counts:\n",
    "    combiner calls: 285\n",
    "    mapper calls: 98955\n",
    "writing to /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostvisitedpage.rcordell.20160208.015734.738794/step-0-mapper-sorted\n",
    "sort /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostvisitedpage.rcordell.20160208.015734.738794/step-0-mapper_part-00000\n",
    "writing to /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostvisitedpage.rcordell.20160208.015734.738794/step-0-reducer_part-00000\n",
    "Counters from step 1:\n",
    "  Execution Counts:\n",
    "    combiner calls: 285\n",
    "    mapper calls: 98955\n",
    "    reducer_count calls: 285\n",
    "writing to /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostvisitedpage.rcordell.20160208.015734.738794/step-1-mapper_part-00000\n",
    "Counters from step 2:\n",
    "  (no counters found)\n",
    "writing to /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostvisitedpage.rcordell.20160208.015734.738794/step-1-mapper-sorted\n",
    "sort /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostvisitedpage.rcordell.20160208.015734.738794/step-1-mapper_part-00000\n",
    "writing to /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostvisitedpage.rcordell.20160208.015734.738794/step-1-reducer_part-00000\n",
    "Counters from step 2:\n",
    "  Execution Counts:\n",
    "    reducer_find_max calls: 1\n",
    "Moving /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostvisitedpage.rcordell.20160208.015734.738794/step-1-reducer_part-00000 -> /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostvisitedpage.rcordell.20160208.015734.738794/output/part-00000\n",
    "Streaming final output from /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostvisitedpage.rcordell.20160208.015734.738794/output\n",
    "\n",
    "__10836\t\"1008\"\n",
    "9383\t\"1034\"\n",
    "8463\t\"1004\"\n",
    "5330\t\"1018\"\n",
    "5108\t\"1017\"__\n",
    "\n",
    "removing tmp directory /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostvisitedpage.rcordell.20160208.015734.738794"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.4: \n",
    "\n",
    "Find the most frequent visitor of each page using MrJob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile mostfreqvisitors.py\n",
    "from mrjob.job import MRJob \n",
    "from mrjob.step import MRStep \n",
    "\n",
    "class MRMostFrequentVisitors(MRJob):\n",
    "    def configure_options(self):\n",
    "        super(MRMostFrequentVisitors, self).configure_options()\n",
    "        self.SORT_VALUES = True\n",
    "        \n",
    "    # generate a dictionary of pages and URLs for them\n",
    "    def mapper_get_visits_init(self):\n",
    "        # create a dictionary to use for the page URLs and ids\n",
    "        self.pages = {}\n",
    "        \n",
    "    # generate keys of page,customer,url and values of 1\n",
    "    def mapper_get_visits(self, _, record):\n",
    "        self.increment_counter('Execution Counts', 'mapper calls', 1)\n",
    "        tokens = record.split(',')\n",
    "        \n",
    "        # the page definitions come first in the file so create a dictionary from them.\n",
    "        if tokens[0] == 'A':\n",
    "            self.pages[tokens[1]] = tokens[4].strip('\"')\n",
    "            \n",
    "        # emit a key = (page_id, client_id, url) and value = 1\n",
    "        elif tokens[0] == 'V':\n",
    "            yield ((tokens[1], tokens[4], self.pages[tokens[1]]), 1)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # combine page visits by key where the key is page,customer\n",
    "    def combiner_count_visits(self, key, counts): \n",
    "        self.increment_counter('Execution Counts', 'combiner count visits', 1)\n",
    "        # sum the keys we've seen so far.\n",
    "        # the key is (page_id, cust_id, page_url) so we're counting page views by client\n",
    "        yield (key, sum(counts))\n",
    "        \n",
    "    # set up instance variables to use to calculate the max visits to a page by a single customer\n",
    "    def reducer_count_visits_init(self):\n",
    "        self.current_page = None\n",
    "        self.max_count = 0\n",
    "        \n",
    "    # count the visits per page per customer and also compute the max visits per page by a single customer\n",
    "    def reducer_count_visits(self, key, counts):\n",
    "        self.increment_counter('Execution Counts', 'reducer_count visits', 1)\n",
    "        # make sure we have sums of all keys \n",
    "        s = sum(counts)\n",
    "        if self.current_page == key[0]:\n",
    "            if self.max_count < s:\n",
    "                self.max_count = s\n",
    "        else:\n",
    "            if self.current_page:\n",
    "                p = self.current_page\n",
    "                t = self.max_count\n",
    "                yield((self.current_page,'*',key[2]), t)\n",
    "                \n",
    "            self.current_page = key[0]\n",
    "            self.max_count = s\n",
    "\n",
    "        yield (key, s)\n",
    "\n",
    "    # set up a variable to contain the current page max count value\n",
    "    def reducer_find_max_visits_init(self):\n",
    "        self.page_max = 0\n",
    "     \n",
    "    # yield the max visits to a page and the customers that made them\n",
    "    def reducer_find_max_visits(self, key, counts):\n",
    "        self.increment_counter('Execution Counts', 'reducer_find_max visits', 1)\n",
    "        \n",
    "        # if this is the key with the max visits for the page then stash it\n",
    "        if key[1] == '*':\n",
    "            self.page_max = sum(counts)\n",
    "        else:\n",
    "            # otherwise sum the counts and store a local copy because it exhausts the generator\n",
    "            p = sum(counts)\n",
    "            # if this count is the same as the max visits for the page, yield it\n",
    "            if p == self.page_max:\n",
    "                yield key, p\n",
    "        \n",
    "          \n",
    "    def steps(self): return [\n",
    "            MRStep(mapper_init=self.mapper_get_visits_init,\n",
    "                    mapper=self.mapper_get_visits,\n",
    "                   combiner=self.combiner_count_visits,\n",
    "                   reducer_init=self.reducer_count_visits_init,\n",
    "                   reducer=self.reducer_count_visits),\n",
    "            MRStep(reducer_init=self.reducer_find_max_visits_init,\n",
    "                    reducer=self.reducer_find_max_visits)\n",
    "        ]\n",
    "    \n",
    "if __name__ == '__main__': \n",
    "    MRMostFrequentVisitors.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python mostfreqvisitors.py anonymous-msweb-transformed.data > max_page_visits_customer.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NB from working notebook on local machine*\n",
    "\n",
    "no configs found; falling back on auto-configuration\n",
    "no configs found; falling back on auto-configuration\n",
    "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
    "creating tmp directory /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostfreqvisitors.rcordell.20160208.053219.800269\n",
    "writing to /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostfreqvisitors.rcordell.20160208.053219.800269/step-0-mapper_part-00000\n",
    "Counters from step 1:\n",
    "  Execution Counts:\n",
    "    combiner count visits: 98654\n",
    "    mapper calls: 98955\n",
    "writing to /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostfreqvisitors.rcordell.20160208.053219.800269/step-0-mapper-sorted\n",
    "sort /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostfreqvisitors.rcordell.20160208.053219.800269/step-0-mapper_part-00000\n",
    "writing to /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostfreqvisitors.rcordell.20160208.053219.800269/step-0-reducer_part-00000\n",
    "Counters from step 1:\n",
    "  Execution Counts:\n",
    "    combiner count visits: 98654\n",
    "    mapper calls: 98955\n",
    "    reducer_count visits: 98654\n",
    "writing to /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostfreqvisitors.rcordell.20160208.053219.800269/step-1-mapper_part-00000\n",
    "Counters from step 2:\n",
    "  (no counters found)\n",
    "writing to /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostfreqvisitors.rcordell.20160208.053219.800269/step-1-mapper-sorted\n",
    "sort /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostfreqvisitors.rcordell.20160208.053219.800269/step-1-mapper_part-00000\n",
    "writing to /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostfreqvisitors.rcordell.20160208.053219.800269/step-1-reducer_part-00000\n",
    "Counters from step 2:\n",
    "  Execution Counts:\n",
    "    reducer_find_max visits: 98938\n",
    "Moving /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostfreqvisitors.rcordell.20160208.053219.800269/step-1-reducer_part-00000 -> /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostfreqvisitors.rcordell.20160208.053219.800269/output/part-00000\n",
    "Streaming final output from /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostfreqvisitors.rcordell.20160208.053219.800269/output\n",
    "removing tmp directory /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/mostfreqvisitors.rcordell.20160208.053219.800269"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat max_page_visits_customer.output | head -100"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[\"1000\", \"10001\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10010\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10039\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10073\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10087\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10101\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10132\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10141\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10154\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10162\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10166\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10201\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10218\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10220\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10324\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10348\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10376\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10384\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10409\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10429\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10454\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10457\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10471\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10497\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10511\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10520\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10541\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10564\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10599\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10752\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10756\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10861\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10935\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10943\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"10969\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"11027\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"11050\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"11410\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"11429\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"11440\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"11490\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"11501\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"11528\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"11539\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"11544\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"11685\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"11695\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"11723\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"11766\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"11774\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"11779\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"11898\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"11964\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12017\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12020\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12035\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12086\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12123\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12143\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12155\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12201\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12220\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12228\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12262\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12273\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12306\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12315\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12324\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12337\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12343\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12400\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12415\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12484\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12485\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12537\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12571\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12583\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12674\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12700\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12740\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12815\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12853\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12893\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12897\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12930\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12944\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12970\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"12982\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"13015\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"13049\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"13079\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"13080\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"13085\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"13128\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"13176\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"13197\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"13223\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"13248\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"13275\", \"/regwiz\"]\t1\n",
    "[\"1000\", \"13294\", \"/regwiz\"]\t1\n",
    "cat: stdout: Broken pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat max_page_visits_customer.output | tail -100"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[\"1295\", \"38244\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"38296\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"38313\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"38454\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"38571\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"38573\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"38661\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"38678\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"38755\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"38831\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"38869\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"38953\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"38981\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"38998\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"39024\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"39033\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"39058\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"39066\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"39094\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"39105\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"39112\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"39131\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"39194\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"39221\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"39284\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"39293\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"39493\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"39505\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"39550\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"39604\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"39617\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"39627\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"39645\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"39719\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"39730\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"39760\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"39863\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"39899\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"39900\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"39902\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"39948\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"39977\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"40025\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"40046\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"40207\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"40233\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"40274\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"40310\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"40390\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"40419\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"40482\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"40597\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"40616\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"40679\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"40758\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"40787\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"40827\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"40923\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"40930\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"40942\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"40946\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"40965\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"41068\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"41075\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"41093\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"41100\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"41117\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"41175\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"41183\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"41207\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"41255\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"41269\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"41273\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"41367\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"41429\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"41580\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"41594\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"41598\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"41692\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"41715\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"41730\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"41748\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"41843\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"41953\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"42065\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"42146\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"42161\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"42198\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"42234\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"42241\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"42262\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"42313\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"42353\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"42385\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"42497\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"42516\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"42568\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"42576\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"42600\", \"/train_cert\"]\t1\n",
    "[\"1295\", \"42616\", \"/train_cert\"]\t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that no user visited a webpage more than once. That is each user visited each vroot exactly one time, meaning all visitors to a vroot are tied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.5 \n",
    "*Implement a 1000-dimensional K-means algorithm in MrJob on the users\n",
    "by their 1000-dimensional word stripes/vectors using several \n",
    "centroid initializations and values of K:*\n",
    "\n",
    "* *(A) K=4 uniform random centroid-distributions over the 1000 words*\n",
    "* *(B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution *\n",
    "* *(C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution *\n",
    "* *(D) K=4 \"trained\" centroids, determined by the sums across the classes.*\n",
    "\n",
    "*Report the composition as measured by the total portion of each class type (0-3) contained in each cluster, and discuss your findings and any differences in outcomes across parts A-D.*\n",
    "\n",
    "#### Solution\n",
    "\n",
    "To accomplish this task in an extensible way, we wrote our `kmeans` architecture in a way that compartmentalizes different portions of the algorithm. This allows us to change properties of the `kmeans` iteration, such as initialization type, without substantially changing the code. \n",
    "\n",
    "##### Driver: \n",
    "\n",
    "Below is the driver, `kmeans.py` that is used to initialize the MRJob. It takes two arguments - initialization type and number of clusters. The initialization type can be one of the following three options: \n",
    "\n",
    "* *uniform* - this is a random initialization using points selected from our data set \n",
    "* *perturbation* - this uses the mean value of the different fields to perturb the centroids\n",
    "* *trained* - this uses the class labels as groups by which centroids are constructed using averaging\n",
    "\n",
    "The driver import five other sub-MRJob files to accomplish the different portions of the procedure, as we'll see below. The driver communicates with the other components in two main ways- pass-through arguments and flat files. The pass-through arguments are used to initialize the different portions of the algorithm, as well as specifying lookup files when necessary. Flat files are used to communicate updated cluster coordinates, and cluster labels for each customer. \n",
    "\n",
    "Finally, the progress and results of the algorithm are presented to the terminal in readable format as the algorithm runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile kmeans.py\n",
    "from __future__ import division\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "from init_centroids import initializeCentroids\n",
    "from assign_clusters import assignClusters\n",
    "from update_centroids import updateCentroids\n",
    "from get_error import getError\n",
    "from diagnostics import diagnostics\n",
    "\n",
    "import cPickle as pickle\n",
    "from collections import defaultdict\n",
    "import sys \n",
    "\n",
    "# Storage files \n",
    "trackerFile = './.tracker'\n",
    "centroidFile = './.clusters'\n",
    "scoreFile = './.scores'\n",
    "dataFile = 'topUsers_Apr-Jul_2014_1000-words.txt'\n",
    "\n",
    "\n",
    "def getName(obj, namespace):\n",
    "\treturn [name for name in namespace if namespace[name] is obj]\n",
    "\n",
    "\n",
    "def extractValues(job, runner):\n",
    "\toutput = defaultdict(int)\n",
    "\tfor line in runner.stream_output(): \n",
    "\t\tkey, value = job.parse_output_line(line)\n",
    "\t\toutput[key] = value\n",
    "\n",
    "\treturn output \n",
    "\n",
    "\n",
    "def dumpToFile(variable, filename):\n",
    "\twith open(filename, 'w') as f: \n",
    "\t\tpickle.dump(variable, f)\n",
    "\n",
    "\n",
    "def dumpToTracker(variable, filename):\n",
    "\twith open(filename, 'a') as f: \n",
    "\t\tf.write('dumping...' + '\\n')\n",
    "\t\tf.write(str(variable) + '\\n')\n",
    "\t\tf.write('dump complete.' + '\\n')\n",
    "\n",
    "\n",
    "def runJob(method, args, dFile=centroidFile):\n",
    "\tjob = method(args=args)\n",
    "\n",
    "\tmethodName = getName(method, globals())[0]\n",
    "\tprint '\\n\\t' + 'Running ' + methodName + '...'\n",
    "\n",
    "\twith job.make_runner() as runner: \n",
    "\n",
    "\t\t# Surpress console \n",
    "\t\trunner.run()\n",
    "\n",
    "\t\tresult = extractValues(job, runner)\n",
    "\t\tdumpToTracker(result, trackerFile)\n",
    "\n",
    "\t\tprint '\\t' + 'Complete: ' + methodName\n",
    "\n",
    "\t\tif dFile: \t\t\t\n",
    "\t\t\tdumpToFile(result, dFile)\t\n",
    "\n",
    "\t\telse:\n",
    "\t\t\treturn result\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\targs = sys.argv[1:]\n",
    "\n",
    "\t# Clear files \n",
    "\topen(trackerFile, 'w').close()\n",
    "\topen(centroidFile, 'w').close()\n",
    "\topen(scoreFile, 'w').close()\n",
    "\n",
    "\t# Step 1: Create initial clusters \n",
    "\tinit = '--' + str(args[0])\n",
    "\tnumClusters = '--k=' + str(args[1])\n",
    "\n",
    "\t# runJob(initializeCentroids, args=[dataFile, '--perturbation', '--k=4'])\n",
    "\trunJob(initializeCentroids, args=[dataFile, init, numClusters])\n",
    "\n",
    "\t# Loop initializations \n",
    "\tpriorError = 1\n",
    "\tthreshold = 0.0001\n",
    "\tmaxIter = 10\n",
    "\n",
    "\t# Loop\n",
    "\tfor i in range(maxIter):\n",
    "\n",
    "\t\tprint '\\n' + 'Iteration ' + str(i) + '.'\n",
    "\n",
    "\t\t# Score based on clusters \n",
    "\t\tcentroidArg = '--centroids='+centroidFile\n",
    "\t\trunJob(assignClusters, args=[dataFile, centroidArg], dFile=scoreFile)\n",
    "\n",
    "\t\t# Update clusters \n",
    "\t\tscoreArg = '--scores='+scoreFile\n",
    "\t\trunJob(updateCentroids, args=[dataFile, centroidArg, scoreArg])\n",
    "\n",
    "\t\t# Get error \n",
    "\t\terror = runJob(getError, args=[dataFile, centroidArg, scoreArg], dFile=None)\n",
    "\t\tcurrentError = error.values()[0]\n",
    "\n",
    "\t\t# Check threshold\n",
    "\t\tif priorError - currentError <= threshold: \n",
    "\n",
    "\t\t\t# Get diagnostics \n",
    "\t\t\tdiag = runJob(diagnostics, args=[dataFile, scoreArg], dFile=None)\n",
    "\t\t\tprint '\\n' + 'Purity characteristics: ' + '\\n'\n",
    "\t\t\t\n",
    "\t\t\t# Get cluster info \n",
    "\t\t\tfor cluster, clusterDiag in diag.iteritems(): \n",
    "\t\t\t\tprint '\\t' + 'Cluster ' + str(cluster) + ':'\n",
    "\n",
    "\t\t\t\tfor label, portion in clusterDiag.iteritems(): \n",
    "\t\t\t\t\tprint '\\t\\t' + 'Label: ' + str(label) + ', ' + 'Portion: ' + str(portion)\n",
    "\n",
    "\t\t\t\tprint ''\n",
    "\n",
    "\t\t\tprint '\\n' + 'RMSE: ' + str(currentError) + '\\n'\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\telse: \n",
    "\t\t\tpriorError = currentError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initializing the Centroids\n",
    "\n",
    "The centroids initialization is handled by `init_centroids.py`. This is the MRJob portion that handles different initialization arguments through an `add_passthrough_option` which takes input from the driver. To handle the different initializations, there are six separate map and reduce tasks to handle the three cases. \n",
    "\n",
    "Additionally, there is basic error-handling in case the appropriate options are not provided. Ultimately results of this job are yielded to a `extractValue()` parser in the driver to appropriately collate and pickle the results. The pickling allows one to systematically transfer Python data objects between the MRJob's in an asynchronous fashion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile init_centroids.py\n",
    "from __future__ import division\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import random\n",
    "import cPickle as pickle \n",
    "\n",
    "class initializeCentroids(MRJob):\n",
    "\n",
    "\tdef __init__(self, args):\n",
    "\t\tMRJob.__init__(self, args)\n",
    "\n",
    "\tdef configure_options(self):\n",
    "\t\tsuper(initializeCentroids, self).configure_options()\n",
    "\n",
    "\t\tself.add_passthrough_option(\n",
    "\t\t\t'--k', type='int', default=4, help='k: number of clusters')\n",
    "\n",
    "\t\tself.add_passthrough_option(\n",
    "\t\t\t'--uniform', action='store_true', default=False, help='uniform: even cluster initialization')\n",
    "\n",
    "\t\tself.add_passthrough_option(\n",
    "\t\t\t'--perturbation', action='store_true', default=False, help='perturbation: randomized cluster initialization')\n",
    "\n",
    "\t\tself.add_passthrough_option(\n",
    "\t\t\t'--trained', action='store_true', default=False, help='trained: class-based cluster initialization')\n",
    "\n",
    "\n",
    "\tdef uniform_init_centroids_map(self, _, line):\n",
    "\n",
    "\t\t# Parse data\n",
    "\t\tline = [int(x) for x in line.split(',')]\n",
    "\t\ttotal = line[2]\n",
    "\t\tbody = [x / total for x in line[3:]]\n",
    "\n",
    "\t\tk = self.options.k\n",
    "\t\tassignment = np.random.randint(k)\n",
    "\t\t\n",
    "\t\tyield assignment, body\n",
    "\n",
    "\tdef uniform_get_centroids(self, assignment, arrays):\n",
    "\t\t\n",
    "\t\t# Parse iteratble \n",
    "\t\tarrays = list(arrays)\n",
    "\n",
    "\t\t# Get random element \n",
    "\t\trandom.shuffle(arrays)\n",
    "\t\tnewCluster = arrays[0]\n",
    "\n",
    "\t\t# Cluster labels \n",
    "\t\ts = string.ascii_uppercase\n",
    "\n",
    "\t\tyield s[assignment], newCluster\n",
    "\n",
    "\n",
    "\n",
    "\tdef perturbation_init_centroids_map(self, _, line):\n",
    "\n",
    "\t\t# Parse data\n",
    "\t\tline = [int(x) for x in line.split(',')]\n",
    "\t\ttotal = line[2]\n",
    "\t\tbody = [x / total for x in line[3:]]\n",
    "\t\t\n",
    "\t\tyield None, body\n",
    "\n",
    "\tdef perturbation_get_centroids(self, _, totals):\n",
    "\n",
    "\t\t# Find the mean \n",
    "\t\tk = self.options.k\n",
    "\t\ts = string.ascii_uppercase\n",
    "\t\tclusterCenter = [np.mean(x) for x in zip(*totals)]\n",
    "\n",
    "\t\t# Emit\n",
    "\t\tfor i in range(k):\n",
    "\t\t\tcluster = [x + np.random.sample() for x in clusterCenter]\n",
    "\t\t\tyield s[i], cluster\n",
    "\n",
    "\n",
    "\n",
    "\tdef trained_init_centroids_map(self, _, line):\n",
    "\n",
    "\t\t# Parse data\n",
    "\t\tline = [int(x) for x in line.split(',')]\n",
    "\t\ttotal = line[2]\n",
    "\t\tlabel = line[1]\n",
    "\t\tbody = [x / total for x in line[3:]]\n",
    "\t\t\n",
    "\t\tyield label, body\n",
    "\n",
    "\tdef trained_get_centroids(self, label, arrays):\n",
    "\t\t\n",
    "\t\t# Compute new cluster \n",
    "\t\tarrays = list(arrays)\n",
    "\t\tnewCluster = [np.mean(x) for x in zip(*arrays)]\n",
    "\n",
    "\t\t# Cluster labels \n",
    "\t\ts = string.ascii_uppercase\n",
    "\n",
    "\t\tyield s[label], newCluster\n",
    "\n",
    "\n",
    "\n",
    "\tdef steps(self):\n",
    "\n",
    "\t\tif self.options.uniform:\n",
    "\t\t\treturn [MRStep(mapper=self.uniform_init_centroids_map, \n",
    "\t\t\t\t\t\t\treducer=self.uniform_get_centroids)]\n",
    "\n",
    "\t\telif self.options.perturbation: \n",
    "\t\t\treturn [MRStep(mapper=self.perturbation_init_centroids_map, \n",
    "\t\t\t\t\t\t\treducer=self.perturbation_get_centroids)]\n",
    "\n",
    "\t\telif self.options.trained: \n",
    "\t\t\treturn [MRStep(mapper=self.trained_init_centroids_map, \n",
    "\t\t\t\t\t\t\treducer=self.trained_get_centroids)]\n",
    "\n",
    "\t\telse: \n",
    "\n",
    "\t\t\t# No initialization\n",
    "\t\t\traise ValueError('ERROR: Please enter initialization type. See help for more details.')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tinitializeCentroids.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assigning Cluster ID's\n",
    "\n",
    "Here, we describe the MRJob used to assign cluster ID's to the various points in our training data. Here, the pickled cluster characteristics dumped to `.clusters` is read in the reducer. The distance to each cluster is computed, and the `argmin` is returned for each customer ID. \n",
    "\n",
    "The results of the assignment are pickled by the driver in `.scores`, which are used in the following MRJob segment used to update the centroid coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile assign_clusters.py\n",
    "from __future__ import division\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "class assignClusters(MRJob):\n",
    "\n",
    "\tdef __init__(self, args):\n",
    "\t\tMRJob.__init__(self, args)\n",
    "\n",
    "\tdef configure_options(self):\n",
    "\t\tsuper(assignClusters, self).configure_options()\n",
    "\t\tself.add_file_option('--centroids', \n",
    "\t\t\thelp='pointer to centroids file. See main runner for details.')\n",
    "\n",
    "\tdef mapper_diff_comp(self, _, line):\n",
    "\n",
    "\t\t# Parse data\n",
    "\t\tline = [int(x) for x in line.split(',')]\n",
    "\t\tcustID = line[0]\n",
    "\t\ttotal = line[2]\n",
    "\t\tbody = [x / total for x in line[3:]]\n",
    "\n",
    "\t\t# Read clusters \n",
    "\t\twith open(self.options.centroids, 'r') as f: \n",
    "\t\t\tclusters = pickle.load(f)\n",
    "\n",
    "\t\t# Compute distances \n",
    "\t\tfor clusterID, cluster in clusters.iteritems(): \n",
    "\t\t\t\n",
    "\t\t\tcluster = np.array(cluster)\n",
    "\t\t\tbody = np.array(body)\n",
    "\t\t\tdist = np.linalg.norm(body-cluster)\n",
    "\n",
    "\t\t\tyield custID, [clusterID, dist]\n",
    "\n",
    "\n",
    "\tdef reducer_find_min_cluster(self, custID, distArray):\n",
    "\n",
    "\t\t# Find closest cluster\n",
    "\t\tdistArray = np.array(list(distArray))\n",
    "\t\tclusterIndex = np.argmin(distArray[:, 1])\n",
    "\t\tclusterID = distArray[clusterIndex, 0]\n",
    "\n",
    "\t\tyield custID, clusterID\n",
    "\n",
    "\tdef steps(self):\n",
    "\t\treturn [MRStep(mapper=self.mapper_diff_comp, \n",
    "\t\t\t\t\t\treducer=self.reducer_find_min_cluster)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tassignClusters.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Update Centroids \n",
    "\n",
    "Once the scores have been written, the cluster centers must be updated given the new class labels. Occasionally, some clusters may be eliminated or be reduced to singletons depending on their initialization. The centroids handling is flexible enough to allow this case when scoring and updating cluster centers. \n",
    "\n",
    "The scores are read in the mapper, where the cluster ID is extracted and attached to each customer. Then, the reducer reads the centroid coordinates and computes a mean over the dimensions of the matching customer cluster labels. The shuffling between the mapper and reducer step guarantees that each cluster label is grouped with the respective arrays necessary for updating the centroids. \n",
    "\n",
    "The update criteria is set to `i=10` absolute iterations, or a minimum `RMSE` change of `0.0001` between iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile update_centroids.py\n",
    "from __future__ import division\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class updateCentroids(MRJob):\n",
    "\n",
    "\tdef __init__(self, args):\n",
    "\t\tMRJob.__init__(self, args)\n",
    "\n",
    "\tdef configure_options(self):\n",
    "\t\tsuper(updateCentroids, self).configure_options()\n",
    "\n",
    "\t\tself.add_file_option('--scores', \n",
    "\t\t\thelp='pointer to scores file. See main runner for details.')\n",
    "\n",
    "\t\tself.add_file_option('--centroids', \n",
    "\t\t\thelp='pointer to centroids file. See main runner for details.')\n",
    "\n",
    "\tdef mapper_telegraph_id(self, _, line):\n",
    "\n",
    "\t\t# Parse data\n",
    "\t\tline = [int(x) for x in line.split(',')]\n",
    "\t\tcustID = line[0]\n",
    "\t\ttotal = line[2]\n",
    "\t\tbody = [x / total for x in line[3:]]\n",
    "\n",
    "\t\t# Load scores \n",
    "\t\twith open(self.options.scores, 'r') as f: \n",
    "\t\t\tscores = pickle.load(f)\n",
    "\n",
    "\t\t# Get individual\n",
    "\t\tlabel = scores[custID]\n",
    "\t\tyield label, body\n",
    "\n",
    "\tdef reducer_emit_clusters(self, label, arrays):\n",
    "\t\t\n",
    "\t\t# Compute new cluster \n",
    "\t\tarrays = list(arrays)\n",
    "\t\tnewCluster = [np.mean(x) for x in zip(*arrays)]\n",
    "\n",
    "\t\tyield label, newCluster\n",
    "\n",
    "\n",
    "\tdef steps(self):\n",
    "\t\treturn [MRStep(mapper=self.mapper_telegraph_id,\n",
    "\t\t\t\t\t\treducer=self.reducer_emit_clusters)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tupdateCentroids.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting Error \n",
    "\n",
    "After the centroids have been updated, we compute the `RMSE` for the new cluster centers. This is job is important as it allows the driver to terminate the algorithm if the `RMSE` fails to decrease by a minimum threshold after each iteration. \n",
    "\n",
    "Anecdotally, when running this K-Means implementation over the different option, the `RMSE` threshold will be a much stronger deciding factor as to the runtime of the algorithm as compared with the capping the absolute iteration count. This fits with the intuition one expects when updating centroid centers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile get_error.py\n",
    "from __future__ import division\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "class getError(MRJob):\n",
    "\n",
    "\tdef __init__(self, args):\n",
    "\t\tMRJob.__init__(self, args)\n",
    "\n",
    "\tdef configure_options(self):\n",
    "\t\tsuper(getError, self).configure_options()\n",
    "\n",
    "\t\tself.add_file_option('--centroids', \n",
    "\t\t\thelp='pointer to centroids file. See main runner for details.')\n",
    "\n",
    "\t\tself.add_file_option('--scores', \n",
    "\t\t\thelp='pointer to scores file. See main runner for details.')\n",
    "\n",
    "\tdef mapper_telegraph_id(self, _, line):\n",
    "\n",
    "\t\t# Parse data\n",
    "\t\tline = [int(x) for x in line.split(',')]\n",
    "\t\tcustID = line[0]\n",
    "\t\ttotal = line[2]\n",
    "\t\tbody = [x / total for x in line[3:]]\n",
    "\n",
    "\t\t# Load scores \n",
    "\t\twith open(self.options.scores, 'r') as f: \n",
    "\t\t\tscores = pickle.load(f)\n",
    "\n",
    "\t\t# Get individual\n",
    "\t\tlabel = scores[custID]\n",
    "\t\tyield label, body\n",
    "\n",
    "\tdef reducer_compute_dist(self, label, vectors):\n",
    "\t\tvectors = np.array(list(vectors))\n",
    "\n",
    "\t\t# Load centroids\n",
    "\t\twith open(self.options.centroids, 'r') as f: \n",
    "\t\t\tcentroids = pickle.load(f)\n",
    "\n",
    "\t\tcluster = centroids[label]\n",
    "\t\tdist = sum([np.linalg.norm(body-cluster) for body in vectors])\n",
    "\n",
    "\t\tyield None, dist\n",
    "\n",
    "\tdef reducer_rmse(self, _, dists):\n",
    "\t\t\n",
    "\t\t# Load scores \n",
    "\t\twith open(self.options.scores, 'r') as f: \n",
    "\t\t\tscores = pickle.load(f)\t\n",
    "\n",
    "\t\tN = len(scores)\n",
    "\n",
    "\t\tRMSE = np.sqrt(sum(dists) / N)\n",
    "\n",
    "\t\tyield None, RMSE\t\n",
    "\n",
    "\tdef steps(self):\n",
    "\t\treturn [MRStep(mapper=self.mapper_telegraph_id,\n",
    "\t\t\t\t\t\treducer=self.reducer_compute_dist),\n",
    "\n",
    "\t\t\t\tMRStep(reducer=self.reducer_rmse)]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tgetError.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Diagnostics- Purity Values \n",
    "\n",
    "After the iterations have been terminated, a final MRJob is called to compute the purity of the final round of clusters. This loops through the initial training data as well as latest customer scores to determine the proprtion of each customer label within each cluster. \n",
    "\n",
    "To prevent confusion, customer labels are assigned a number (0-3) and cluster labels are assigned an uppercase ASCII character (A,B,C...). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile diagnostics.py\n",
    "from __future__ import division\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "class diagnostics(MRJob):\n",
    "\n",
    "\tdef __init__(self, args):\n",
    "\t\tMRJob.__init__(self, args)\n",
    "\n",
    "\tdef configure_options(self):\n",
    "\t\tsuper(diagnostics, self).configure_options()\n",
    "\n",
    "\t\tself.add_file_option('--scores', \n",
    "\t\t\thelp='pointer to scores file. See main runner for details.')\n",
    "\n",
    "\tdef mapper_load_scores(self, _, line):\n",
    "\n",
    "\t\t# Parse data\n",
    "\t\tline = [int(x) for x in line.split(',')]\n",
    "\t\tcustID, label = line[:2]\n",
    "\n",
    "\t\t# Load scores \n",
    "\t\twith open(self.options.scores, 'r') as f: \n",
    "\t\t\tscores = pickle.load(f)\n",
    "\n",
    "\t\t# Get individual\n",
    "\t\tclusterID = scores[custID]\n",
    "\t\tyield clusterID, label\n",
    "\n",
    "\tdef reducer_compute_diagnostics(self, clusterID, labels):\n",
    "\t\t\n",
    "\t\t# Get counts \n",
    "\t\tlabels = list(labels)\n",
    "\t\tuniqueLabels = set(labels)\n",
    "\t\tlabelTally = defaultdict(int)\n",
    "\n",
    "\t\t# Assign\n",
    "\t\tfor label in uniqueLabels: \n",
    "\t\t\tlabelTally[label] = labels.count(label)\n",
    "\n",
    "\t\ttotal = sum(labelTally.values())\n",
    "\n",
    "\t\t# Get portions\n",
    "\t\tfor key, value in labelTally.iteritems(): \n",
    "\t\t\tlabelTally[key] = value / total \n",
    "\n",
    "\t\t# Emit\n",
    "\t\tyield clusterID, labelTally\n",
    "\n",
    "\t\n",
    "\tdef steps(self):\n",
    "\t\treturn [MRStep(mapper=self.mapper_load_scores, \n",
    "\t\t\t\t\t\treducer=self.reducer_compute_diagnostics)]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tdiagnostics.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part (A)\n",
    "\n",
    "Below we implement the solution for part (A). Here, we use a uniform initialization with 4 clusters. Notice the progress of the algorithm and final results are printed concisely as the algorithm progresses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python kmeans.py 'uniform' 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running initializeCentroids...\n",
    "No handlers could be found for logger \"mrjob.runner\"\n",
    "\tComplete: initializeCentroids\n",
    "\n",
    "Iteration 0.\n",
    "\n",
    "\tRunning assignClusters...\n",
    "\tComplete: assignClusters\n",
    "\n",
    "\tRunning updateCentroids...\n",
    "\tComplete: updateCentroids\n",
    "\n",
    "\tRunning getError...\n",
    "\tComplete: getError\n",
    "\n",
    "Iteration 1.\n",
    "\n",
    "\tRunning assignClusters...\n",
    "\tComplete: assignClusters\n",
    "\n",
    "\tRunning updateCentroids...\n",
    "\tComplete: updateCentroids\n",
    "\n",
    "\tRunning getError...\n",
    "\tComplete: getError\n",
    "\n",
    "Iteration 2.\n",
    "\n",
    "\tRunning assignClusters...\n",
    "\tComplete: assignClusters\n",
    "\n",
    "\tRunning updateCentroids...\n",
    "\tComplete: updateCentroids\n",
    "\n",
    "\tRunning getError...\n",
    "\tComplete: getError\n",
    "\n",
    "Iteration 3.\n",
    "\n",
    "\tRunning assignClusters...\n",
    "\tComplete: assignClusters\n",
    "\n",
    "\tRunning updateCentroids...\n",
    "\tComplete: updateCentroids\n",
    "\n",
    "\tRunning getError...\n",
    "\tComplete: getError\n",
    "\n",
    "\tRunning diagnostics...\n",
    "\tComplete: diagnostics\n",
    "\n",
    "Purity characteristics: \n",
    "\n",
    "\tCluster A:\n",
    "\t\tLabel: 1, Portion: 0.00149253731343\n",
    "\t\tLabel: 0, Portion: 0.879104477612\n",
    "\t\tLabel: 3, Portion: 0.119402985075\n",
    "\n",
    "\tCluster C:\n",
    "\t\tLabel: 1, Portion: 0.0111731843575\n",
    "\t\tLabel: 0, Portion: 0.815642458101\n",
    "\t\tLabel: 3, Portion: 0.106145251397\n",
    "\t\tLabel: 2, Portion: 0.0670391061453\n",
    "\n",
    "\tCluster B:\n",
    "\t\tLabel: 1, Portion: 0.676923076923\n",
    "\t\tLabel: 0, Portion: 0.00769230769231\n",
    "\t\tLabel: 3, Portion: 0.0307692307692\n",
    "\t\tLabel: 2, Portion: 0.284615384615\n",
    "\n",
    "\tCluster D:\n",
    "\t\tLabel: 0, Portion: 0.761904761905\n",
    "\t\tLabel: 2, Portion: 0.238095238095\n",
    "\n",
    "\n",
    "RMSE: 0.258822076078\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part (B)\n",
    "\n",
    "For this portion, we implement a perturbation intitialization that displaces each cluster from the \"uniform\" data centroid by random amounts. We specify `k=2` clusters for this question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python kmeans.py 'perturbation' 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running initializeCentroids...\n",
    "No handlers could be found for logger \"mrjob.runner\"\n",
    "\tComplete: initializeCentroids\n",
    "\n",
    "Iteration 0.\n",
    "\n",
    "\tRunning assignClusters...\n",
    "\tComplete: assignClusters\n",
    "\n",
    "\tRunning updateCentroids...\n",
    "\tComplete: updateCentroids\n",
    "\n",
    "\tRunning getError...\n",
    "\tComplete: getError\n",
    "\n",
    "Iteration 1.\n",
    "\n",
    "\tRunning assignClusters...\n",
    "\tComplete: assignClusters\n",
    "\n",
    "\tRunning updateCentroids...\n",
    "\tComplete: updateCentroids\n",
    "\n",
    "\tRunning getError...\n",
    "\tComplete: getError\n",
    "\n",
    "\tRunning diagnostics...\n",
    "\tComplete: diagnostics\n",
    "\n",
    "Purity characteristics: \n",
    "\n",
    "\tCluster B:\n",
    "\t\tLabel: 1, Portion: 0.091\n",
    "\t\tLabel: 0, Portion: 0.752\n",
    "\t\tLabel: 3, Portion: 0.103\n",
    "\t\tLabel: 2, Portion: 0.054\n",
    "\n",
    "\n",
    "RMSE: 0.282547403061"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part (C)\n",
    "\n",
    "We do a similar initialization here, except we ask for `k=4` clusters using the same perturbation method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python kmeans.py 'perturbation' 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running initializeCentroids...\n",
    "No handlers could be found for logger \"mrjob.runner\"\n",
    "\tComplete: initializeCentroids\n",
    "\n",
    "Iteration 0.\n",
    "\n",
    "\tRunning assignClusters...\n",
    "\tComplete: assignClusters\n",
    "\n",
    "\tRunning updateCentroids...\n",
    "\tComplete: updateCentroids\n",
    "\n",
    "\tRunning getError...\n",
    "\tComplete: getError\n",
    "\n",
    "Iteration 1.\n",
    "\n",
    "\tRunning assignClusters...\n",
    "\tComplete: assignClusters\n",
    "\n",
    "\tRunning updateCentroids...\n",
    "\tComplete: updateCentroids\n",
    "\n",
    "\tRunning getError...\n",
    "\tComplete: getError\n",
    "\n",
    "Iteration 2.\n",
    "\n",
    "\tRunning assignClusters...\n",
    "\tComplete: assignClusters\n",
    "\n",
    "\tRunning updateCentroids...\n",
    "\tComplete: updateCentroids\n",
    "\n",
    "\tRunning getError...\n",
    "\tComplete: getError\n",
    "\n",
    "Iteration 3.\n",
    "\n",
    "\tRunning assignClusters...\n",
    "\tComplete: assignClusters\n",
    "\n",
    "\tRunning updateCentroids...\n",
    "\tComplete: updateCentroids\n",
    "\n",
    "\tRunning getError...\n",
    "\tComplete: getError\n",
    "\n",
    "\tRunning diagnostics...\n",
    "\tComplete: diagnostics\n",
    "\n",
    "Purity characteristics: \n",
    "\n",
    "\tCluster C:\n",
    "\t\tLabel: 1, Portion: 0.661654135338\n",
    "\t\tLabel: 0, Portion: 0.00751879699248\n",
    "\t\tLabel: 3, Portion: 0.0300751879699\n",
    "\t\tLabel: 2, Portion: 0.300751879699\n",
    "\n",
    "\tCluster D:\n",
    "\t\tLabel: 1, Portion: 0.00346020761246\n",
    "\t\tLabel: 0, Portion: 0.866205305652\n",
    "\t\tLabel: 3, Portion: 0.114186851211\n",
    "\t\tLabel: 2, Portion: 0.0161476355248\n",
    "\n",
    "\n",
    "RMSE: 0.263805098751"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part (D)\n",
    "\n",
    "Finally, we use the \"trained\" initialization here to initialize centroids over the class labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python kmeans.py 'trained' 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running initializeCentroids...\n",
    "No handlers could be found for logger \"mrjob.runner\"\n",
    "\tComplete: initializeCentroids\n",
    "\n",
    "Iteration 0.\n",
    "\n",
    "\tRunning assignClusters...\n",
    "\tComplete: assignClusters\n",
    "\n",
    "\tRunning updateCentroids...\n",
    "\tComplete: updateCentroids\n",
    "\n",
    "\tRunning getError...\n",
    "\tComplete: getError\n",
    "\n",
    "Iteration 1.\n",
    "\n",
    "\tRunning assignClusters...\n",
    "\tComplete: assignClusters\n",
    "\n",
    "\tRunning updateCentroids...\n",
    "\tComplete: updateCentroids\n",
    "\n",
    "\tRunning getError...\n",
    "\tComplete: getError\n",
    "\n",
    "Iteration 2.\n",
    "\n",
    "\tRunning assignClusters...\n",
    "\tComplete: assignClusters\n",
    "\n",
    "\tRunning updateCentroids...\n",
    "\tComplete: updateCentroids\n",
    "\n",
    "\tRunning getError...\n",
    "\tComplete: getError\n",
    "\n",
    "Iteration 3.\n",
    "\n",
    "\tRunning assignClusters...\n",
    "\tComplete: assignClusters\n",
    "\n",
    "\tRunning updateCentroids...\n",
    "\tComplete: updateCentroids\n",
    "\n",
    "\tRunning getError...\n",
    "\tComplete: getError\n",
    "\n",
    "\tRunning diagnostics...\n",
    "\tComplete: diagnostics\n",
    "\n",
    "Purity characteristics: \n",
    "\n",
    "\tCluster A:\n",
    "\t\tLabel: 1, Portion: 0.00373599003736\n",
    "\t\tLabel: 0, Portion: 0.932752179328\n",
    "\t\tLabel: 3, Portion: 0.0473225404732\n",
    "\t\tLabel: 2, Portion: 0.0161892901619\n",
    "\n",
    "\tCluster C:\n",
    "\t\tLabel: 1, Portion: 0.44578313253\n",
    "\t\tLabel: 0, Portion: 0.0120481927711\n",
    "\t\tLabel: 3, Portion: 0.0481927710843\n",
    "\t\tLabel: 2, Portion: 0.493975903614\n",
    "\n",
    "\tCluster B:\n",
    "\t\tLabel: 1, Portion: 1.0\n",
    "\n",
    "\tCluster D:\n",
    "\t\tLabel: 0, Portion: 0.031746031746\n",
    "\t\tLabel: 3, Portion: 0.968253968254\n",
    "\n",
    "\n",
    "RMSE: 0.255077922595"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discussion \n",
    "\n",
    "The results of the four different initializations fit our intuition. The trained initialization did the best, with the lowest `RMSE` of the four. However, it contains a singleton cluster and is symptomatic of either poor initialization or skewed data. Since the lexigraphical data follows Zipf's law, a singleton, or especially pure clusters are unsurprising. \n",
    "\n",
    "The two perturbation initializations perform the worst, and this is perhaps unsurprising. Given that the data is normalized prior to perturbation, adding random numbers with 0-mean is may have a dramatic impact on their sucessful path through 'updating', as the data is highly skewed. In addition, both perturbation initializations saw their clusters reduced to half of their original count, which suggests that perhaps more clustered are needed for an accurate classification to be produced. \n",
    "\n",
    "Finally, the uniform initialization did only slightly worse than the trained initialization. This is also unsurprising, as we are picking random customers to represent our cluster centers, then updating from there. For this reason, none of the clusters will drop through the update process, as the original centroid point is going to be clusters going forward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
