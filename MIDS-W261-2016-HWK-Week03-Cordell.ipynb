{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS W261 Spring 2016 Homework Week 3\n",
    "\n",
    "Ron Cordell<br />\n",
    "W261-4<br />\n",
    "ron.cordell@ischool.berkeley.edu<br />\n",
    "January 26, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.0.\n",
    "\n",
    "**What is a merge sort? Where is it used in Hadoop?**\n",
    "\n",
    "A merge sort merges sorted lists into a single sorted list. The merge sort works by establishing a pointer to the beginning of each sorted list as well as a new \"merge\" list. The objects or keys in each list referenced by the pointers are compared and the chosen one moved or copied to the location indicated by the pointer of the merge list. The merge list pointer is advanced as is the pointer for the list from which the object was moved. This is repeated until all objects in all list have been moved or copied to the new merge list. The comparator function \"chooses\" the object from the source lists based on the rules coded into the comparator function such as the largest, the smallest, etc.\n",
    "\n",
    "Hadoop uses a merge sort during the shuffle process when it takes output from multiple sources such as mappers or combiners and merges them into the sorted streams used by downstream processes.\n",
    "\n",
    "**How is  a combiner function in the context of Hadoop? \n",
    "Give an example where it can be used and justify why it should be used in the context of this problem.**\n",
    "\n",
    "A combiner function is a function that can be used by Hadoop anywhere between the mappers and producers to help eliminate network and data traffic, especially as part of the shuffle. A an example combiner function typically provides a partial aggregation point for data emitted from the mapper to reduce hotspots in the shuffle.\n",
    "\n",
    "An example where a combiner can be used to good effect is in a word count scenario, where the mapper emits the word as key and a value of 1. A combiner can perform aggregations on the key-value pairs by combining those with the same key and adding their values. This greatly reduces the granularity of the data required to shuffle and sort and provide to the reducers and helps reduce the amount of network and disk traffic of the shuffle, decreasing the overall run time.\n",
    "\n",
    "**What is the Hadoop shuffle?**\n",
    "\n",
    "![MapReduce Workflow](MapReduceWorkflow.png)\n",
    "\n",
    "The Hadoop shuffle takes the mapper outpus, merges them and sorts them, computes a hash to partition them, and routes each partition to a reducer. It corresponds the the shuffle and sort section of the above diagram, between the mappers and reducers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.1 Use Counters to do EDA (exploratory data analysis and to monitor progress)**\n",
    "\n",
    "**The consumer complaints dataset consists of diverse consumer complaints, which have been reported across the United States regarding various types of loans. The dataset consists of records of the form:**\n",
    "\n",
    "    Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    "**Here’s is the first few lines of the  of the Consumer Complaints  Dataset:**\n",
    "\n",
    "    Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "    1114245,Debt collection,Medical,Disclosure verification of debt,Not given enough info to verify debt,FL,32219,Web,11/13/2014,11/13/2014,\"Choice Recovery, Inc.\",Closed with explanation,Yes,\n",
    "    1114488,Debt collection,Medical,Disclosure verification of debt,Right to dispute notice not received,TX,75006,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "    1114255,Bank account or service,Checking account,Deposits and withdrawals,,NY,11102,Web,11/13/2014,11/13/2014,\"FNIS (Fidelity National Information Services, Inc.)\",In progress,Yes,\n",
    "    1115106,Debt collection,\"Other (phone, health club, etc.)\",Communication tactics,Frequent or repeated calls,GA,31721,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "\n",
    "**Now, let’s use Hadoop Counters to identify the number of complaints pertaining to debt collection, mortgage and other categories (all other categories get lumped into this one) in the consumer complaints dataset. Basically produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "#\n",
    "# W261 HW 3.1 MapReduce and Counters\n",
    "#\n",
    "# Read from a CSV file of consumer complaints with fields as follows:\n",
    "#\n",
    "# Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,\n",
    "# Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "#\n",
    "# Use counters to count the number of complaints for Product Id's of debt collection, mortgage,\n",
    "# and everything else (3 categories)\n",
    "#\n",
    "# Remember that in Hadoop streaming, to update a counter is to write to STDERR in the format\n",
    "# reporter:counter:<group>,<counter>,<amount>\n",
    "\n",
    "import sys\n",
    "\n",
    "# Read data from STDIN and use counters to count the data\n",
    "def main(separator=','):\n",
    "    # input comes from STDIN (standard input) as the fields from the CSV\n",
    "    for line in (sys.stdin):\n",
    "        fields = line.split(separator)\n",
    "        try:\n",
    "            # check to see if this is a header by trying to convert the first field to an integer\n",
    "            id = int(fields[0])\n",
    "            # we have a real record, so do some mapping\n",
    "            counter_name = None\n",
    "            if (fields[1].lower() == 'debt collection' or \\\n",
    "                fields[1].lower() == 'mortgage'):\n",
    "                counter_name = fields[1].strip().lower()\n",
    "            else:\n",
    "                counter_name = 'other'\n",
    "            # update the counter\n",
    "            sys.stderr.write(\"reporter:counter:Category Counters,{0},1\\n\".format(counter_name))\n",
    "        except:\n",
    "            # must be a header record so skip it\n",
    "            pass\n",
    "\n",
    "            \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the appropriate execution mode on the file so we can execute it\n",
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-rcordell-resourcemanager-Rons-iMac-Retina.local.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-rcordell-nodemanager-Rons-iMac-Retina.local.out\n",
      "16/01/29 23:50:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-rcordell-namenode-Rons-iMac-Retina.local.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-rcordell-datanode-Rons-iMac-Retina.local.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-rcordell-secondarynamenode-Rons-iMac-Retina.local.out\n",
      "16/01/29 23:50:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "starting historyserver, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/mapred-rcordell-historyserver-Rons-iMac-Retina.local.out\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/./mr-jobhistory-daemon.sh \\\n",
    "    --config /usr/local/Cellar/hadoop/2.7.1/libexec/etc/hadoop/ start historyserver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 09:43:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 09:43:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 09:43:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir -p /user/rcordell\n",
    "!hdfs dfs -put Consumer_Complaints.csv /user/rcordell\n",
    "!hdfs dfs -put short_complaints.csv /user/rcordell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 09:43:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `/user/rcordell/recordsOutput': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/recordsOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 09:44:16 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/30 09:44:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/hadoop-unjar6059806740624592914/] [] /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/streamjob8735250773221487987.jar tmpDir=null\n",
      "16/01/30 09:44:17 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/30 09:44:17 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/30 09:44:17 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/30 09:44:17 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/30 09:44:17 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/01/30 09:44:17 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454175435207_0002\n",
      "16/01/30 09:44:18 INFO impl.YarnClientImpl: Submitted application application_1454175435207_0002\n",
      "16/01/30 09:44:18 INFO mapreduce.Job: The url to track the job: http://Rons-iMac-Retina.local:8088/proxy/application_1454175435207_0002/\n",
      "16/01/30 09:44:18 INFO mapreduce.Job: Running job: job_1454175435207_0002\n",
      "16/01/30 09:44:23 INFO mapreduce.Job: Job job_1454175435207_0002 running in uber mode : false\n",
      "16/01/30 09:44:23 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/30 09:44:29 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/30 09:44:29 INFO mapreduce.Job: Job job_1454175435207_0002 completed successfully\n",
      "16/01/30 09:44:29 INFO mapreduce.Job: Counters: 33\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=236642\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910105\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5031\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=5031\n",
      "\t\tTotal vcore-seconds taken by all map tasks=5031\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=5151744\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=0\n",
      "\t\tInput split bytes=222\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=85\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=284688384\n",
      "\tCategory Counters\n",
      "\t\tdebt collection=44372\n",
      "\t\tmortgage=125752\n",
      "\t\tother=142788\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50909883\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n",
      "16/01/30 09:44:29 INFO streaming.StreamJob: Output directory: recordsOutput\n"
     ]
    }
   ],
   "source": [
    "!yarn jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "    -D mapred.reduce.tasks=0 \\\n",
    "    -file mapper.py \\\n",
    "    -mapper mapper.py \\\n",
    "    -input Consumer_Complaints.csv \\\n",
    "    -output recordsOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "localhost: nodemanager did not stop gracefully after 5 seconds: killing with kill -9\n",
      "no proxyserver to stop\n",
      "16/01/30 00:34:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/30 00:34:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "no historyserver to stop\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/./mr-jobhistory-daemon.sh \\\n",
    "    --config /usr/local/Cellar/hadoop/2.7.1/libexec/etc/hadoop/ stop historyserver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your  job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible. **\n",
    "\n",
    "![Job Tracker](jobtrackercounters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 3.2 Analyze the performance of your Mappers, Combiners and Reducers using Counters\n",
    "\n",
    "For this brief study the Input file will be one record (the next line only): \n",
    "foo foo quux labs foo bar quux\n",
    "\n",
    "\n",
    "Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively. Please explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper32.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper32.py\n",
    "#!/usr/bin/env python\n",
    "#\n",
    "# W261 HW 3.2 MapReduce and Counters for Code Analysis\n",
    "#\n",
    "# Simple word counter\n",
    "#\n",
    "# Use counters to count the number of times the mapper is called\n",
    "#\n",
    "# Remember that in Hadoop streaming, to update a counter is to write to STDERR in the format\n",
    "# reporter:counter:<group>,<counter>,<amount>\n",
    "\n",
    "import sys\n",
    "\n",
    "def read_input(file):\n",
    "    for line in file:\n",
    "        # split the line into words\n",
    "        yield line.split()\n",
    "        \n",
    "# Read data from STDIN and use counters to count the data\n",
    "def main(separator=','):    \n",
    "    data = read_input(sys.stdin)\n",
    "    for words in data:\n",
    "        # write the results to STDOUT\n",
    "        # tab-delimited; the trivial word count is 1\n",
    "        for word in words:\n",
    "            sys.stdout.write('{0}{1}{2}\\n'.format(word, separator, 1))         \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # increment counter for mapper call, write to STDERR\n",
    "    sys.stderr.write(\"reporter:counter:Code Call Counters,mapper,1\\n\")\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer32.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer32.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "#\n",
    "# Use counters to count the number of times the reducer is called\n",
    "#\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "def read(file, separator='\\t'):\n",
    "    for line in file:\n",
    "        yield line.rstrip().split(separator, 1)\n",
    "\n",
    "def main(separator='\\t'):    \n",
    "    # input comes from STDIN (standard input)\n",
    "    data = read(sys.stdin, separator=separator)\n",
    "    # groupby groups multiple word-count pairs by word,\n",
    "    # and creates an iterator that returns consecutive keys and their group:\n",
    "    #   current_word - string containing a word (the key)\n",
    "    #   group - iterator yielding all [\"&lt;current_word&gt;\", \"&lt;count&gt;\"] items\n",
    "    for current_word, group in groupby(data, itemgetter(0)):\n",
    "        try:\n",
    "            total_count = sum(int(count) for current_word, count in group)\n",
    "            sys.stdout.write(\"{0}{1}{2}\".format(current_word, separator, total_count))\n",
    "        except ValueError:\n",
    "            # count was not a number, so silently discard this item\n",
    "            pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # increment counter for reducer call, write to STDERR\n",
    "    sys.stderr.write(\"reporter:counter:Code Call Counters,reducer,1\\n\")\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 10:01:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"foo foo quux labs foo bar quux\" >foo.txt\n",
    "!hdfs dfs -put foo.txt /user/rcordell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 17:20:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 17:20:10 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/rcordell/recordsOutput\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/recordsOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 17:20:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/hadoop-unjar8517455371226977358/] [] /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/streamjob5581936008517874544.jar tmpDir=null\n",
      "16/01/30 17:20:15 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/30 17:20:15 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/30 17:20:16 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/30 17:20:16 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/30 17:20:16 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454175435207_0011\n",
      "16/01/30 17:20:16 INFO impl.YarnClientImpl: Submitted application application_1454175435207_0011\n",
      "16/01/30 17:20:16 INFO mapreduce.Job: The url to track the job: http://Rons-iMac-Retina.local:8088/proxy/application_1454175435207_0011/\n",
      "16/01/30 17:20:16 INFO mapreduce.Job: Running job: job_1454175435207_0011\n",
      "16/01/30 17:20:20 INFO mapreduce.Job: Job job_1454175435207_0011 running in uber mode : false\n",
      "16/01/30 17:20:20 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/30 17:20:25 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/30 17:20:29 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "16/01/30 17:20:30 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/01/30 17:20:31 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "16/01/30 17:20:32 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/30 17:20:32 INFO mapreduce.Job: Job job_1454175435207_0011 completed successfully\n",
      "16/01/30 17:20:32 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=90\n",
      "\t\tFILE: Number of bytes written=595152\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=126\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=1825\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=6465\n",
      "\t\tTotal time spent by all map tasks (ms)=1825\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6465\n",
      "\t\tTotal vcore-seconds taken by all map tasks=1825\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=6465\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1868800\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=6620160\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=7\n",
      "\t\tMap output bytes=52\n",
      "\t\tMap output materialized bytes=90\n",
      "\t\tInput split bytes=95\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=90\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=0\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=165\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=788004864\n",
      "\tCode Call Counters\n",
      "\t\tmapper=1\n",
      "\t\treducer=4\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=31\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n",
      "16/01/30 17:20:32 INFO streaming.StreamJob: Output directory: recordsOutput\n"
     ]
    }
   ],
   "source": [
    "!yarn jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "    -D mapreduce.job.reduces=4 \\\n",
    "    -D mapreduce.job.maps=1 \\\n",
    "    -files \"mapper32.py,reducer32.py\" \\\n",
    "    -mapper mapper32.py \\\n",
    "    -reducer reducer32.py \\\n",
    "    -input foo.txt \\\n",
    "    -output recordsOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper32b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper32b.py\n",
    "#!/usr/bin/env python\n",
    "#\n",
    "# W261 HW 3.2 MapReduce and Counters for Code Analysis\n",
    "# Read from a CSV file of consumer complaints with fields as follows:\n",
    "#\n",
    "# Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,\n",
    "# Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "#\n",
    "# Use counters to count the number of times the mapper is called\n",
    "#\n",
    "# Remember that in Hadoop streaming, to update a counter is to write to STDERR in the format\n",
    "# reporter:counter:<group>,<counter>,<amount>\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# Read data from STDIN and use counters to count the data\n",
    "def main(separator='\\t'):   \n",
    "    for line in sys.stdin:\n",
    "        fields = line.split(',')\n",
    "        try:\n",
    "            # check to see if this is a header by trying to convert the first field to an integer\n",
    "            id = int(fields[0])\n",
    "            # we have a real record, so do some mapping\n",
    "            for word in WORD_RE.findall(fields[3]):\n",
    "                sys.stdout.write('{0}{1}{2}\\n'.format(word, separator, 1))\n",
    "        except:\n",
    "            # must be a header record so skip it\n",
    "            pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # increment counter for mapper call, write to STDERR\n",
    "    sys.stderr.write(\"reporter:counter:Code Call Counters,mapper,1\\n\")\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer32b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer32b.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "def read(file, separator='\\t'):\n",
    "    for line in file:\n",
    "        yield line.rstrip().split(separator, 1)\n",
    "\n",
    "def main(separator='\\t'):\n",
    "    # input comes from STDIN (standard input)\n",
    "    data = read(sys.stdin, separator=separator)\n",
    "    # groupby groups multiple word-count pairs by word,\n",
    "    # and creates an iterator that returns consecutive keys and their group:\n",
    "    #   current_word - string containing a word (the key)\n",
    "    #   group - iterator yielding all [\"&lt;current_word&gt;\", \"&lt;count&gt;\"] items\n",
    "    for current_word, group in groupby(data, itemgetter(0)):\n",
    "        try:\n",
    "            total_count = sum(int(count) for current_word, count in group)\n",
    "            sys.stderr.write(\"reporter:counter:Code Call Counters,reducer pairs,1\\n\")\n",
    "            sys.stdout.write(\"{0}{1}{2}\\n\".format(current_word, separator, total_count))\n",
    "        except ValueError:\n",
    "            sys.stderr.write(\"reporter:counter:Code Call Counters,reducer skipped pairs,1\\n\")\n",
    "            # count was not a number, so silently discard this item\n",
    "            pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # increment counter for reducer call, write to STDERR\n",
    "    sys.stderr.write(\"reporter:counter:Code Call Counters,reducer,1\\n\")\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper32b.py\n",
    "!chmod a+x reducer32b.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat short_complaints.csv | ./mapper32b.py | sort -k1,1 | ./reducer32b.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 19:16:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 19:16:41 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/rcordell/recordsOutput\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/recordsOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 19:16:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/hadoop-unjar3393828723429341869/] [] /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/streamjob6639340843874969134.jar tmpDir=null\n",
      "16/01/30 19:16:44 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/30 19:16:44 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/30 19:16:44 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/30 19:16:44 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/30 19:16:45 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454175435207_0018\n",
      "16/01/30 19:16:45 INFO impl.YarnClientImpl: Submitted application application_1454175435207_0018\n",
      "16/01/30 19:16:45 INFO mapreduce.Job: The url to track the job: http://Rons-iMac-Retina.local:8088/proxy/application_1454175435207_0018/\n",
      "16/01/30 19:16:45 INFO mapreduce.Job: Running job: job_1454175435207_0018\n",
      "16/01/30 19:16:49 INFO mapreduce.Job: Job job_1454175435207_0018 running in uber mode : false\n",
      "16/01/30 19:16:49 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/30 19:16:55 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/30 19:17:01 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/01/30 19:17:02 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/30 19:17:02 INFO mapreduce.Job: Job job_1454175435207_0018 completed successfully\n",
      "16/01/30 19:17:02 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=11233477\n",
      "\t\tFILE: Number of bytes written=22943108\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910105\n",
      "\t\tHDFS: Number of bytes written=2221\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5782\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5489\n",
      "\t\tTotal time spent by all map tasks (ms)=5782\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5489\n",
      "\t\tTotal vcore-seconds taken by all map tasks=5782\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=5489\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=5920768\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=5620736\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=980482\n",
      "\t\tMap output bytes=9272501\n",
      "\t\tMap output materialized bytes=11233489\n",
      "\t\tInput split bytes=222\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=180\n",
      "\t\tReduce shuffle bytes=11233489\n",
      "\t\tReduce input records=980482\n",
      "\t\tReduce output records=180\n",
      "\t\tSpilled Records=1960964\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=142\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=693108736\n",
      "\tCode Call Counters\n",
      "\t\tmapper=2\n",
      "\t\treducer=2\n",
      "\t\treducer pairs=180\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50909883\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2221\n",
      "16/01/30 19:17:02 INFO streaming.StreamJob: Output directory: recordsOutput\n"
     ]
    }
   ],
   "source": [
    "!yarn jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "    -D mapreduce.job.reduces=2 \\\n",
    "    -D mapreduce.job.maps=2 \\\n",
    "    -files \"mapper32b.py,reducer32b.py\" \\\n",
    "    -mapper mapper32b.py \\\n",
    "    -reducer reducer32b.py \\\n",
    "    -input Consumer_Complaints.csv \\\n",
    "    -output recordsOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 19:17:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "      86\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/rcordell/recordsOutput/part-00000 | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing combinerQ2part3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combinerQ2part3.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "def read(file, separator='\\t'):\n",
    "    for line in file:\n",
    "        yield line.rstrip().split(separator, 1)\n",
    "\n",
    "def main(separator='\\t'):\n",
    "    # input comes from STDIN (standard input)\n",
    "    data = read(sys.stdin, separator=separator)\n",
    "    # groupby groups multiple word-count pairs by word,\n",
    "    # and creates an iterator that returns consecutive keys and their group:\n",
    "    #   current_word - string containing a word (the key)\n",
    "    #   group - iterator yielding all [\"&lt;current_word&gt;\", \"&lt;count&gt;\"] items\n",
    "    for current_word, group in groupby(data, itemgetter(0)):\n",
    "        try:\n",
    "            total_count = sum(int(count) for current_word, count in group)\n",
    "            sys.stderr.write(\"reporter:counter:Code Call Counters,combiner pairs,1\\n\")\n",
    "            sys.stdout.write(\"{0}{1}{2}\\n\".format(current_word, separator, total_count))\n",
    "        except ValueError:\n",
    "            sys.stderr.write(\"reporter:counter:Code Call Counters,combiner skipped pairs,1\\n\")            \n",
    "            # count was not a number, so silently discard this item\n",
    "            pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # increment counter for combiner call, write to STDERR\n",
    "    sys.stderr.write(\"reporter:counter:Code Call Counters,combiner,1\\n\")\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x combinerQ2part3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 14:02:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 14:02:02 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/rcordell/recordsOutput\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/recordsOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 14:02:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/hadoop-unjar6847100501139577256/] [] /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/streamjob7429796512504517614.jar tmpDir=null\n",
      "16/01/31 14:02:03 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/31 14:02:03 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/31 14:02:04 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/31 14:02:04 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/31 14:02:04 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454175435207_0083\n",
      "16/01/31 14:02:04 INFO impl.YarnClientImpl: Submitted application application_1454175435207_0083\n",
      "16/01/31 14:02:04 INFO mapreduce.Job: The url to track the job: http://Rons-iMac-Retina.local:8088/proxy/application_1454175435207_0083/\n",
      "16/01/31 14:02:04 INFO mapreduce.Job: Running job: job_1454175435207_0083\n",
      "16/01/31 14:02:09 INFO mapreduce.Job: Job job_1454175435207_0083 running in uber mode : false\n",
      "16/01/31 14:02:09 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/31 14:02:15 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/01/31 14:02:16 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/31 14:02:19 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/01/31 14:02:20 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/31 14:02:20 INFO mapreduce.Job: Job job_1454175435207_0083 completed successfully\n",
      "16/01/31 14:02:20 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4690\n",
      "\t\tFILE: Number of bytes written=488314\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910105\n",
      "\t\tHDFS: Number of bytes written=2221\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6623\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3217\n",
      "\t\tTotal time spent by all map tasks (ms)=6623\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3217\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6623\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3217\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6781952\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3294208\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=980482\n",
      "\t\tMap output bytes=9272501\n",
      "\t\tMap output materialized bytes=4702\n",
      "\t\tInput split bytes=222\n",
      "\t\tCombine input records=980482\n",
      "\t\tCombine output records=331\n",
      "\t\tReduce input groups=180\n",
      "\t\tReduce shuffle bytes=4702\n",
      "\t\tReduce input records=331\n",
      "\t\tReduce output records=180\n",
      "\t\tSpilled Records=662\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=142\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=696778752\n",
      "\tCode Call Counters\n",
      "\t\tcombiner=4\n",
      "\t\tcombiner pairs=331\n",
      "\t\tmapper=2\n",
      "\t\treducer=2\n",
      "\t\treducer pairs=180\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50909883\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2221\n",
      "16/01/31 14:02:20 INFO streaming.StreamJob: Output directory: recordsOutput\n"
     ]
    }
   ],
   "source": [
    "!yarn jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "    -D mapreduce.job.reduces=2 \\\n",
    "    -D mapreduce.job.maps=2 \\\n",
    "    -files \"mapper32b.py,combinerQ2part3.py,reducer32b.py\" \\\n",
    "    -mapper mapper32b.py \\\n",
    "    -reducer reducer32b.py \\\n",
    "    -combiner combinerQ2part3.py \\\n",
    "    -input Consumer_Complaints.csv \\\n",
    "    -output recordsOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 14:03:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "APR\t3431\n",
      "Account\t16555\n",
      "Applied\t139\n",
      "Arbitration\t168\n",
      "Bankruptcy\t222\n",
      "Billing\t8158\n",
      "Can't\t1999\n",
      "Cash\t240\n",
      "Closing\t2795\n",
      "Cont'd\t11848\n",
      "Convenience\t75\n",
      "Credit\t14768\n",
      "Debt\t1343\n",
      "Delinquent\t1061\n",
      "Deposits\t10555\n",
      "Disclosure\t5214\n",
      "False\t2508\n",
      "I\t925\n",
      "Incorrect\t29133\n",
      "Making\t3226\n",
      "Overlimit\t127\n",
      "Payoff\t1155\n",
      "Received\t118\n",
      "Repaying\t3844\n",
      "Sale\t139\n",
      "Settlement\t4350\n",
      "Unable\t4357\n",
      "Unsolicited\t640\n",
      "Workout\t350\n",
      "Wrong\t98\n",
      "a\t3503\n",
      "account\t4126\n",
      "acct\t163\n",
      "an\t2505\n",
      "and\t16448\n",
      "available\t274\n",
      "being\t5663\n",
      "by\t5663\n",
      "caused\t5663\n",
      "changes\t350\n",
      "charges\t131\n",
      "checks\t75\n",
      "company's\t4858\n",
      "credit\t40483\n",
      "debt\t17966\n",
      "delay\t243\n",
      "determination\t1490\n",
      "did\t139\n",
      "disputes\t6938\n",
      "expect\t807\n",
      "fees\t807\n",
      "for\t929\n",
      "issuance\t640\n",
      "issue\t1098\n",
      "not\t12353\n",
      "of\t10885\n",
      "on\t29069\n",
      "or\t22533\n",
      "owed\t11848\n",
      "payments\t3226\n",
      "process\t5505\n",
      "processing\t243\n",
      "promised\t274\n",
      "protection\t4139\n",
      "receive\t139\n",
      "received\t98\n",
      "relations\t1367\n",
      "repay\t1647\n",
      "representation\t2508\n",
      "service\t1518\n",
      "servicer\t1944\n",
      "statement\t1220\n",
      "tactics\t6920\n",
      "terms\t350\n",
      "the\t6248\n",
      "theft\t3276\n",
      "threatening\t2505\n",
      "to\t8401\n",
      "transfer\t597\n",
      "unable\t3821\n",
      "use\t1477\n",
      "verification\t5214\n",
      "was\t274\n",
      "wrong\t71\n",
      "you\t3821\n",
      "your\t3844\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/rcordell/recordsOutput/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "Using a single reducer: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapperQ2part4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapperQ2part4.py\n",
    "#!/usr/bin/env python\n",
    "#\n",
    "# W261 HW 3.2 MapReduce and Counters for Code Analysis\n",
    "# Read from a CSV file of consumer complaints with fields as follows:\n",
    "#\n",
    "# Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,\n",
    "# Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "#\n",
    "# Use counters to count the number of times the mapper is called\n",
    "#\n",
    "# Remember that in Hadoop streaming, to update a counter is to write to STDERR in the format\n",
    "# reporter:counter:<group>,<counter>,<amount>\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# Read data from STDIN and use counters to count the data\n",
    "def main(separator='\\t'):   \n",
    "    for line in sys.stdin:\n",
    "        fields = line.split(',')\n",
    "        try:\n",
    "            # check to see if this is a header by trying to convert the first field to an integer\n",
    "            id = int(fields[0])\n",
    "            # we have a real record, so do some mapping\n",
    "            for word in WORD_RE.findall(fields[3]):\n",
    "                sys.stdout.write('{0}{1}{2}\\n'.format(word.lower(), separator, 1))\n",
    "                sys.stdout.write('{0}{1}{2}\\n'.format('*',  separator, 1))\n",
    "        except:\n",
    "            # must be a header record so skip it\n",
    "            pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # increment counter for mapper call, write to STDERR\n",
    "    sys.stderr.write(\"reporter:counter:Code Call Counters,mapper,1\\n\")\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapperQ2part4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing combinerQ2part4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combinerQ2part4.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "def read(file, separator='\\t'):\n",
    "    for line in file:\n",
    "        yield line.rstrip().split(separator, 1)\n",
    "\n",
    "def main(separator='\\t'):\n",
    "    # input comes from STDIN (standard input)\n",
    "    data = read(sys.stdin, separator=separator)\n",
    "    # groupby groups multiple word-count pairs by word,\n",
    "    # and creates an iterator that returns consecutive keys and their group:\n",
    "    #   current_word - string containing a word (the key)\n",
    "    #   group - iterator yielding all [\"&lt;current_word&gt;\", \"&lt;count&gt;\"] items\n",
    "    for current_word, group in groupby(data, itemgetter(0)):\n",
    "        try:\n",
    "            total_count = sum(int(count) for current_word, count in group)\n",
    "            sys.stderr.write(\"reporter:counter:Code Call Counters,combiner pairs,1\\n\")\n",
    "            sys.stdout.write(\"{0}{1}{2}\\n\".format(current_word, separator, total_count))\n",
    "            if current_word == '*':\n",
    "                sys.stderr.write(\"reporter:counter:Code Call Counters,combiner total flags,1\\n\")\n",
    "        except ValueError:\n",
    "            sys.stderr.write(\"reporter:counter:Code Call Counters,combiner skipped pairs,1\\n\")            \n",
    "            # count was not a number, so silently discard this item\n",
    "            pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # increment counter for combiner call, write to STDERR\n",
    "    sys.stderr.write(\"reporter:counter:Code Call Counters,combiner,1\\n\")\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x combinerQ2part4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducerQ2part4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducerQ2part4.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "def read(file, separator='\\t'):\n",
    "    for line in file:\n",
    "        yield line.rstrip().split(separator, 1)\n",
    "\n",
    "def main(separator='\\t'):\n",
    "    total = 1\n",
    "    total_first = True\n",
    "    \n",
    "    # input comes from STDIN (standard input)\n",
    "    data = read(sys.stdin, separator=separator)\n",
    "    # groupby groups multiple word-count pairs by word,\n",
    "    # and creates an iterator that returns consecutive keys and their group:\n",
    "    #   current_word - string containing a word (the key)\n",
    "    #   group - iterator yielding all [\"&lt;current_word&gt;\", \"&lt;count&gt;\"] items\n",
    "    for current_word, group in groupby(data, itemgetter(0)):\n",
    "        try:\n",
    "            total_count = sum(int(count) for current_word, count in group)\n",
    "            if current_word == '*':\n",
    "                total = total_count\n",
    "                sys.stderr.write(\"reporter:counter:Code Call Counters,reducer total indicators,1\\n\")\n",
    "                sys.stderr.write(\"reporter:counter:Code Call Counters,reducer word count,{0}\\n\".format(total))\n",
    "                if total_first:\n",
    "                    sys.stderr.write(\"reporter:counter:Code Call Counters,reducer recvd total first,1\\n\")\n",
    "            else:\n",
    "                sys.stderr.write(\"reporter:counter:Code Call Counters,reducer processed,1\\n\")\n",
    "                sys.stdout.write(\"{0:20}\\t{1:10}\\t{2}\\n\".format(current_word, total_count, \n",
    "                                                              float(total_count)/float(total))) \n",
    "                total_first = False\n",
    "                \n",
    "        except ValueError:\n",
    "            sys.stderr.write(\"reporter:counter:Code Call Counters,reducer skipped pairs,1\\n\")            \n",
    "            # count was not a number, so silently discard this item\n",
    "            pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # increment counter for combiner call, write to STDERR\n",
    "    sys.stderr.write(\"reporter:counter:Code Call Counters,reducer,1\\n\")\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducerQ2part4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 14:04:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 14:04:37 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/rcordell/recordsOutput\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/recordsOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 14:04:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/hadoop-unjar3426155122428197504/] [] /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/streamjob7024171405453559567.jar tmpDir=null\n",
      "16/01/31 14:04:39 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/31 14:04:39 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/31 14:04:40 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/31 14:04:40 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/31 14:04:40 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454175435207_0084\n",
      "16/01/31 14:04:40 INFO impl.YarnClientImpl: Submitted application application_1454175435207_0084\n",
      "16/01/31 14:04:40 INFO mapreduce.Job: The url to track the job: http://Rons-iMac-Retina.local:8088/proxy/application_1454175435207_0084/\n",
      "16/01/31 14:04:40 INFO mapreduce.Job: Running job: job_1454175435207_0084\n",
      "16/01/31 14:04:44 INFO mapreduce.Job: Job job_1454175435207_0084 running in uber mode : false\n",
      "16/01/31 14:04:44 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/31 14:04:54 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/01/31 14:04:56 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/31 14:05:01 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/31 14:05:01 INFO mapreduce.Job: Job job_1454175435207_0084 completed successfully\n",
      "16/01/31 14:05:01 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=19077327\n",
      "\t\tFILE: Number of bytes written=38513861\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910105\n",
      "\t\tHDFS: Number of bytes written=8287\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=14672\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4113\n",
      "\t\tTotal time spent by all map tasks (ms)=14672\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4113\n",
      "\t\tTotal vcore-seconds taken by all map tasks=14672\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4113\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=15024128\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4211712\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=1960964\n",
      "\t\tMap output bytes=15155393\n",
      "\t\tMap output materialized bytes=19077333\n",
      "\t\tInput split bytes=222\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=170\n",
      "\t\tReduce shuffle bytes=19077333\n",
      "\t\tReduce input records=1960964\n",
      "\t\tReduce output records=169\n",
      "\t\tSpilled Records=3921928\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=165\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=539492352\n",
      "\tCode Call Counters\n",
      "\t\tmapper=2\n",
      "\t\treducer=1\n",
      "\t\treducer processed=169\n",
      "\t\treducer recvd total first=1\n",
      "\t\treducer total indicators=1\n",
      "\t\treducer word count=980482\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50909883\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=8287\n",
      "16/01/31 14:05:01 INFO streaming.StreamJob: Output directory: recordsOutput\n"
     ]
    }
   ],
   "source": [
    "# mapreduce.partition.keycomparator.options -> partition on which part of the key\n",
    "# stream.num.map.output.key.fields=2 -> tells the partitioner to treat the output of the mapper\n",
    "#       as a key with 2 fields and no value\n",
    "# mapreduce.partition.keycomparator.options=\"-k2nr -k1\" -> sort first on the second key field in numeric descending\n",
    "#       then sort on the first key field as alpha ascending\n",
    "\n",
    "!yarn jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=\"-k2,2nr\" \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k2nr -k1\" \\\n",
    "    -D mapreduce.job.reduces=1 \\\n",
    "    -D mapreduce.job.maps=2 \\\n",
    "    -files \"mapperQ2part4.py,reducerQ2part4.py\" \\\n",
    "    -mapper mapperQ2part4.py \\\n",
    "    -reducer reducerQ2part4.py \\\n",
    "    -input Consumer_Complaints.csv \\\n",
    "    -output recordsOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Top 50 Occuring Words with Count and Frequency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 14:05:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "loan                \t    119630\t0.122011418874\n",
      "modification        \t     70487\t0.0718901519865\n",
      "credit              \t     55251\t0.0563508560076\n",
      "servicing           \t     36767\t0.0374989036005\n",
      "report              \t     34903\t0.0355977978178\n",
      "incorrect           \t     29133\t0.0297129371064\n",
      "information         \t     29069\t0.0296476630882\n",
      "on                  \t     29069\t0.0296476630882\n",
      "or                  \t     22533\t0.0229815539704\n",
      "account             \t     20681\t0.0210926870662\n",
      "debt                \t     19309\t0.0196933752991\n",
      "and                 \t     16448\t0.0167754227003\n",
      "opening             \t     16205\t0.0165275854121\n",
      "club                \t     12545\t0.0127947274912\n",
      "health              \t     12545\t0.0127947274912\n",
      "not                 \t     12353\t0.0125989054363\n",
      "attempts            \t     11848\t0.0120838526357\n",
      "collect             \t     11848\t0.0120838526357\n",
      "cont'd              \t     11848\t0.0120838526357\n",
      "owed                \t     11848\t0.0120838526357\n",
      "of                  \t     10885\t0.0111016826418\n",
      "my                  \t     10731\t0.0109446170353\n",
      "deposits            \t     10555\t0.010765113485\n",
      "withdrawals         \t     10555\t0.010765113485\n",
      "problems            \t      9484\t0.0096727935852\n",
      "application         \t      8868\t0.00904453115916\n",
      "to                  \t      8401\t0.00856823480696\n",
      "unable              \t      8178\t0.00834079564949\n",
      "billing             \t      8158\t0.00832039751877\n",
      "other               \t      7886\t0.00804298294104\n",
      "disputes            \t      6938\t0.00707611154514\n",
      "communication       \t      6920\t0.00705775322749\n",
      "tactics             \t      6920\t0.00705775322749\n",
      "reporting           \t      6559\t0.00668956696808\n",
      "lease               \t      6337\t0.00646314771714\n",
      "the                 \t      6248\t0.00637237603546\n",
      "being               \t      5663\t0.00577573071204\n",
      "by                  \t      5663\t0.00577573071204\n",
      "caused              \t      5663\t0.00577573071204\n",
      "funds               \t      5663\t0.00577573071204\n",
      "low                 \t      5663\t0.00577573071204\n",
      "process             \t      5505\t0.00561458547939\n",
      "disclosure          \t      5214\t0.00531779267748\n",
      "verification        \t      5214\t0.00531779267748\n",
      "managing            \t      5006\t0.00510565211804\n",
      "company's           \t      4858\t0.00495470595075\n",
      "investigation       \t      4858\t0.00495470595075\n",
      "identity            \t      4729\t0.00482313800763\n",
      "card                \t      4405\t0.00449268829005\n",
      "get                 \t      4357\t0.00444373277633\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/rcordell/recordsOutput/part-00000 | sort -k2,2nr | head -50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Bottom 10 Occuring Words with Count and Frequency **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 14:05:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "apply               \t       118\t0.00012034897122\n",
      "amount              \t        98\t9.9950840505e-05\n",
      "credited            \t        92\t9.38314012904e-05\n",
      "payment             \t        92\t9.38314012904e-05\n",
      "checks              \t        75\t7.64929901824e-05\n",
      "convenience         \t        75\t7.64929901824e-05\n",
      "amt                 \t        71\t7.24133640393e-05\n",
      "day                 \t        71\t7.24133640393e-05\n",
      "disclosures         \t        64\t6.5274018289e-05\n",
      "missing             \t        64\t6.5274018289e-05\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/rcordell/recordsOutput/part-00000 | sort -k2,2nr | tail -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 OPTIONAL \n",
    "Using 2 reducers: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 14:05:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 14:05:40 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/rcordell/recordsOutput\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/recordsOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 14:05:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/hadoop-unjar5153604838863109387/] [] /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/streamjob47573597575542206.jar tmpDir=null\n",
      "16/01/31 14:05:43 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/31 14:05:43 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/31 14:05:44 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/31 14:05:44 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/31 14:05:44 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454175435207_0085\n",
      "16/01/31 14:05:44 INFO impl.YarnClientImpl: Submitted application application_1454175435207_0085\n",
      "16/01/31 14:05:44 INFO mapreduce.Job: The url to track the job: http://Rons-iMac-Retina.local:8088/proxy/application_1454175435207_0085/\n",
      "16/01/31 14:05:44 INFO mapreduce.Job: Running job: job_1454175435207_0085\n",
      "16/01/31 14:05:49 INFO mapreduce.Job: Job job_1454175435207_0085 running in uber mode : false\n",
      "16/01/31 14:05:49 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/31 14:05:55 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/01/31 14:05:56 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/31 14:06:00 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/01/31 14:06:02 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/31 14:06:03 INFO mapreduce.Job: Job job_1454175435207_0085 completed successfully\n",
      "16/01/31 14:06:03 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=19077333\n",
      "\t\tFILE: Number of bytes written=38633616\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910105\n",
      "\t\tHDFS: Number of bytes written=7464\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7246\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=6302\n",
      "\t\tTotal time spent by all map tasks (ms)=7246\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6302\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7246\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=6302\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=7419904\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=6453248\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=1960964\n",
      "\t\tMap output bytes=15155393\n",
      "\t\tMap output materialized bytes=19077345\n",
      "\t\tInput split bytes=222\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=170\n",
      "\t\tReduce shuffle bytes=19077345\n",
      "\t\tReduce input records=1960964\n",
      "\t\tReduce output records=169\n",
      "\t\tSpilled Records=3921928\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=235\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=724566016\n",
      "\tCode Call Counters\n",
      "\t\tmapper=2\n",
      "\t\treducer=2\n",
      "\t\treducer processed=169\n",
      "\t\treducer recvd total first=1\n",
      "\t\treducer total indicators=1\n",
      "\t\treducer word count=980482\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50909883\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=7464\n",
      "16/01/31 14:06:03 INFO streaming.StreamJob: Output directory: recordsOutput\n"
     ]
    }
   ],
   "source": [
    "# mapreduce.partition.keycomparator.options -> partition on which part of the key\n",
    "# stream.num.map.output.key.fields=2 -> tells the partitioner to treat the output of the mapper\n",
    "#       as a key with 2 fields and no value\n",
    "# mapreduce.partition.keycomparator.options=\"-k2nr -k1\" -> sort first on the second key field in numeric descending\n",
    "#       then sort on the first key field as alpha ascending\n",
    "\n",
    "!yarn jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=\"-k2,2nr\" \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k2nr -k1\" \\\n",
    "    -D mapreduce.job.reduces=2 \\\n",
    "    -D mapreduce.job.maps=2 \\\n",
    "    -files \"mapperQ2part4.py,reducerQ2part4.py\" \\\n",
    "    -mapper mapperQ2part4.py \\\n",
    "    -reducer reducerQ2part4.py \\\n",
    "    -input Consumer_Complaints.csv \\\n",
    "    -output recordsOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Needs more work - the word count totals are not getting to the reducers; only one of the them is getting it.**\n",
    "\n",
    "May need more partitioning control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.3. Shopping Cart Analysis\n",
    "Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "\n",
    "For this homework use the online browsing behavior dataset located at: \n",
    "\n",
    "       https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "Each line in this dataset represents a browsing session of a customer. \n",
    "On each line, each string of 8 characters represents the id of an item browsed during that session. \n",
    "The items are separated by spaces.\n",
    "\n",
    "Here are the first few lines of the ProductPurchaseData \n",
    "\n",
    "    FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \n",
    "    GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \n",
    "    ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 \n",
    "    ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 \n",
    "    ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444 \n",
    "\n",
    "\n",
    "Do some exploratory data analysis of this dataset. \n",
    "\n",
    "How many unique items are available from this supplier?\n",
    "\n",
    "Using a single reducer: Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapperQ3part1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapperQ3part1.py\n",
    "#!/usr/bin/env python\n",
    "#\n",
    "# W261 HW 3.3 MapReduce and Counters for Code Analysis\n",
    "#\n",
    "# Read a line of product ids where each line is a user session and the ids are products the user viewed\n",
    "# Emit product_id, 1\n",
    "#\n",
    "# Use counters to count the number of times the mapper is called\n",
    "#\n",
    "# Remember that in Hadoop streaming, to update a counter is to write to STDERR in the format\n",
    "# reporter:counter:<group>,<counter>,<amount>\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "# Read data from STDIN and use counters to count the data\n",
    "def main(separator='\\t'):   \n",
    "    for line in sys.stdin:\n",
    "        basket = line.split()\n",
    "        for item in basket:\n",
    "            # emit item, 1, item count in basket\n",
    "            sys.stdout.write('{0}{1}{2}{1}{3}\\n'.format(item, separator, 1, len(basket)))\n",
    "            \n",
    "            # emit *, 1, item count in basket\n",
    "            sys.stdout.write('{0}{1}{2}{1}{3}\\n'.format('*',  separator, 1, len(basket)))\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    # increment counter for mapper call, write to STDERR\n",
    "    sys.stderr.write(\"reporter:counter:Code Call Counters,mapper,1\\n\")\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapperQ3part1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combinerQ3part1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combinerQ3part1.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "def main(separator='\\t'):\n",
    "    largest_basket = 0\n",
    "    current_item = None\n",
    "    current_count = 0\n",
    "    \n",
    "    # input comes from STDIN (standard input)\n",
    "    for line in sys.stdin:\n",
    "        # read a line and split into item, count, and basketsize\n",
    "        item, count, basketsize = line.split(separator)\n",
    "\n",
    "        # still counting the same key, keep accumulating\n",
    "        if current_item == item.strip():\n",
    "            current_count += int(count.strip())\n",
    "        else:\n",
    "            if current_item:\n",
    "                # emit the accumulated key count\n",
    "                sys.stdout.write(\"{0}{1}{2}{1}{3}\\n\".format(current_item, separator, current_count, largest_basket))\n",
    "\n",
    "            # set/reset state\n",
    "            current_count = int(count.strip())\n",
    "            current_item = item.strip()\n",
    "            largest_basket = max(largest_basket, int(basketsize.strip()))\n",
    "\n",
    "    if current_item:\n",
    "        # emit the last accumulated key count\n",
    "        sys.stdout.write(\"{0}{1}{2}{1}{3}\\n\".format(current_item, separator, current_count, largest_basket))\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # increment counter for combiner call, write to STDERR\n",
    "    sys.stderr.write(\"reporter:counter:Code Call Counters,combiner,1\\n\")\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x combinerQ3part1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducerQ3part1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducerQ3part1.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "def main(separator='\\t'):\n",
    "    total = 0\n",
    "    unique_items = 0\n",
    "    largest_basket = 0\n",
    "    current_item = None\n",
    "    current_count = 0\n",
    "    \n",
    "    # input comes from STDIN (standard input)\n",
    "    for line in sys.stdin:\n",
    "        item, count, basketsize = line.split(separator)\n",
    "        if item == '*':\n",
    "            total += int(count.strip())\n",
    "            sys.stderr.write(\"reporter:counter:Code Call Counters,Recieved Totals,1\\n\")\n",
    "        else:\n",
    "            # still counting the same key, keep accumulating\n",
    "            if current_item == item.strip():\n",
    "                current_count += int(count.strip())\n",
    "            else:\n",
    "                if current_item:\n",
    "                    # emit the accumulated key count\n",
    "                    sys.stdout.write(\"{0}{1}{2}\\n\".format(current_item, separator, current_count))\n",
    "\n",
    "                # set/reset state\n",
    "                current_count = int(count.strip())\n",
    "                current_item = item.strip()\n",
    "                largest_basket = max(largest_basket, int(basketsize.strip()))\n",
    "                # increment the unique item count\n",
    "                unique_items += 1\n",
    "                # increment the unique item counter\n",
    "                sys.stderr.write(\"reporter:counter:Code Call Counters,Unique Item Counter,1\\n\")\n",
    "\n",
    "    if current_item:\n",
    "            # emit the last accumulated key count\n",
    "            sys.stdout.write(\"{0}{1}{2}\\n\".format(current_item, separator, current_count))\n",
    "    \n",
    "    # emit special key for max basket size\n",
    "    sys.stdout.write(\"{0}{1}{2}\\n\".format('*MAXBASKET', separator, largest_basket))\n",
    "    \n",
    "    # emit special key for max basket size\n",
    "    sys.stdout.write(\"{0}{1}{2}\\n\".format('*UNIQUEITEMS', separator, unique_items))\n",
    "    \n",
    "    #emit special key for total items\n",
    "    sys.stdout.write(\"{0}{1}{2}\\n\".format('*TOTALITEMS', separator, total))    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # increment counter for combiner call, write to STDERR\n",
    "    sys.stderr.write(\"reporter:counter:Code Call Counters,reducer,1\\n\")\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducerQ3part1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducerAQ3part1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducerAQ3part1.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "def main(separator='\\t'):\n",
    "    total = 1\n",
    "    total_first = True\n",
    "    \n",
    "    # input comes from STDIN (standard input)\n",
    "    for line in (sys.stdin):\n",
    "        item, count = line.split(separator)\n",
    "        try:\n",
    "            if item == '*':\n",
    "                total = int(count)\n",
    "                if total_first:\n",
    "                    sys.stderr.write(\"reporter:counter:Code Call Counters,reducer saw total first,1\\n\")\n",
    "                    sys.stderr.write(\"reporter:counter:Code Call Counters,unique items,1\\total\")\n",
    "            else:\n",
    "                sys.stderr.write(\"reporter:counter:Code Call Counters,reducer pairs,1\\n\")\n",
    "                sys.stdout.write(\"{0}{1}{2}\\n\".format(item, separator, count))\n",
    "                total_first = False\n",
    "\n",
    "        except ValueError:\n",
    "            sys.stderr.write(\"reporter:counter:Code Call Counters,reducer skipped pairs,1\\n\")            \n",
    "            # count was not a number, so silently discard this item\n",
    "            pass\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    # increment counter for combiner call, write to STDERR\n",
    "    sys.stderr.write(\"reporter:counter:Code Call Counters,reducer,1\\n\")\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducerAQ3part1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 14:22:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "put: `/user/rcordell/ProductPurchaseData.txt': File exists\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put ~/Dropbox/W261/ProductPurchaseData.txt /user/rcordell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 23:53:18 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 23:53:18 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/rcordell/recordsOutput\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/recordsOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 23:53:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/hadoop-unjar5190047823412462063/] [] /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/streamjob2633578674413320649.jar tmpDir=null\n",
      "16/01/31 23:53:20 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/31 23:53:20 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/31 23:53:20 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/31 23:53:20 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/31 23:53:20 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/01/31 23:53:20 INFO Configuration.deprecation: mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "16/01/31 23:53:20 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/01/31 23:53:20 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454175435207_0158\n",
      "16/01/31 23:53:21 INFO impl.YarnClientImpl: Submitted application application_1454175435207_0158\n",
      "16/01/31 23:53:21 INFO mapreduce.Job: The url to track the job: http://Rons-iMac-Retina.local:8088/proxy/application_1454175435207_0158/\n",
      "16/01/31 23:53:21 INFO mapreduce.Job: Running job: job_1454175435207_0158\n",
      "16/01/31 23:53:25 INFO mapreduce.Job: Job job_1454175435207_0158 running in uber mode : false\n",
      "16/01/31 23:53:25 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/31 23:53:31 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/01/31 23:53:32 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/31 23:53:35 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/31 23:53:35 INFO mapreduce.Job: Job job_1454175435207_0158 completed successfully\n",
      "16/01/31 23:53:35 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=289193\n",
      "\t\tFILE: Number of bytes written=939594\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462089\n",
      "\t\tHDFS: Number of bytes written=142710\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5557\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1602\n",
      "\t\tTotal time spent by all map tasks (ms)=5557\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1602\n",
      "\t\tTotal vcore-seconds taken by all map tasks=5557\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=1602\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=5690368\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=1640448\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=761648\n",
      "\t\tMap output bytes=7845208\n",
      "\t\tMap output materialized bytes=289199\n",
      "\t\tInput split bytes=222\n",
      "\t\tCombine input records=761648\n",
      "\t\tCombine output records=17747\n",
      "\t\tReduce input groups=12593\n",
      "\t\tReduce shuffle bytes=289199\n",
      "\t\tReduce input records=17747\n",
      "\t\tReduce output records=12595\n",
      "\t\tSpilled Records=35494\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=134\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=532152320\n",
      "\tCode Call Counters\n",
      "\t\tRecieved Totals=2\n",
      "\t\tUnique Item Counter=12592\n",
      "\t\tcombiner=2\n",
      "\t\tmapper=2\n",
      "\t\treducer=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3461867\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=142710\n",
      "16/01/31 23:53:35 INFO streaming.StreamJob: Output directory: recordsOutput\n"
     ]
    }
   ],
   "source": [
    "# mapreduce.partition.keycomparator.options -> partition on which part of the key\n",
    "# stream.num.map.output.key.fields=2 -> tells the partitioner to treat the output of the mapper\n",
    "#       as a key with 2 fields and no value\n",
    "# mapreduce.partition.keycomparator.options=\"-k2nr -k1\" -> sort first on the second key field in numeric descending\n",
    "#       then sort on the first key field as alpha ascending\n",
    "\n",
    "!yarn jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D mapred.text.key.partitioner.options=\"-k1,1\" \\\n",
    "    -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapred.text.key.comparator.options=\"-k1,1 -k2,2nr\" \\\n",
    "    -D mapreduce.job.reduces=1 \\\n",
    "    -D mapreduce.job.maps=2 \\\n",
    "    -files \"mapperQ3part1.py,combinerQ3part1.py,reducerQ3part1.py\" \\\n",
    "    -mapper mapperQ3part1.py \\\n",
    "    -combiner combinerQ3part1.py \\\n",
    "    -reducer reducerQ3part1.py \\\n",
    "    -input ProductPurchaseData.txt \\\n",
    "    -output recordsOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapperbQ3part1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapperbQ3part1.py\n",
    "#!/usr/bin/env python\n",
    "#\n",
    "# W261 HW 3.3 MapReduce and Counters for Code Analysis\n",
    "#\n",
    "# Read a line of product ids where each line is a user session and the ids are products the user viewed\n",
    "# Emit product_id, 1\n",
    "#\n",
    "# Use counters to count the number of times the mapper is called\n",
    "#\n",
    "# Remember that in Hadoop streaming, to update a counter is to write to STDERR in the format\n",
    "# reporter:counter:<group>,<counter>,<amount>\n",
    "\n",
    "import sys\n",
    "\n",
    "# Read data from STDIN and use counters to count the data\n",
    "def main(separator='\\t'):   \n",
    "    for line in sys.stdin:\n",
    "        item, count = line.strip().split(separator)\n",
    "        sys.stdout.write('{0}{1}{2}\\n'.format(item, separator, count))\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    # increment counter for mapper call, write to STDERR\n",
    "    sys.stderr.write(\"reporter:counter:Code Call Counters,mapper,1\\n\")\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapperbQ3part1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducerbQ3part1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducerbQ3part1.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "total_items = 1.0\n",
    "unique_items = 0\n",
    "max_basket = 0\n",
    "\n",
    "def main(separator='\\t'):    \n",
    "    # input comes from STDIN (standard input)\n",
    "    for line in (sys.stdin):\n",
    "        item, count = line.strip().split(separator)\n",
    "        \n",
    "        # these special keys should arrive first\n",
    "        if item.strip() == '*TOTALITEMS':\n",
    "            total_items = float(count.strip())\n",
    "        elif item.strip() == '*UNIQUEITEMS':\n",
    "            unique_items = int(count.strip())\n",
    "        elif item.strip() == '*MAXBASKET':\n",
    "            max_basket = int(count.strip())\n",
    "        else:    \n",
    "            sys.stdout.write('{0}{1}{2}{1}{3}\\n'.format(item, separator, count, float(count)/total_items))\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    # increment counter for combiner call, write to STDERR\n",
    "    sys.stderr.write(\"reporter:counter:Code Call Counters,reducer,1\\n\")\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducerbQ3part1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 00:06:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 00:06:07 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/rcordell/recordsOutput2\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/recordsOutput2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 00:06:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/hadoop-unjar4708819701684813221/] [] /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/streamjob5995398753935075682.jar tmpDir=null\n",
      "16/02/01 00:06:10 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/01 00:06:10 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/02/01 00:06:10 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 00:06:10 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 00:06:10 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454175435207_0159\n",
      "16/02/01 00:06:10 INFO impl.YarnClientImpl: Submitted application application_1454175435207_0159\n",
      "16/02/01 00:06:10 INFO mapreduce.Job: The url to track the job: http://Rons-iMac-Retina.local:8088/proxy/application_1454175435207_0159/\n",
      "16/02/01 00:06:10 INFO mapreduce.Job: Running job: job_1454175435207_0159\n",
      "16/02/01 00:06:15 INFO mapreduce.Job: Job job_1454175435207_0159 running in uber mode : false\n",
      "16/02/01 00:06:15 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/01 00:06:20 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/01 00:06:24 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 00:06:25 INFO mapreduce.Job: Job job_1454175435207_0159 completed successfully\n",
      "16/02/01 00:06:25 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=180501\n",
      "\t\tFILE: Number of bytes written=600165\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=142822\n",
      "\t\tHDFS: Number of bytes written=368635\n",
      "\t\tHDFS: Number of read operations=6\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=1931\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1574\n",
      "\t\tTotal time spent by all map tasks (ms)=1931\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1574\n",
      "\t\tTotal vcore-seconds taken by all map tasks=1931\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=1574\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1977344\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=1611776\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=12595\n",
      "\t\tMap output records=12595\n",
      "\t\tMap output bytes=155305\n",
      "\t\tMap output materialized bytes=180501\n",
      "\t\tInput split bytes=112\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12595\n",
      "\t\tReduce shuffle bytes=180501\n",
      "\t\tReduce input records=12595\n",
      "\t\tReduce output records=12592\n",
      "\t\tSpilled Records=25190\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=66\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=349175808\n",
      "\tCode Call Counters\n",
      "\t\tmapper=1\n",
      "\t\treducer=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=142710\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=368635\n",
      "16/02/01 00:06:25 INFO streaming.StreamJob: Output directory: recordsOutput2\n"
     ]
    }
   ],
   "source": [
    "# mapreduce.partition.keycomparator.options -> partition on which part of the key\n",
    "# stream.num.map.output.key.fields=2 -> tells the partitioner to treat the output of the mapper\n",
    "#       as a key with 2 fields and no value\n",
    "# mapreduce.partition.keycomparator.options=\"-k2nr -k1\" -> sort first on the second key field in numeric descending\n",
    "#       then sort on the first key field as alpha ascending\n",
    "\n",
    "!yarn jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "    -D mapreduce.job.reduces=1 \\\n",
    "    -D mapreduce.job.maps=1 \\\n",
    "    -files \"mapperbQ3part1.py,reducerbQ3part1.py\" \\\n",
    "    -mapper mapperbQ3part1.py \\\n",
    "    -reducer reducerbQ3part1.py \\\n",
    "    -input recordsOutput/part-00000 \\\n",
    "    -output recordsOutput2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 00:06:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "DAI62779\t6667\t0.0175067747831\n",
      "FRO40251\t3881\t0.010191059387\n",
      "ELE17451\t3875\t0.0101753040775\n",
      "GRO73461\t3602\t0.00945843749344\n",
      "SNA80324\t3044\t0.00799319370628\n",
      "ELE32164\t2851\t0.0074863979161\n",
      "DAI75645\t2736\t0.00718442114993\n",
      "SNA45677\t2455\t0.0064465474865\n",
      "FRO31317\t2330\t0.0061183118711\n",
      "DAI85309\t2293\t0.00602115412894\n",
      "ELE26917\t2292\t0.00601852824402\n",
      "FRO80039\t2233\t0.00586360103355\n",
      "GRO21487\t2115\t0.00555374661261\n",
      "SNA99873\t2083\t0.00546971829507\n",
      "GRO59710\t2004\t0.00526227338613\n",
      "GRO71621\t1920\t0.00504169905258\n",
      "FRO85978\t1918\t0.00503644728273\n",
      "GRO30386\t1840\t0.00483162825872\n",
      "ELE74009\t1816\t0.00476860702057\n",
      "GRO56726\t1784\t0.00468457870302\n",
      "DAI63921\t1773\t0.00465569396887\n",
      "GRO46854\t1756\t0.00461105392517\n",
      "ELE66600\t1713\t0.00449814087347\n",
      "DAI83733\t1712\t0.00449551498855\n",
      "FRO32293\t1702\t0.00446925613932\n",
      "ELE66810\t1697\t0.0044561267147\n",
      "SNA55762\t1646\t0.00432220658362\n",
      "DAI22177\t1627\t0.00427231477008\n",
      "FRO78087\t1531\t0.00402022981745\n",
      "ELE99737\t1516\t0.0039808415436\n",
      "ELE34057\t1489\t0.00390994265067\n",
      "GRO94758\t1489\t0.00390994265067\n",
      "FRO35904\t1436\t0.00377077074974\n",
      "FRO53271\t1420\t0.00372875659097\n",
      "SNA93860\t1407\t0.00369462008697\n",
      "SNA90094\t1390\t0.00364998004327\n",
      "GRO38814\t1352\t0.00355019641619\n",
      "ELE56788\t1345\t0.00353181522173\n",
      "GRO61133\t1321\t0.00346879398357\n",
      "DAI88807\t1316\t0.00345566455896\n",
      "ELE74482\t1316\t0.00345566455896\n",
      "ELE59935\t1311\t0.00344253513434\n",
      "SNA96271\t1295\t0.00340052097557\n",
      "DAI43223\t1290\t0.00338739155095\n",
      "ELE91337\t1289\t0.00338476566603\n",
      "GRO15017\t1275\t0.0033480032771\n",
      "DAI31081\t1261\t0.00331124088818\n",
      "GRO81087\t1220\t0.00320357960633\n",
      "DAI22896\t1219\t0.0032009537214\n",
      "GRO85051\t1214\t0.00318782429679\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/rcordell/recordsOutput2/part-00000 | head -50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 OPTIONAL \n",
    "Using 2 reducers:  Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.4. (Computationally prohibitive but then again Hadoop can handle this) Pairs\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a map-reduce program \n",
    "to find products which are frequently browsed together. Fix the support count (cooccurence count) to s = 100 \n",
    "(i.e. product pairs need to occur together at least 100 times to be considered frequent) \n",
    "and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more.\n",
    "\n",
    "List the top 50 product pairs with corresponding support count (aka frequency), and relative frequency or support (number of records where they coccur, the number of records where they coccur/the number of baskets in the dataset)  in decreasing order of support  for frequent (100>count) itemsets of size 2. \n",
    "\n",
    "Use the Pairs pattern (lecture 3)  to  extract these frequent itemsets of size 2. Free free to use combiners if they bring value. Instrument your code with counters for count the number of times your mapper, combiner and reducers are called.  \n",
    "\n",
    "Please output records of the following form for the top 50 pairs (itemsets of size 2): \n",
    "\n",
    "      item1, item2, support count, support\n",
    "\n",
    "\n",
    "\n",
    "Fix the ordering of the pairs lexicographically (left to right), \n",
    "and break ties in support (between pairs, if any exist) \n",
    "by taking the first ones in lexicographically increasing order. \n",
    "\n",
    "Report  the compute time for the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapperQ3part1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapperQ4.py\n",
    "#!/usr/bin/env python\n",
    "#\n",
    "# W261 HW 3.3 MapReduce and Counters for Code Analysis\n",
    "#\n",
    "# Read a line of product ids where each line is a user session and the ids are products the user viewed\n",
    "# Emit product_id, 1\n",
    "#\n",
    "# Use counters to count the number of times the mapper is called\n",
    "#\n",
    "# Remember that in Hadoop streaming, to update a counter is to write to STDERR in the format\n",
    "# reporter:counter:<group>,<counter>,<amount>\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "# Read data from STDIN and use counters to count the data\n",
    "def main(separator='\\t'):   \n",
    "    for line in sys.stdin:\n",
    "        basket = line.split()\n",
    "        for item in basket:\n",
    "            # emit item, 1, item count in basket\n",
    "            sys.stdout.write('{0}{1}{2}{1}{3}\\n'.format(item, separator, 1, len(basket)))\n",
    "            \n",
    "            # emit *, 1, item count in basket\n",
    "            sys.stdout.write('{0}{1}{2}{1}{3}\\n'.format('*',  separator, 1, len(basket)))\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    # increment counter for mapper call, write to STDERR\n",
    "    sys.stderr.write(\"reporter:counter:Code Call Counters,mapper,1\\n\")\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapperQ4part1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combinerQ3part1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combinerQ4.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "def main(separator='\\t'):\n",
    "    largest_basket = 0\n",
    "    current_item = None\n",
    "    current_count = 0\n",
    "    \n",
    "    # input comes from STDIN (standard input)\n",
    "    for line in sys.stdin:\n",
    "        # read a line and split into item, count, and basketsize\n",
    "        item, count, basketsize = line.split(separator)\n",
    "\n",
    "        # still counting the same key, keep accumulating\n",
    "        if current_item == item.strip():\n",
    "            current_count += int(count.strip())\n",
    "        else:\n",
    "            if current_item:\n",
    "                # emit the accumulated key count\n",
    "                sys.stdout.write(\"{0}{1}{2}{1}{3}\\n\".format(current_item, separator, current_count, largest_basket))\n",
    "\n",
    "            # set/reset state\n",
    "            current_count = int(count.strip())\n",
    "            current_item = item.strip()\n",
    "            largest_basket = max(largest_basket, int(basketsize.strip()))\n",
    "\n",
    "    if current_item:\n",
    "        # emit the last accumulated key count\n",
    "        sys.stdout.write(\"{0}{1}{2}{1}{3}\\n\".format(current_item, separator, current_count, largest_basket))\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # increment counter for combiner call, write to STDERR\n",
    "    sys.stderr.write(\"reporter:counter:Code Call Counters,combiner,1\\n\")\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x combinerQ4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile reducerQ4.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "def main(separator='\\t'):\n",
    "    total = 0\n",
    "    unique_items = 0\n",
    "    largest_basket = 0\n",
    "    current_item = None\n",
    "    current_count = 0\n",
    "    \n",
    "    # input comes from STDIN (standard input)\n",
    "    for line in sys.stdin:\n",
    "        item, count, basketsize = line.split(separator)\n",
    "        if item == '*':\n",
    "            total += int(count.strip())\n",
    "            sys.stderr.write(\"reporter:counter:Code Call Counters,Recieved Totals,1\\n\")\n",
    "        else:\n",
    "            # still counting the same key, keep accumulating\n",
    "            if current_item == item.strip():\n",
    "                current_count += int(count.strip())\n",
    "            else:\n",
    "                if current_item:\n",
    "                    # emit the accumulated key count\n",
    "                    sys.stdout.write(\"{0}{1}{2}\\n\".format(current_item, separator, current_count))\n",
    "\n",
    "                # set/reset state\n",
    "                current_count = int(count.strip())\n",
    "                current_item = item.strip()\n",
    "                largest_basket = max(largest_basket, int(basketsize.strip()))\n",
    "                # increment the unique item count\n",
    "                unique_items += 1\n",
    "                # increment the unique item counter\n",
    "                sys.stderr.write(\"reporter:counter:Code Call Counters,Unique Item Counter,1\\n\")\n",
    "\n",
    "    if current_item:\n",
    "            # emit the last accumulated key count\n",
    "            sys.stdout.write(\"{0}{1}{2}\\n\".format(current_item, separator, current_count))\n",
    "    \n",
    "    # emit special key for max basket size\n",
    "    sys.stdout.write(\"{0}{1}{2}\\n\".format('*MAXBASKET', separator, largest_basket))\n",
    "    \n",
    "    # emit special key for max basket size\n",
    "    sys.stdout.write(\"{0}{1}{2}\\n\".format('*UNIQUEITEMS', separator, unique_items))\n",
    "    \n",
    "    #emit special key for total items\n",
    "    sys.stdout.write(\"{0}{1}{2}\\n\".format('*TOTALITEMS', separator, total))    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # increment counter for combiner call, write to STDERR\n",
    "    sys.stderr.write(\"reporter:counter:Code Call Counters,reducer,1\\n\")\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.5: Stripes\n",
    "Repeat 3.4 using the stripes design pattern for finding cooccuring pairs.\n",
    "\n",
    "Report  the compute times for stripes job versus the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts. Discuss the differences in these counts between the Pairs and Stripes jobs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "OPTIONAL: all HW below this are optional \n",
    "\n",
    "NOTE:   -- as of 1/28/2016 the instructions needs to be completed (Talk to Jimi)\n",
    "\n",
    "\n",
    "\n",
    "== Preliminary information ===\n",
    "\n",
    "Much of this homework beyond this point will focus on the Apriori algorithm for frequent itemset  mining and the additional step for extracting association rules from these frequent itemsets.\n",
    "Please acquaint yourself with the background information (below)\n",
    "before approaching the remaining  assignments.\n",
    "\n",
    "=== Apriori background information ===\n",
    "\n",
    "Some background material for the  Apriori algorithm is located at:\n",
    "\n",
    " - Slides in Live Session #3\n",
    " - https://en.wikipedia.org/wiki/Apriori_algorithm\n",
    " - https://www.dropbox.com/s/k2zm4otych279z2/Apriori-good-slides.pdf?dl=0\n",
    " - http://snap.stanford.edu/class/cs246-2014/slides/02-assocrules.pdf\n",
    "\n",
    "Association Rules are frequently used for Market Basket Analysis (MBA) by retailers to\n",
    "understand the purchase behavior of their customers. This information can be then used for\n",
    "many different purposes such as cross-selling and up-selling of products, sales promotions,\n",
    "loyalty programs, store design, discount plans and many others.\n",
    "Evaluation of item sets: Once you have found the frequent itemsets of a dataset, you need\n",
    "to choose a subset of them as your recommendations. Commonly used metrics for measuring\n",
    "significance and interest for selecting rules for recommendations are: confidence; lift; and conviction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.6\n",
    "What is the Apriori algorithm? Describe an example use in your domain of expertise and what kind of . Define confidence and lift.\n",
    "\n",
    "NOTE:\n",
    "For the remaining homework use the online browsing behavior dataset located at (same dataset as used above): \n",
    "\n",
    "       https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "Each line in this dataset represents a browsing session of a customer. \n",
    "On each line, each string of 8 characters represents the id of an item browsed during that session. \n",
    "The items are separated by spaces.\n",
    "\n",
    "Here are the first few lines of the ProductPurchaseData \n",
    "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \n",
    "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \n",
    "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 \n",
    "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 \n",
    "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.7. Shopping Cart Analysis\n",
    "Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a program using the A-priori algorithm\n",
    "to find products which are frequently browsed together. Fix the support to s = 100 \n",
    "(i.e. product sets need to occur together at least 100 times to be considered frequent) \n",
    "and find itemsets of size 2 and 3.\n",
    "\n",
    "Then extract association rules from these frequent items. \n",
    "\n",
    "A rule is of the form: \n",
    "\n",
    "(item1, item5) ⇒ item2.\n",
    "\n",
    "List the top 10 discovered rules in descreasing order of confidence in the following format\n",
    " \n",
    "(item1, item5) ⇒ item2, supportCount ,support, confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.8\n",
    "\n",
    "Benchmark your results using the pyFIM implementation of the Apriori algorithm\n",
    "(Apriori - Association Rule Induction / Frequent Item Set Mining implemented by Christian Borgelt). \n",
    "You can download pyFIM from here: \n",
    "\n",
    "http://www.borgelt.net/pyfim.html\n",
    "\n",
    "Comment on the results from both implementations (your Hadoop MapReduce of apriori versus pyFIM) \n",
    "in terms of results and execution times.\n",
    "\n",
    "\n",
    "HW3.8 (Conceptual Exercise)\n",
    "\n",
    "Suppose that you wished to perform the Apriori algorithm once again,\n",
    "though this time now with the goal of listing the top 5 rules with corresponding confidence scores \n",
    "in decreasing order of confidence score for itemsets of size 3 using Hadoop MapReduce.\n",
    "A rule is now of the form: \n",
    "\n",
    "(item1, item2) ⇒ item3 \n",
    "\n",
    "Recall that the Apriori algorithm is iterative for increasing itemset size,\n",
    "working off of the frequent itemsets of the previous size to explore \n",
    "ONLY the NECESSARY subset of a large combinatorial space. \n",
    "Describe how you might design a framework to perform this exercise.\n",
    "\n",
    "In particular, focus on the following:\n",
    "  — map-reduce steps required\n",
    "  - enumeration of item sets and filtering for frequent candidates\n",
    "\n",
    "\n",
    "END OF HOMEWORK\n",
    "==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
