{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS W261 Spring 2016 Homework Week 3\n",
    "\n",
    "**Ted Dunmire**  glenn.dunmire.iv@gmail.com<br/>\n",
    "**Filip Krunic**  fkrunic@ischool.berkeley.edu<br />\n",
    "**Ron Cordell** ron.cordell@ischool.berkeley.edu<br />\n",
    "\n",
    "W261-4<br />\n",
    "January 26, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.0.\n",
    "\n",
    "**What is a merge sort? Where is it used in Hadoop?**\n",
    "\n",
    "A merge sort merges sorted lists into a single sorted list. The merge sort works by establishing a pointer to the beginning of each sorted list as well as a new \"merge\" list. The objects or keys in each list referenced by the pointers are compared and the chosen one moved or copied to the location indicated by the pointer of the merge list. The merge list pointer is advanced as is the pointer for the list from which the object was moved. This is repeated until all objects in all list have been moved or copied to the new merge list. The comparator function \"chooses\" the object from the source lists based on the rules coded into the comparator function such as the largest, the smallest, etc.\n",
    "\n",
    "Hadoop uses a merge sort during the shuffle process when it takes output from multiple sources such as mappers or combiners and merges them into the sorted streams used by downstream processes.\n",
    "\n",
    "**How is  a combiner function in the context of Hadoop? \n",
    "Give an example where it can be used and justify why it should be used in the context of this problem.**\n",
    "\n",
    "A combiner function is a function that can be used by Hadoop anywhere between the mappers and producers to help eliminate network and data traffic, especially as part of the shuffle. A an example combiner function typically provides a partial aggregation point for data emitted from the mapper to reduce hotspots in the shuffle.\n",
    "\n",
    "An example where a combiner can be used to good effect is in a word count scenario, where the mapper emits the word as key and a value of 1. A combiner can perform aggregations on the key-value pairs by combining those with the same key and adding their values. This greatly reduces the granularity of the data required to shuffle and sort and provide to the reducers and helps reduce the amount of network and disk traffic of the shuffle, decreasing the overall run time.\n",
    "\n",
    "**What is the Hadoop shuffle?**\n",
    "\n",
    "![MapReduce Workflow](https://rawgit.com/dunmireg/HW3Submission/master/MapReduceWorkflow.png)\n",
    "\n",
    "The Hadoop shuffle takes the mapper outpus, merges them and sorts them, computes a hash to partition them, and routes each partition to a reducer. It corresponds the the shuffle and sort section of the above diagram, between the mappers and reducers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## HW3.1 Use Counters to do EDA (exploratory data analysis and to monitor progress)**\n",
    "\n",
    "**The consumer complaints dataset consists of diverse consumer complaints, which have been reported across the United States regarding various types of loans. The dataset consists of records of the form:**\n",
    "\n",
    "    Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    "**Here’s is the first few lines of the  of the Consumer Complaints  Dataset:**\n",
    "\n",
    "    Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "    1114245,Debt collection,Medical,Disclosure verification of debt,Not given enough info to verify debt,FL,32219,Web,11/13/2014,11/13/2014,\"Choice Recovery, Inc.\",Closed with explanation,Yes,\n",
    "    1114488,Debt collection,Medical,Disclosure verification of debt,Right to dispute notice not received,TX,75006,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "    1114255,Bank account or service,Checking account,Deposits and withdrawals,,NY,11102,Web,11/13/2014,11/13/2014,\"FNIS (Fidelity National Information Services, Inc.)\",In progress,Yes,\n",
    "    1115106,Debt collection,\"Other (phone, health club, etc.)\",Communication tactics,Frequent or repeated calls,GA,31721,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "\n",
    "**Now, let’s use Hadoop Counters to identify the number of complaints pertaining to debt collection, mortgage and other categories (all other categories get lumped into this one) in the consumer complaints dataset. Basically produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.1 Map Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "line_num = 0\n",
    "for line in sys.stdin: #read input\n",
    "    if line_num == 0:\n",
    "        line_num += 1 #skip the first line, which is a header\n",
    "        continue\n",
    "    else:\n",
    "        line = line.strip() #remove extra chars\n",
    "        line = line.rstrip()\n",
    "        line = line.split(',') #split on comma delimiter\n",
    "        if line[1] == \"Debt collection\": #line[1] is the issue part of the complaint\n",
    "            sys.stderr.write('reporter:counter:Debt-Counter,Total,1\\n') #increment counter based on complaint\n",
    "        elif line[1] == 'Mortgage':\n",
    "            sys.stderr.write('reporter:counter:Mortgage-Counter,Total,1\\n')\n",
    "        else:\n",
    "            sys.stderr.write('reporter:counter:Other-Counter,Total,1\\n') #all other issues are lumped together\n",
    "        sys.stderr.write(\"reporter:counter:Tokens,Total,1\\n\")\n",
    "        print line[1] + '\\t' + '1' #This just prints the issue and a 1. Will use to check if counters are correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.1 Reduce Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "#keep counters to see how many results we have\n",
    "debt_counter = 0\n",
    "mortgage_counter = 0\n",
    "other_counter = 0\n",
    "\n",
    "for line in sys.stdin: #read input\n",
    "    line = line.strip().split('\\t')\n",
    "    if line[0] == \"Debt collection\": #recall we passed the issue as line[0] from the mapper, here we parse \n",
    "        debt_counter +=1  \n",
    "    elif line[0] == 'Mortgage':\n",
    "        mortgage_counter += 1\n",
    "    else:\n",
    "        other_counter += 1\n",
    "print \"Debt collection: \" + str(debt_counter) #print results. These should match the counters from the mapper\n",
    "print \"Mortgage: \" + str(mortgage_counter)\n",
    "print \"Other: \" + str(other_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 3.1 Start Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-resourcemanager-Glenns-Air.home.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-nodemanager-Glenns-Air.home.out\n",
      "16/01/29 21:04:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-namenode-Glenns-Air.home.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-datanode-Glenns-Air.home.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-secondarynamenode-Glenns-Air.home.out\n",
      "16/01/29 21:05:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start hadoop yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/29 21:05:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#add input to hdfs\n",
    "!hdfs dfs -put Consumer_Complaints.csv /user/dunmireg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.1 Hadoop MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/29 21:05:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/29 21:05:34 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/29 21:05:34 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/29 21:05:34 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/29 21:05:35 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/29 21:05:35 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/29 21:05:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1206054309_0001\n",
      "16/01/29 21:05:35 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/29 21:05:35 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/29 21:05:35 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/29 21:05:35 INFO mapreduce.Job: Running job: job_local1206054309_0001\n",
      "16/01/29 21:05:35 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/29 21:05:35 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/29 21:05:35 INFO mapred.LocalJobRunner: Starting task: attempt_local1206054309_0001_m_000000_0\n",
      "16/01/29 21:05:36 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/29 21:05:36 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/29 21:05:36 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/29 21:05:36 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/Consumer_Complaints.csv:0+50906486\n",
      "16/01/29 21:05:36 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/29 21:05:36 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/29 21:05:36 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/29 21:05:36 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/29 21:05:36 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/29 21:05:36 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/29 21:05:36 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/29 21:05:36 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./mapper.py]\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/29 21:05:36 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:36 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:36 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:36 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:36 INFO streaming.PipeMapRed: Records R/W=2032/1\n",
      "16/01/29 21:05:36 INFO streaming.PipeMapRed: R/W/S=10000/7896/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:36 INFO mapreduce.Job: Job job_local1206054309_0001 running in uber mode : false\n",
      "16/01/29 21:05:36 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/29 21:05:37 INFO streaming.PipeMapRed: R/W/S=100000/98646/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:37 INFO streaming.PipeMapRed: R/W/S=200000/198558/0 in:200000=200000/1 [rec/s] out:198558=198558/1 [rec/s]\n",
      "16/01/29 21:05:38 INFO streaming.PipeMapRed: R/W/S=300000/298306/0 in:150000=300000/2 [rec/s] out:149153=298306/2 [rec/s]\n",
      "16/01/29 21:05:38 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/29 21:05:38 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/29 21:05:38 INFO mapred.LocalJobRunner: \n",
      "16/01/29 21:05:38 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/29 21:05:38 INFO mapred.MapTask: Spilling map output\n",
      "16/01/29 21:05:38 INFO mapred.MapTask: bufstart = 0; bufend = 4878322; bufvoid = 104857600\n",
      "16/01/29 21:05:38 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24962748(99850992); length = 1251649/6553600\n",
      "16/01/29 21:05:38 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/29 21:05:38 INFO mapred.Task: Task:attempt_local1206054309_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/29 21:05:38 INFO mapred.LocalJobRunner: Records R/W=2032/1\n",
      "16/01/29 21:05:38 INFO mapred.Task: Task 'attempt_local1206054309_0001_m_000000_0' done.\n",
      "16/01/29 21:05:38 INFO mapred.LocalJobRunner: Finishing task: attempt_local1206054309_0001_m_000000_0\n",
      "16/01/29 21:05:38 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/29 21:05:38 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/29 21:05:38 INFO mapred.LocalJobRunner: Starting task: attempt_local1206054309_0001_r_000000_0\n",
      "16/01/29 21:05:38 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/29 21:05:38 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/29 21:05:38 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/29 21:05:38 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2102fea2\n",
      "16/01/29 21:05:38 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/29 21:05:38 INFO reduce.EventFetcher: attempt_local1206054309_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/29 21:05:38 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1206054309_0001_m_000000_0 decomp: 5504150 len: 5504154 to MEMORY\n",
      "16/01/29 21:05:38 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/29 21:05:38 INFO reduce.InMemoryMapOutput: Read 5504150 bytes from map-output for attempt_local1206054309_0001_m_000000_0\n",
      "16/01/29 21:05:38 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 5504150, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->5504150\n",
      "16/01/29 21:05:38 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/29 21:05:38 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/29 21:05:38 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/29 21:05:38 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/29 21:05:38 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 5504124 bytes\n",
      "16/01/29 21:05:39 INFO reduce.MergeManagerImpl: Merged 1 segments, 5504150 bytes to disk to satisfy reduce memory limit\n",
      "16/01/29 21:05:39 INFO reduce.MergeManagerImpl: Merging 1 files, 5504154 bytes from disk\n",
      "16/01/29 21:05:39 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/29 21:05:39 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/29 21:05:39 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 5504124 bytes\n",
      "16/01/29 21:05:39 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./reducer.py]\n",
      "16/01/29 21:05:39 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/29 21:05:39 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: Records R/W=312913/1\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/29 21:05:39 INFO mapred.Task: Task:attempt_local1206054309_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/29 21:05:39 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/29 21:05:39 INFO mapred.Task: Task attempt_local1206054309_0001_r_000000_0 is allowed to commit now\n",
      "16/01/29 21:05:39 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1206054309_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/consumer_counters/_temporary/0/task_local1206054309_0001_r_000000\n",
      "16/01/29 21:05:39 INFO mapred.LocalJobRunner: Records R/W=312913/1 > reduce\n",
      "16/01/29 21:05:39 INFO mapred.Task: Task 'attempt_local1206054309_0001_r_000000_0' done.\n",
      "16/01/29 21:05:39 INFO mapred.LocalJobRunner: Finishing task: attempt_local1206054309_0001_r_000000_0\n",
      "16/01/29 21:05:39 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/29 21:05:39 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/29 21:05:39 INFO mapreduce.Job: Job job_local1206054309_0001 completed successfully\n",
      "16/01/29 21:05:39 INFO mapreduce.Job: Counters: 39\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=11220438\n",
      "\t\tFILE: Number of bytes written=17314316\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=101812972\n",
      "\t\tHDFS: Number of bytes written=57\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=312913\n",
      "\t\tMap output bytes=4878322\n",
      "\t\tMap output materialized bytes=5504154\n",
      "\t\tInput split bytes=111\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10\n",
      "\t\tReduce shuffle bytes=5504154\n",
      "\t\tReduce input records=312913\n",
      "\t\tReduce output records=3\n",
      "\t\tSpilled Records=625826\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=9\n",
      "\t\tTotal committed heap usage (bytes)=671088640\n",
      "\tDebt-Counter\n",
      "\t\tTotal=44372\n",
      "\tMortgage-Counter\n",
      "\t\tTotal=125752\n",
      "\tOther-Counter\n",
      "\t\tTotal=142789\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tTokens\n",
      "\t\tTotal=312913\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=57\n",
      "16/01/29 21:05:39 INFO streaming.StreamJob: Output directory: consumer_counters\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input Consumer_Complaints.csv \\\n",
    "-output consumer_counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/29 21:06:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Debt collection: 44372\t\n",
      "Mortgage: 125752\t\n",
      "Other: 142789\t\n"
     ]
    }
   ],
   "source": [
    "#show results\n",
    "!hdfs dfs -cat /user/dunmireg/consumer_counters/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your  job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible. **\n",
    "\n",
    "![Job Tracker](https://rawgit.com/dunmireg/HW3Submission/master/jobtrackercounters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the mapper counters and the results from counting in the reducer match. In total we found: \n",
    "\n",
    "__Debt collection: 44372\t\n",
    "Mortgage: 125752\t\n",
    "Other: 142789__\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-----\n",
    "\n",
    "## HW 3.2 Analyze the performance of your Mappers, Combiners and Reducers using Counters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.2 Part 1\n",
    "\n",
    "For this brief study the Input file will be one record (the next line only): \n",
    "foo foo quux labs foo bar quux\n",
    "\n",
    "\n",
    "Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively. Please explain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Write input string to a file on disk **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make a basic text file to test\n",
    "with open('input_text.txt', 'w') as myfile:\n",
    "    myfile.write('foo foo quux labs foo bar quux')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.2 Word Count Map Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "sys.stderr.write('reporter:counter:Map-Count,Total,1\\n') #increment mapper counter\n",
    "for line in sys.stdin: #read input\n",
    "    line = line.strip()\n",
    "    line = line.rstrip()\n",
    "    line = line.split() #split on space delimiter\n",
    "    for word in line:\n",
    "        print word + '\\t' + '1' #emit word and count of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.2 Word Count Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "current_word = None #keep track of current word  and current count\n",
    "current_count = None\n",
    "word = None\n",
    "sys.stderr.write('reporter:counter:Reduce-Counter,Total,1\\n') #increment reducer counter\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    line = line.rstrip()\n",
    "    word, count = line.split('\\t') #split here on tab\n",
    "    \n",
    "    if current_word == word: #increment the counter\n",
    "        current_count += int(count)\n",
    "    else:\n",
    "        if current_word:\n",
    "            print '%s\\t%s' % (current_word, current_count) #print results when we have found a new word\n",
    "        current_word = word #change word\n",
    "        current_count = int(count)\n",
    "\n",
    "#print final word\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!cat input.txt | python mapper.py | sort | python reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.2 Hadoop MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 21:00:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "# create input file and add input to hdfs\n",
    "!echo \"foo foo quux labs foo bar quux\" >input_text.txt\n",
    "!hdfs dfs -put input_text.txt /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 22:33:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 22:33:49 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 22:33:49 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 22:33:49 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 22:33:49 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 22:33:49 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 22:33:49 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 22:33:49 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/01 22:33:49 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local927319060_0001\n",
      "16/02/01 22:33:49 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 22:33:49 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 22:33:49 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 22:33:49 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 22:33:49 INFO mapreduce.Job: Running job: job_local927319060_0001\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Starting task: attempt_local927319060_0001_m_000000_0\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 22:33:50 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 22:33:50 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 22:33:50 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 22:33:50 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/input_text.txt:0+30\n",
      "16/02/01 22:33:50 INFO mapred.MapTask: numReduceTasks: 4\n",
      "16/02/01 22:33:50 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 22:33:50 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 22:33:50 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 22:33:50 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 22:33:50 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 22:33:50 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./mapper.py]\n",
      "16/02/01 22:33:50 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 22:33:50 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 22:33:50 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 22:33:50 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 22:33:50 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 22:33:50 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 22:33:50 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 22:33:50 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 22:33:50 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 22:33:50 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 22:33:50 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 22:33:50 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: \n",
      "16/02/01 22:33:50 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 22:33:50 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 22:33:50 INFO mapred.MapTask: bufstart = 0; bufend = 45; bufvoid = 104857600\n",
      "16/02/01 22:33:50 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214372(104857488); length = 25/6553600\n",
      "16/02/01 22:33:50 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 22:33:50 INFO mapred.Task: Task:attempt_local927319060_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
      "16/02/01 22:33:50 INFO mapred.Task: Task 'attempt_local927319060_0001_m_000000_0' done.\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Finishing task: attempt_local927319060_0001_m_000000_0\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Starting task: attempt_local927319060_0001_r_000000_0\n",
      "16/02/01 22:33:50 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 22:33:50 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 22:33:50 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 22:33:50 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@347fc133\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 22:33:50 INFO reduce.EventFetcher: attempt_local927319060_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 22:33:50 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local927319060_0001_m_000000_0 decomp: 20 len: 24 to MEMORY\n",
      "16/02/01 22:33:50 INFO reduce.InMemoryMapOutput: Read 20 bytes from map-output for attempt_local927319060_0001_m_000000_0\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 20, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->20\n",
      "16/02/01 22:33:50 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 13 bytes\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: Merged 1 segments, 20 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: Merging 1 files, 24 bytes from disk\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 13 bytes\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./reducer.py]\n",
      "16/02/01 22:33:50 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 22:33:50 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: Records R/W=2/1\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 22:33:50 INFO mapred.Task: Task:attempt_local927319060_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:33:50 INFO mapred.Task: Task attempt_local927319060_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 22:33:50 INFO output.FileOutputCommitter: Saved output of task 'attempt_local927319060_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/word_count/_temporary/0/task_local927319060_0001_r_000000\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Records R/W=2/1 > reduce\n",
      "16/02/01 22:33:50 INFO mapred.Task: Task 'attempt_local927319060_0001_r_000000_0' done.\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Finishing task: attempt_local927319060_0001_r_000000_0\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Starting task: attempt_local927319060_0001_r_000001_0\n",
      "16/02/01 22:33:50 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 22:33:50 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 22:33:50 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 22:33:50 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@34751e75\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 22:33:50 INFO reduce.EventFetcher: attempt_local927319060_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 22:33:50 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local927319060_0001_m_000000_0 decomp: 26 len: 30 to MEMORY\n",
      "16/02/01 22:33:50 INFO reduce.InMemoryMapOutput: Read 26 bytes from map-output for attempt_local927319060_0001_m_000000_0\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 26, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->26\n",
      "16/02/01 22:33:50 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 20 bytes\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: Merged 1 segments, 26 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: Merging 1 files, 30 bytes from disk\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 20 bytes\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./reducer.py]\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: Records R/W=3/1\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 22:33:50 INFO mapred.Task: Task:attempt_local927319060_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:33:50 INFO mapred.Task: Task attempt_local927319060_0001_r_000001_0 is allowed to commit now\n",
      "16/02/01 22:33:50 INFO output.FileOutputCommitter: Saved output of task 'attempt_local927319060_0001_r_000001_0' to hdfs://localhost:9000/user/dunmireg/word_count/_temporary/0/task_local927319060_0001_r_000001\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Records R/W=3/1 > reduce\n",
      "16/02/01 22:33:50 INFO mapred.Task: Task 'attempt_local927319060_0001_r_000001_0' done.\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Finishing task: attempt_local927319060_0001_r_000001_0\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Starting task: attempt_local927319060_0001_r_000002_0\n",
      "16/02/01 22:33:50 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 22:33:50 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 22:33:50 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 22:33:50 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4e432ccb\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 22:33:50 INFO reduce.EventFetcher: attempt_local927319060_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 22:33:50 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local927319060_0001_m_000000_0 decomp: 10 len: 14 to MEMORY\n",
      "16/02/01 22:33:50 INFO reduce.InMemoryMapOutput: Read 10 bytes from map-output for attempt_local927319060_0001_m_000000_0\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->10\n",
      "16/02/01 22:33:50 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4 bytes\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: Merged 1 segments, 10 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: Merging 1 files, 14 bytes from disk\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4 bytes\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./reducer.py]\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 22:33:50 INFO mapred.Task: Task:attempt_local927319060_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:33:50 INFO mapred.Task: Task attempt_local927319060_0001_r_000002_0 is allowed to commit now\n",
      "16/02/01 22:33:50 INFO output.FileOutputCommitter: Saved output of task 'attempt_local927319060_0001_r_000002_0' to hdfs://localhost:9000/user/dunmireg/word_count/_temporary/0/task_local927319060_0001_r_000002\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Records R/W=1/1 > reduce\n",
      "16/02/01 22:33:50 INFO mapred.Task: Task 'attempt_local927319060_0001_r_000002_0' done.\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Finishing task: attempt_local927319060_0001_r_000002_0\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Starting task: attempt_local927319060_0001_r_000003_0\n",
      "16/02/01 22:33:50 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 22:33:50 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 22:33:50 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 22:33:50 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@15760f0b\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 22:33:50 INFO reduce.EventFetcher: attempt_local927319060_0001_r_000003_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 22:33:50 INFO reduce.LocalFetcher: localfetcher#4 about to shuffle output of map attempt_local927319060_0001_m_000000_0 decomp: 11 len: 15 to MEMORY\n",
      "16/02/01 22:33:50 INFO reduce.InMemoryMapOutput: Read 11 bytes from map-output for attempt_local927319060_0001_m_000000_0\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 11, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->11\n",
      "16/02/01 22:33:50 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4 bytes\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: Merged 1 segments, 11 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: Merging 1 files, 15 bytes from disk\n",
      "16/02/01 22:33:50 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 22:33:50 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4 bytes\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./reducer.py]\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 22:33:50 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 22:33:50 INFO mapred.Task: Task:attempt_local927319060_0001_r_000003_0 is done. And is in the process of committing\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:33:50 INFO mapred.Task: Task attempt_local927319060_0001_r_000003_0 is allowed to commit now\n",
      "16/02/01 22:33:50 INFO output.FileOutputCommitter: Saved output of task 'attempt_local927319060_0001_r_000003_0' to hdfs://localhost:9000/user/dunmireg/word_count/_temporary/0/task_local927319060_0001_r_000003\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Records R/W=1/1 > reduce\n",
      "16/02/01 22:33:50 INFO mapred.Task: Task 'attempt_local927319060_0001_r_000003_0' done.\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: Finishing task: attempt_local927319060_0001_r_000003_0\n",
      "16/02/01 22:33:50 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 22:33:50 INFO mapreduce.Job: Job job_local927319060_0001 running in uber mode : false\n",
      "16/02/01 22:33:50 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 22:33:50 INFO mapreduce.Job: Job job_local927319060_0001 completed successfully\n",
      "16/02/01 22:33:51 INFO mapreduce.Job: Counters: 37\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=532031\n",
      "\t\tFILE: Number of bytes written=1998924\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=150\n",
      "\t\tHDFS: Number of bytes written=65\n",
      "\t\tHDFS: Number of read operations=55\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=25\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=7\n",
      "\t\tMap output bytes=45\n",
      "\t\tMap output materialized bytes=83\n",
      "\t\tInput split bytes=102\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=83\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=7\n",
      "\t\tTotal committed heap usage (bytes)=1551892480\n",
      "\tMap-Count\n",
      "\t\tTotal=1\n",
      "\tReduce-Counter\n",
      "\t\tTotal=4\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=30\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=26\n",
      "16/02/01 22:33:51 INFO streaming.StreamJob: Output directory: word_count\n"
     ]
    }
   ],
   "source": [
    "#run hadoop, manually setting the number of mappers and reducers\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.map.tasks=1 \\\n",
    "-D mapred.reduce.tasks=4 \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input input_text.txt \\\n",
    "-output word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** RESULT: Mappers - 1, Reducers - 4 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.2 Part 2\n",
    "\n",
    "Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.2 Part 2 Map Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper32b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper32b.py\n",
    "#!/usr/bin/env python\n",
    "#\n",
    "# W261 HW 3.2 MapReduce and Counters for Code Analysis\n",
    "# Read from a CSV file of consumer complaints with fields as follows:\n",
    "#\n",
    "# Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,\n",
    "# Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "#\n",
    "# Use counters to count the number of times the mapper is called\n",
    "#\n",
    "# Remember that in Hadoop streaming, to update a counter is to write to STDERR in the format\n",
    "# reporter:counter:<group>,<counter>,<amount>\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# Read data from STDIN and use counters to count the data\n",
    "def main(separator='\\t'):   \n",
    "    for line in sys.stdin:\n",
    "        fields = line.split(',')\n",
    "        try:\n",
    "            # check to see if this is a header by trying to convert the first field to an integer\n",
    "            id = int(fields[0])\n",
    "            # we have a real record, so do some mapping\n",
    "            for word in WORD_RE.findall(fields[3]):\n",
    "                sys.stdout.write('{0}{1}{2}\\n'.format(word, separator, 1))\n",
    "        except:\n",
    "            # must be a header record so skip it\n",
    "            pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # increment counter for mapper call, write to STDERR\n",
    "    sys.stderr.write(\"reporter:counter:Code Call Counters,mapper,1\\n\")\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.2 Part Reduce Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer32b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer32b.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "def read(file, separator='\\t'):\n",
    "    for line in file:\n",
    "        yield line.rstrip().split(separator, 1)\n",
    "\n",
    "def main(separator='\\t'):\n",
    "    # input comes from STDIN (standard input)\n",
    "    data = read(sys.stdin, separator=separator)\n",
    "    # groupby groups multiple word-count pairs by word,\n",
    "    # and creates an iterator that returns consecutive keys and their group:\n",
    "    #   current_word - string containing a word (the key)\n",
    "    #   group - iterator yielding all [\"&lt;current_word&gt;\", \"&lt;count&gt;\"] items\n",
    "    for current_word, group in groupby(data, itemgetter(0)):\n",
    "        try:\n",
    "            total_count = sum(int(count) for current_word, count in group)\n",
    "            sys.stderr.write(\"reporter:counter:Code Call Counters,reducer pairs,1\\n\")\n",
    "            sys.stdout.write(\"{0}{1}{2}\\n\".format(current_word, separator, total_count))\n",
    "        except ValueError:\n",
    "            sys.stderr.write(\"reporter:counter:Code Call Counters,reducer skipped pairs,1\\n\")\n",
    "            # count was not a number, so silently discard this item\n",
    "            pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # increment counter for reducer call, write to STDERR\n",
    "    sys.stderr.write(\"reporter:counter:Code Call Counters,reducer,1\\n\")\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.2 Part 2 Hadoop MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper32b.py\n",
    "!chmod a+x reducer32b.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 19:16:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/hadoop-unjar3393828723429341869/] [] /var/folders/z_/rfp5q2cd6db13d19v6yw0n8w0000gn/T/streamjob6639340843874969134.jar tmpDir=null\n",
      "16/01/30 19:16:44 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/30 19:16:44 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/30 19:16:44 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/30 19:16:44 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/30 19:16:45 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1454175435207_0018\n",
      "16/01/30 19:16:45 INFO impl.YarnClientImpl: Submitted application application_1454175435207_0018\n",
      "16/01/30 19:16:45 INFO mapreduce.Job: The url to track the job: http://Rons-iMac-Retina.local:8088/proxy/application_1454175435207_0018/\n",
      "16/01/30 19:16:45 INFO mapreduce.Job: Running job: job_1454175435207_0018\n",
      "16/01/30 19:16:49 INFO mapreduce.Job: Job job_1454175435207_0018 running in uber mode : false\n",
      "16/01/30 19:16:49 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/30 19:16:55 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/30 19:17:01 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/01/30 19:17:02 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/30 19:17:02 INFO mapreduce.Job: Job job_1454175435207_0018 completed successfully\n",
      "16/01/30 19:17:02 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=11233477\n",
      "\t\tFILE: Number of bytes written=22943108\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910105\n",
      "\t\tHDFS: Number of bytes written=2221\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5782\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5489\n",
      "\t\tTotal time spent by all map tasks (ms)=5782\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5489\n",
      "\t\tTotal vcore-seconds taken by all map tasks=5782\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=5489\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=5920768\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=5620736\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=980482\n",
      "\t\tMap output bytes=9272501\n",
      "\t\tMap output materialized bytes=11233489\n",
      "\t\tInput split bytes=222\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=180\n",
      "\t\tReduce shuffle bytes=11233489\n",
      "\t\tReduce input records=980482\n",
      "\t\tReduce output records=180\n",
      "\t\tSpilled Records=1960964\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=142\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=693108736\n",
      "\tCode Call Counters\n",
      "\t\tmapper=2\n",
      "\t\treducer=2\n",
      "\t\treducer pairs=180\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50909883\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2221\n",
      "16/01/30 19:17:02 INFO streaming.StreamJob: Output directory: recordsOutput\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/rcordell/recordsOutput\n",
    "!yarn jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "    -D mapreduce.job.reduces=2 \\\n",
    "    -D mapreduce.job.maps=2 \\\n",
    "    -files \"mapper32b.py,reducer32b.py\" \\\n",
    "    -mapper mapper32b.py \\\n",
    "    -reducer reducer32b.py \\\n",
    "    -input Consumer_Complaints.csv \\\n",
    "    -output recordsOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 19:17:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "      86\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/rcordell/recordsOutput/part-00000 | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** RESULTS: Mapper - 2, Reducer - 2, Unique Words - 86 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### HW3.2 Part 3\n",
    "\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.2 Part 3 - Combiner Function\n",
    "\n",
    "The combiner is essentially the same as the aggregation part of the reducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "#we added a combiner here to perform the exact same function as the reducer above, the only difference being the counter\n",
    "current_word = None\n",
    "current_count = None\n",
    "word = None\n",
    "sys.stderr.write('reporter:counter:Combiner-Counter,Total,1\\n') #increment combiner counter\n",
    "for line in sys.stdin:\n",
    "    line = line.split('\\t') #split line on tab from standard input\n",
    "    word = line[0]\n",
    "    count = int(line[1])\n",
    "    \n",
    "    if current_word == word: #increment word count\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            print '%s\\t%s' % (current_word, current_count) #print result when found a new word\n",
    "        current_word = word\n",
    "        current_count = count\n",
    "\n",
    "#print last word\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.2 Part 3 Hadoop MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 22:36:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 22:36:51 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 22:36:51 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 22:36:51 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 22:36:51 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 22:36:51 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 22:36:52 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 22:36:52 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/01 22:36:52 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1588555268_0001\n",
      "16/02/01 22:36:52 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 22:36:52 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 22:36:52 INFO mapreduce.Job: Running job: job_local1588555268_0001\n",
      "16/02/01 22:36:52 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 22:36:52 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 22:36:52 INFO mapred.LocalJobRunner: Starting task: attempt_local1588555268_0001_m_000000_0\n",
      "16/02/01 22:36:52 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 22:36:52 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 22:36:52 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 22:36:52 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 22:36:52 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/Consumer_Complaints.csv:0+50906486\n",
      "16/02/01 22:36:52 INFO mapred.MapTask: numReduceTasks: 2\n",
      "16/02/01 22:36:52 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 22:36:52 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 22:36:52 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 22:36:52 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 22:36:52 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 22:36:52 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 22:36:52 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./mapper.py]\n",
      "16/02/01 22:36:52 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 22:36:52 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 22:36:52 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 22:36:52 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 22:36:52 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 22:36:52 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 22:36:52 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 22:36:52 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 22:36:52 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 22:36:52 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 22:36:52 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 22:36:52 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 22:36:52 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:52 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:52 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:52 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:52 INFO streaming.PipeMapRed: Records R/W=1043/1\n",
      "16/02/01 22:36:52 INFO streaming.PipeMapRed: R/W/S=10000/39415/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:53 INFO mapreduce.Job: Job job_local1588555268_0001 running in uber mode : false\n",
      "16/02/01 22:36:53 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/01 22:36:54 INFO streaming.PipeMapRed: R/W/S=100000/448120/0 in:100000=100000/1 [rec/s] out:448120=448120/1 [rec/s]\n",
      "16/02/01 22:36:55 INFO streaming.PipeMapRed: R/W/S=200000/884534/0 in:100000=200000/2 [rec/s] out:442267=884534/2 [rec/s]\n",
      "16/02/01 22:36:56 INFO streaming.PipeMapRed: R/W/S=300000/1296452/0 in:100000=300000/3 [rec/s] out:432150=1296452/3 [rec/s]\n",
      "16/02/01 22:36:56 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 22:36:56 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 22:36:56 INFO mapred.LocalJobRunner: \n",
      "16/02/01 22:36:56 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 22:36:56 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 22:36:56 INFO mapred.MapTask: bufstart = 0; bufend = 13424739; bufvoid = 104857600\n",
      "16/02/01 22:36:56 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 20821152(83284608); length = 5393245/6553600\n",
      "16/02/01 22:36:57 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./combiner.py]\n",
      "16/02/01 22:36:57 INFO Configuration.deprecation: mapred.skip.map.auto.incr.proc.count is deprecated. Instead, use mapreduce.map.skip.proc-count.auto-incr\n",
      "16/02/01 22:36:57 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:57 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:57 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:57 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:57 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:57 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:57 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:57 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:57 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:57 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:57 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:57 INFO streaming.PipeMapRed: Records R/W=687448/1\n",
      "16/02/01 22:36:57 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 22:36:57 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 22:36:58 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./combiner.py]\n",
      "16/02/01 22:36:58 INFO Configuration.deprecation: mapred.skip.reduce.auto.incr.proc.count is deprecated. Instead, use mapreduce.reduce.skip.proc-count.auto-incr\n",
      "16/02/01 22:36:58 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:58 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:58 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:58 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:58 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:58 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:58 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:58 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:58 INFO mapred.LocalJobRunner: Records R/W=687448/1 > sort\n",
      "16/02/01 22:36:58 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:58 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:600000=600000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 22:36:59 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: Records R/W=660864/1\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 22:36:59 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 22:36:59 INFO mapred.Task: Task:attempt_local1588555268_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: Records R/W=660864/1\n",
      "16/02/01 22:36:59 INFO mapred.Task: Task 'attempt_local1588555268_0001_m_000000_0' done.\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: Finishing task: attempt_local1588555268_0001_m_000000_0\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: Starting task: attempt_local1588555268_0001_r_000000_0\n",
      "16/02/01 22:36:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 22:36:59 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 22:36:59 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 22:36:59 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@448f7373\n",
      "16/02/01 22:36:59 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 22:36:59 INFO reduce.EventFetcher: attempt_local1588555268_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 22:36:59 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1588555268_0001_m_000000_0 decomp: 1201 len: 1205 to MEMORY\n",
      "16/02/01 22:36:59 INFO reduce.InMemoryMapOutput: Read 1201 bytes from map-output for attempt_local1588555268_0001_m_000000_0\n",
      "16/02/01 22:36:59 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1201, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1201\n",
      "16/02/01 22:36:59 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:36:59 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 22:36:59 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 22:36:59 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1197 bytes\n",
      "16/02/01 22:36:59 INFO reduce.MergeManagerImpl: Merged 1 segments, 1201 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 22:36:59 INFO reduce.MergeManagerImpl: Merging 1 files, 1205 bytes from disk\n",
      "16/02/01 22:36:59 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 22:36:59 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 22:36:59 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1197 bytes\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./reducer.py]\n",
      "16/02/01 22:36:59 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 22:36:59 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: Records R/W=84/1\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 22:36:59 INFO mapred.Task: Task:attempt_local1588555268_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:36:59 INFO mapred.Task: Task attempt_local1588555268_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 22:36:59 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1588555268_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/word_count/_temporary/0/task_local1588555268_0001_r_000000\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: Records R/W=84/1 > reduce\n",
      "16/02/01 22:36:59 INFO mapred.Task: Task 'attempt_local1588555268_0001_r_000000_0' done.\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: Finishing task: attempt_local1588555268_0001_r_000000_0\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: Starting task: attempt_local1588555268_0001_r_000001_0\n",
      "16/02/01 22:36:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 22:36:59 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 22:36:59 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 22:36:59 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@71770ed3\n",
      "16/02/01 22:36:59 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 22:36:59 INFO reduce.EventFetcher: attempt_local1588555268_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 22:36:59 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local1588555268_0001_m_000000_0 decomp: 1335 len: 1339 to MEMORY\n",
      "16/02/01 22:36:59 INFO reduce.InMemoryMapOutput: Read 1335 bytes from map-output for attempt_local1588555268_0001_m_000000_0\n",
      "16/02/01 22:36:59 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1335, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1335\n",
      "16/02/01 22:36:59 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:36:59 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 22:36:59 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 22:36:59 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1326 bytes\n",
      "16/02/01 22:36:59 INFO reduce.MergeManagerImpl: Merged 1 segments, 1335 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 22:36:59 INFO reduce.MergeManagerImpl: Merging 1 files, 1339 bytes from disk\n",
      "16/02/01 22:36:59 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 22:36:59 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 22:36:59 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1326 bytes\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./reducer.py]\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: Records R/W=91/1\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 22:36:59 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 22:36:59 INFO mapred.Task: Task:attempt_local1588555268_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 22:36:59 INFO mapred.Task: Task attempt_local1588555268_0001_r_000001_0 is allowed to commit now\n",
      "16/02/01 22:36:59 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1588555268_0001_r_000001_0' to hdfs://localhost:9000/user/dunmireg/word_count/_temporary/0/task_local1588555268_0001_r_000001\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: Records R/W=91/1 > reduce\n",
      "16/02/01 22:36:59 INFO mapred.Task: Task 'attempt_local1588555268_0001_r_000001_0' done.\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: Finishing task: attempt_local1588555268_0001_r_000001_0\n",
      "16/02/01 22:36:59 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 22:37:00 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 22:37:00 INFO mapreduce.Job: Job job_local1588555268_0001 completed successfully\n",
      "16/02/01 22:37:00 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=328491\n",
      "\t\tFILE: Number of bytes written=1217204\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=152719458\n",
      "\t\tHDFS: Number of bytes written=3213\n",
      "\t\tHDFS: Number of read operations=24\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=1348312\n",
      "\t\tMap output bytes=13424739\n",
      "\t\tMap output materialized bytes=2544\n",
      "\t\tInput split bytes=111\n",
      "\t\tCombine input records=1348312\n",
      "\t\tCombine output records=175\n",
      "\t\tReduce input groups=175\n",
      "\t\tReduce shuffle bytes=2544\n",
      "\t\tReduce input records=175\n",
      "\t\tReduce output records=175\n",
      "\t\tSpilled Records=350\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=5\n",
      "\t\tTotal committed heap usage (bytes)=813170688\n",
      "\tCombiner-Counter\n",
      "\t\tTotal=2\n",
      "\tMap-Count\n",
      "\t\tTotal=1\n",
      "\tReduce-Counter\n",
      "\t\tTotal=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2182\n",
      "16/02/01 22:37:00 INFO streaming.StreamJob: Output directory: word_count\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.reduce.tasks=2 \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-combiner combiner.py \\\n",
    "-input Consumer_Complaints.csv \\\n",
    "-output word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "** RESULTS: Mapper - 1, Combiner - 2, Reducer - 2 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### HW3.2 Part 4\n",
    "\n",
    "Using a single reducer: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.2 Part 4 Stage 1 Map Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper-3-2-4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper-3-2-4.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "from csv import reader\n",
    "\n",
    "#Structure of complaints\n",
    "#Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,\n",
    "#Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    "line_num = 0 #for skipping header\n",
    "total = 0 #total number of words in issue\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "for line in reader(sys.stdin): #here we use csv.reader to read the input of the file \n",
    "    if line_num == 0: #skip first row, which is a header\n",
    "        line_num += 1\n",
    "        continue\n",
    "    else:\n",
    "        issue = line[3] #parse the issue of the complaint\n",
    "        if issue == '': #There are exactly four records where the issue was marked as blank. \n",
    "            issue = 'Blank' #We felt that setting to blank was appropriate\n",
    "        words = re.findall(WORD_RE, issue)\n",
    "        for word in words:\n",
    "            total += 1 #increment total word counter\n",
    "            print word.lower() + '\\t' + str(1) #print the word and a count of 1\n",
    "print '*' + '\\t' + str(total) #use order inversion to provide total as first input to reducer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.2 Part 4 Stage 1 Reduce Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer-3-2-4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer-3-2-4.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import operator\n",
    "\n",
    "current_word = None #follows same basic structure as word count we have worked with before\n",
    "current_count = None\n",
    "word = None\n",
    "total = 0\n",
    "#wordcount = {} #a dictionary to store counts. This was used in an earlier version using an in-memory mapper\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.split('\\t') #split line\n",
    "    word = line[0] #read word\n",
    "    count = int(line[1]) #get count\n",
    "    if word == '*':\n",
    "        total = count #if the word is * we know this is the total number of words and set this as a field\n",
    "    else: #otherwise continue as normal\n",
    "        if current_word == word: \n",
    "            current_count += count\n",
    "        else:\n",
    "            if current_word:\n",
    "                #wordcount[current_word] = current_count #used in in-memory dictionary version\n",
    "                #structure of result is word + count + relative count\n",
    "                print current_word + '\\t' + str(current_count) + '\\t' + str(float(current_count)/total) #print result\n",
    "            current_word = word\n",
    "            current_count = count\n",
    "\n",
    "#print last word\n",
    "if current_word == word:\n",
    "    #wordcount[current_word] = current_count\n",
    "    print current_word + '\\t' + str(current_count) + '\\t' + str(float(current_count)/total)\n",
    "    \n",
    "#Code for in-memory dictionary printing\n",
    "# largest = 50\n",
    "# smallest = 10\n",
    "# sortedWordCount = sorted(wordcount.items(), key = operator.itemgetter(1))\n",
    "\n",
    "# print \"The Top 50 terms are\"\n",
    "# for i in range(largest):\n",
    "#     print str(sortedWordCount[-i-1][0]) + '\\t' + str(sortedWordCount[-i-1][1]) + '\\t' + str(float(sortedWordCount[-i-1][1])/total)\n",
    "    \n",
    "# print '\\n'\n",
    "\n",
    "# print \"The bottom 10 terms are\"\n",
    "# for i in range(smallest):\n",
    "#     print str(sortedWordCount[i][0]) + '\\t' + str(sortedWordCount[i][1]) + '\\t' + str(float(sortedWordCount[i][1])/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.2 Part 4 Stage 2 Map Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper-3-2-4-2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper-3-2-4-2.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "#Structure is word + \\t + count + \\t + relative count, to be read from output of first set of jobs\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    line = line.split('\\t') #split the line\n",
    "    print line[1] + '\\t'+ line[0] + '\\t' + line[2] #now we are using the number as the key, this will be sorted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.2 Part 4 Stage 2 Reduce Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer-3-2-4-2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer-3-2-4-2.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "print \"Issue\" + '\\t' + \"Count\" + '\\t' + 'Relative Count'\n",
    "for line in sys.stdin: #reads result of second mapper using number as key\n",
    "    line = line.strip()\n",
    "    line = line.split('\\t')\n",
    "    #structure currently is count + word + relative count\n",
    "    print line[1] + '\\t' + line[0] + '\\t'+ line[2] #now reverse back to display word + count + relative count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!cat Consumer_Complaints.csv | python mapper-3-2-4.py | sort | python reducer-3-2-4.py | python mapper-3-2-4-2.py | sort -n | python reducer-3-2-4-2.py> output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.2 Part 4 Hadoop MapReduce Stage 1 - Word Counts and Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 23:01:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 23:01:12 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/31 23:01:12 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/31 23:01:12 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/31 23:01:13 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/31 23:01:13 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/31 23:01:13 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1964160718_0001\n",
      "16/01/31 23:01:13 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/31 23:01:13 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/31 23:01:13 INFO mapreduce.Job: Running job: job_local1964160718_0001\n",
      "16/01/31 23:01:13 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/31 23:01:13 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 23:01:13 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/31 23:01:13 INFO mapred.LocalJobRunner: Starting task: attempt_local1964160718_0001_m_000000_0\n",
      "16/01/31 23:01:13 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 23:01:13 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/31 23:01:13 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/31 23:01:13 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/Consumer_Complaints.csv:0+50906486\n",
      "16/01/31 23:01:13 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/31 23:01:14 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/31 23:01:14 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/31 23:01:14 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/31 23:01:14 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/31 23:01:14 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/31 23:01:14 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/31 23:01:14 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./mapper-3-2-4.py]\n",
      "16/01/31 23:01:14 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/31 23:01:14 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/31 23:01:14 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/31 23:01:14 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/31 23:01:14 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/31 23:01:14 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/31 23:01:14 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/31 23:01:14 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/31 23:01:14 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/31 23:01:14 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/31 23:01:14 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/31 23:01:14 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/31 23:01:14 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:14 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:14 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:14 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:14 INFO streaming.PipeMapRed: Records R/W=1513/1\n",
      "16/01/31 23:01:14 INFO streaming.PipeMapRed: R/W/S=10000/41100/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:14 INFO mapreduce.Job: Job job_local1964160718_0001 running in uber mode : false\n",
      "16/01/31 23:01:14 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/31 23:01:15 INFO streaming.PipeMapRed: R/W/S=100000/448120/0 in:100000=100000/1 [rec/s] out:448120=448120/1 [rec/s]\n",
      "16/01/31 23:01:16 INFO streaming.PipeMapRed: R/W/S=200000/884534/0 in:100000=200000/2 [rec/s] out:442267=884534/2 [rec/s]\n",
      "16/01/31 23:01:17 INFO streaming.PipeMapRed: R/W/S=300000/1296452/0 in:100000=300000/3 [rec/s] out:432150=1296452/3 [rec/s]\n",
      "16/01/31 23:01:18 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 23:01:18 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 23:01:18 INFO mapred.LocalJobRunner: \n",
      "16/01/31 23:01:18 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/31 23:01:18 INFO mapred.MapTask: Spilling map output\n",
      "16/01/31 23:01:18 INFO mapred.MapTask: bufstart = 0; bufend = 13424749; bufvoid = 104857600\n",
      "16/01/31 23:01:18 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 20821148(83284592); length = 5393249/6553600\n",
      "16/01/31 23:01:19 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/31 23:01:19 INFO mapred.Task: Task:attempt_local1964160718_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/31 23:01:19 INFO mapred.LocalJobRunner: Records R/W=1513/1\n",
      "16/01/31 23:01:19 INFO mapred.Task: Task 'attempt_local1964160718_0001_m_000000_0' done.\n",
      "16/01/31 23:01:19 INFO mapred.LocalJobRunner: Finishing task: attempt_local1964160718_0001_m_000000_0\n",
      "16/01/31 23:01:19 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/31 23:01:19 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/31 23:01:19 INFO mapred.LocalJobRunner: Starting task: attempt_local1964160718_0001_r_000000_0\n",
      "16/01/31 23:01:19 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 23:01:19 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/31 23:01:19 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/31 23:01:19 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@79e9b608\n",
      "16/01/31 23:01:19 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/31 23:01:19 INFO reduce.EventFetcher: attempt_local1964160718_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/31 23:01:19 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1964160718_0001_m_000000_0 decomp: 16121377 len: 16121381 to MEMORY\n",
      "16/01/31 23:01:19 INFO reduce.InMemoryMapOutput: Read 16121377 bytes from map-output for attempt_local1964160718_0001_m_000000_0\n",
      "16/01/31 23:01:19 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 16121377, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->16121377\n",
      "16/01/31 23:01:19 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/31 23:01:19 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 23:01:19 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/31 23:01:19 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 23:01:19 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16121373 bytes\n",
      "16/01/31 23:01:19 INFO reduce.MergeManagerImpl: Merged 1 segments, 16121377 bytes to disk to satisfy reduce memory limit\n",
      "16/01/31 23:01:19 INFO reduce.MergeManagerImpl: Merging 1 files, 16121381 bytes from disk\n",
      "16/01/31 23:01:19 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/31 23:01:19 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 23:01:19 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16121373 bytes\n",
      "16/01/31 23:01:19 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 23:01:19 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./reducer-3-2-4.py]\n",
      "16/01/31 23:01:19 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/31 23:01:19 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/31 23:01:19 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:19 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:19 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:19 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/31 23:01:19 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:19 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:20 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:20 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:20 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:20 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:20 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:20 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:20 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:700000=700000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/31 23:01:20 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:800000=800000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/31 23:01:21 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:900000=900000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/31 23:01:21 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:1000000=1000000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/31 23:01:21 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:1100000=1100000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/31 23:01:21 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:1200000=1200000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/31 23:01:21 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:1300000=1300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/31 23:01:21 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 23:01:21 INFO streaming.PipeMapRed: Records R/W=1348313/1\n",
      "16/01/31 23:01:21 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 23:01:21 INFO mapred.Task: Task:attempt_local1964160718_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/31 23:01:21 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 23:01:21 INFO mapred.Task: Task attempt_local1964160718_0001_r_000000_0 is allowed to commit now\n",
      "16/01/31 23:01:21 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1964160718_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/word_count/_temporary/0/task_local1964160718_0001_r_000000\n",
      "16/01/31 23:01:21 INFO mapred.LocalJobRunner: Records R/W=1348313/1 > reduce\n",
      "16/01/31 23:01:21 INFO mapred.Task: Task 'attempt_local1964160718_0001_r_000000_0' done.\n",
      "16/01/31 23:01:21 INFO mapred.LocalJobRunner: Finishing task: attempt_local1964160718_0001_r_000000_0\n",
      "16/01/31 23:01:21 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/31 23:01:21 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/31 23:01:21 INFO mapreduce.Job: Job job_local1964160718_0001 completed successfully\n",
      "16/01/31 23:01:21 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=32454892\n",
      "\t\tFILE: Number of bytes written=49166017\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=101812972\n",
      "\t\tHDFS: Number of bytes written=5182\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=1348313\n",
      "\t\tMap output bytes=13424749\n",
      "\t\tMap output materialized bytes=16121381\n",
      "\t\tInput split bytes=111\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=176\n",
      "\t\tReduce shuffle bytes=16121381\n",
      "\t\tReduce input records=1348313\n",
      "\t\tReduce output records=175\n",
      "\t\tSpilled Records=2696626\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=4\n",
      "\t\tTotal committed heap usage (bytes)=541065216\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5182\n",
      "16/01/31 23:01:21 INFO streaming.StreamJob: Output directory: word_count\n"
     ]
    }
   ],
   "source": [
    "#run hadoop job \n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper-3-2-4.py \\\n",
    "-reducer reducer-3-2-4.py \\\n",
    "-input Consumer_Complaints.csv \\\n",
    "-output word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.2 Part 4 Hadoop MapReduce Stage 2 - Final Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 23:01:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 23:01:45 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/31 23:01:45 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/31 23:01:45 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/31 23:01:45 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/31 23:01:45 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/01/31 23:01:46 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1842258814_0001\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/31 23:01:46 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/31 23:01:46 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 23:01:46 INFO mapreduce.Job: Running job: job_local1842258814_0001\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: Starting task: attempt_local1842258814_0001_m_000000_0\n",
      "16/01/31 23:01:46 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 23:01:46 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/31 23:01:46 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/31 23:01:46 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/word_count/part-00000:0+5182\n",
      "16/01/31 23:01:46 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/31 23:01:46 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/31 23:01:46 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/31 23:01:46 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/31 23:01:46 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/31 23:01:46 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/31 23:01:46 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/31 23:01:46 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./mapper-3-2-4-2.py]\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/31 23:01:46 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:46 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:46 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:46 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 23:01:46 INFO streaming.PipeMapRed: Records R/W=175/1\n",
      "16/01/31 23:01:46 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: \n",
      "16/01/31 23:01:46 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/31 23:01:46 INFO mapred.MapTask: Spilling map output\n",
      "16/01/31 23:01:46 INFO mapred.MapTask: bufstart = 0; bufend = 5182; bufvoid = 104857600\n",
      "16/01/31 23:01:46 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213700(104854800); length = 697/6553600\n",
      "16/01/31 23:01:46 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/31 23:01:46 INFO mapred.Task: Task:attempt_local1842258814_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: Records R/W=175/1\n",
      "16/01/31 23:01:46 INFO mapred.Task: Task 'attempt_local1842258814_0001_m_000000_0' done.\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: Finishing task: attempt_local1842258814_0001_m_000000_0\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: Starting task: attempt_local1842258814_0001_r_000000_0\n",
      "16/01/31 23:01:46 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/31 23:01:46 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/31 23:01:46 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/31 23:01:46 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@562f392a\n",
      "16/01/31 23:01:46 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/31 23:01:46 INFO reduce.EventFetcher: attempt_local1842258814_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/31 23:01:46 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1842258814_0001_m_000000_0 decomp: 5534 len: 5538 to MEMORY\n",
      "16/01/31 23:01:46 INFO reduce.InMemoryMapOutput: Read 5534 bytes from map-output for attempt_local1842258814_0001_m_000000_0\n",
      "16/01/31 23:01:46 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 5534, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->5534\n",
      "16/01/31 23:01:46 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 23:01:46 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/31 23:01:46 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 23:01:46 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 5530 bytes\n",
      "16/01/31 23:01:46 INFO reduce.MergeManagerImpl: Merged 1 segments, 5534 bytes to disk to satisfy reduce memory limit\n",
      "16/01/31 23:01:46 INFO reduce.MergeManagerImpl: Merging 1 files, 5538 bytes from disk\n",
      "16/01/31 23:01:46 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/31 23:01:46 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/31 23:01:46 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 5530 bytes\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 23:01:46 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./reducer-3-2-4-2.py]\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/31 23:01:46 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/31 23:01:46 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:46 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:46 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/31 23:01:46 INFO streaming.PipeMapRed: Records R/W=175/1\n",
      "16/01/31 23:01:46 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/31 23:01:46 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/31 23:01:46 INFO mapred.Task: Task:attempt_local1842258814_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/31 23:01:46 INFO mapred.Task: Task attempt_local1842258814_0001_r_000000_0 is allowed to commit now\n",
      "16/01/31 23:01:46 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1842258814_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/sortedWordCount/_temporary/0/task_local1842258814_0001_r_000000\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: Records R/W=175/1 > reduce\n",
      "16/01/31 23:01:46 INFO mapred.Task: Task 'attempt_local1842258814_0001_r_000000_0' done.\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: Finishing task: attempt_local1842258814_0001_r_000000_0\n",
      "16/01/31 23:01:46 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/31 23:01:47 INFO mapreduce.Job: Job job_local1842258814_0001 running in uber mode : false\n",
      "16/01/31 23:01:47 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/31 23:01:47 INFO mapreduce.Job: Job job_local1842258814_0001 completed successfully\n",
      "16/01/31 23:01:47 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=223198\n",
      "\t\tFILE: Number of bytes written=820648\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=10364\n",
      "\t\tHDFS: Number of bytes written=5209\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=175\n",
      "\t\tMap output records=175\n",
      "\t\tMap output bytes=5182\n",
      "\t\tMap output materialized bytes=5538\n",
      "\t\tInput split bytes=109\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=116\n",
      "\t\tReduce shuffle bytes=5538\n",
      "\t\tReduce input records=175\n",
      "\t\tReduce output records=176\n",
      "\t\tSpilled Records=350\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=5\n",
      "\t\tTotal committed heap usage (bytes)=511705088\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5182\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5209\n",
      "16/01/31 23:01:47 INFO streaming.StreamJob: Output directory: sortedWordCount\n"
     ]
    }
   ],
   "source": [
    "#run second job to properly sort by counts\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D  mapred.text.key.comparator.options=\"-k1nr -k2\" \\\n",
    "-mapper mapper-3-2-4-2.py \\\n",
    "-reducer reducer-3-2-4-2.py \\\n",
    "-input /user/dunmireg/word_count/part-00000 \\\n",
    "-output sortedWordCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.2 Part 4 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show results, display top 50 words\n",
    "!hdfs dfs -cat /user/dunmireg/sortedWordCount/part-00000 | head -50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16/02/02 23:03:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "\n",
    "loan\t119630\t0.0887257548698\n",
    "\n",
    "collection\t72394\t0.0536923204718\n",
    "\n",
    "foreclosure\t70487\t0.0522779594041\n",
    "\n",
    "modification\t70487\t0.0522779594041\n",
    "\n",
    "account\t57448\t0.0426073490409\n",
    "\n",
    "credit\t55251\t0.0409779042239\n",
    "\n",
    "or\t40508\t0.0300434914174\n",
    "\n",
    "payments\t39993\t0.0296615323456\n",
    "\n",
    "escrow\t36767\t0.0272689110532\n",
    "\n",
    "servicing\t36767\t0.0272689110532\n",
    "\n",
    "report\t34903\t0.0258864417138\n",
    "\n",
    "incorrect\t29133\t0.0216070167736\n",
    "\n",
    "information\t29069\t0.0215595500151\n",
    "\n",
    "on\t29069\t0.0215595500151\n",
    "\n",
    "debt\t27874\t0.020673256635\n",
    "\n",
    "closing\t19000\t0.0140916939106\n",
    "\n",
    "not\t18477\t0.013703801494\n",
    "\n",
    "attempts\t17972\t0.0133292591032\n",
    "\n",
    "collect\t17972\t0.0133292591032\n",
    "\n",
    "cont'd\t17972\t0.0133292591032\n",
    "\n",
    "owed\t17972\t0.0133292591032\n",
    "\n",
    "and\t16448\t0.012198956918\n",
    "\n",
    "management\t16205\t0.0120187315695\n",
    "\n",
    "opening\t16205\t0.0120187315695\n",
    "\n",
    "of\t13983\t0.0103707450501\n",
    "\n",
    "my\t10731\t0.00795884038709\n",
    "\n",
    "deposits\t10555\t0.00782830680139\n",
    "\n",
    "withdrawals\t10555\t0.00782830680139\n",
    "\n",
    "problems\t9484\t0.0070339802657\n",
    "\n",
    "application\t8868\t0.00657711271575\n",
    "\n",
    "communication\t8671\t0.00643100409994\n",
    "\n",
    "tactics\t8671\t0.00643100409994\n",
    "\n",
    "broker\t8625\t0.00639688736732\n",
    "\n",
    "mortgage\t8625\t0.00639688736732\n",
    "\n",
    "originator\t8625\t0.00639688736732\n",
    "\n",
    "to\t8401\t0.00623075371279\n",
    "\n",
    "unable\t8178\t0.00606536172637\n",
    "\n",
    "billing\t8158\t0.00605052836435\n",
    "\n",
    "other\t7886\t0.005848794641\n",
    "\n",
    "disclosure\t7655\t0.00567746930977\n",
    "\n",
    "verification\t7655\t0.00567746930977\n",
    "\n",
    "disputes\t6938\t0.00514569328167\n",
    "\n",
    "reporting\t6559\t0.00486460107156\n",
    "\n",
    "lease\t6337\t0.00469995075324\n",
    "\n",
    "the\t6248\t0.00463394229229\n",
    "\n",
    "being\t5663\t0.00420006645346\n",
    "\n",
    "by\t5663\t0.00420006645346\n",
    "\n",
    "caused\t5663\t0.00420006645346\n",
    "\n",
    "funds\t5663\t0.00420006645346\n",
    "\n",
    "low\t5663\t0.00420006645346"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Display bottom 10 counts (least is on top)\n",
    "!hdfs dfs -cat /user/dunmireg/sortedWordCount/part-00000 | tail | sort -k2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16/02/02 23:06:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "\n",
    "blank\t4\t2.96667240223e-06\n",
    "\n",
    "disclosures\t64\t4.74667584357e-05\n",
    "\n",
    "missing\t64\t4.74667584357e-05\n",
    "\n",
    "amt\t71\t5.26584351396e-05\n",
    "\n",
    "day\t71\t5.26584351396e-05\n",
    "\n",
    "checks\t75\t5.56251075419e-05\n",
    "\n",
    "convenience\t75\t5.56251075419e-05\n",
    "\n",
    "credited\t92\t6.82334652514e-05\n",
    "\n",
    "payment\t92\t6.82334652514e-05\n",
    "\n",
    "amount\t98\t7.26834738547e-05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.3. Shopping Cart Analysis\n",
    "Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "\n",
    "For this homework use the online browsing behavior dataset located at: \n",
    "\n",
    "       https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "Each line in this dataset represents a browsing session of a customer. \n",
    "On each line, each string of 8 characters represents the id of an item browsed during that session. \n",
    "The items are separated by spaces.\n",
    "\n",
    "Here are the first few lines of the ProductPurchaseData \n",
    "\n",
    "    FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \n",
    "    GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \n",
    "    ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 \n",
    "    ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 \n",
    "    ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444 \n",
    "\n",
    "\n",
    "Do some exploratory data analysis of this dataset. \n",
    "\n",
    "How many unique items are available from this supplier?\n",
    "\n",
    "Using a single reducer: Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.3 Mapper\n",
    "\n",
    "Below is the mapper used to accomplish this task. It emits each token along with a basket size to the reducers for additional processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapperQ33.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapperQ33.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "def readInput(file, separator=None):\n",
    "\tfor line in file:\n",
    "\t\tyield line.split(separator)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "\t# Read input \n",
    "\tdata = readInput(sys.stdin)\n",
    "\tfor line in data: \n",
    "\t\tholdingDict = defaultdict(int)\n",
    "\t\tbasketSize = len(line)\n",
    "\n",
    "\n",
    "\t\t# Append elements \n",
    "\t\tfor token in line: \n",
    "\t\t\tholdingDict[token] += 1\n",
    "\n",
    "\n",
    "\t\t# Emit results \n",
    "\t\tfor k, v in holdingDict.iteritems(): \n",
    "\t\t\tbasketInfo = str([v, basketSize])\n",
    "\t\t\tprint '%s%s%s' % (k, '\\t', basketInfo)\n",
    "            \n",
    "\t# Update counter \n",
    "\tsys.stderr.write(\"reporter:counter:User-Defined,Number of Mappers,1\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.3 Reducer\n",
    "\n",
    "Below is the reducer used to accomplish this task. The reducer uses a `defaultdict` object from `collections` to automatically collate tokens and their counts without having to explicitly instantiate the key. This is convenient when reading from `sys.stdin` as one can yield lines and store the tokens simultaneously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducerQ33.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducerQ33.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from __future__ import division \n",
    "import sys \n",
    "from collections import defaultdict\n",
    "import ast \n",
    "\n",
    "def readInput(file, separator='\\t'):\n",
    "\tfor line in file:\n",
    "\t\tyield line.split(separator)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\t# Final store \n",
    "\tstoringDict = defaultdict(int)\n",
    "\tmaxBasket = 0\n",
    "\ttotalTerms = 0\n",
    "\n",
    "\n",
    "\t# Read data \n",
    "\tdata = readInput(sys.stdin)\n",
    "\tfor line in data: \n",
    "\n",
    "\n",
    "\t\t# Parse value \n",
    "\t\ttoken = line[0]\n",
    "\t\ttermCount, basketSize = ast.literal_eval(line[1])\n",
    "\t\ttotalTerms += termCount\n",
    "\n",
    "\n",
    "\t\t# Store results \n",
    "\t\tstoringDict[token] += termCount\n",
    "\t\tmaxBasket = max(maxBasket, basketSize)\n",
    "\n",
    "\n",
    "\t# Metrics\n",
    "\tnumUniqueProducts = len(set(storingDict.keys()))\n",
    "\tlargestBasket = maxBasket\n",
    "\n",
    "\n",
    "\t# Compute frequencies \n",
    "\tfor k, v in storingDict.iteritems():\n",
    "\t\tstoringDict[k] = v\n",
    "\n",
    "\t\n",
    "\t# Find most frequent terms \n",
    "\tmostFrequentTerms = [(k, v, round(v / totalTerms, 4)) for k, v in storingDict.iteritems()]\n",
    "\tmostFrequentTerms = sorted(mostFrequentTerms, \n",
    "\t\t\t\t\t\t\t\tkey = lambda x: x[1], \n",
    "\t\t\t\t\t\t\t\treverse = True)\n",
    "\n",
    "\n",
    "\t# Get results \n",
    "\tprint '\\n' + '========== Number of Unique Products ==========' + '\\n'\n",
    "\tprint 'Answer: ' + str(numUniqueProducts) + '\\n'\n",
    "    \n",
    "\tprint '\\n' + '========= Largest Basket ==========' + '\\n'\n",
    "\tprint 'Answer: ' + str(largestBasket) + '\\n'    \n",
    "\n",
    "\tprint '========== Most Frequent Terms ==========' + '\\n'\n",
    "\ttemplate = \"{0:20}|{1:20}|{2:20}\"\n",
    "\tprint template.format(\"ITEM\", \"FREQUENCY\", \"RELATIVE FREQUENCY\")\n",
    "\t\n",
    "\t# Print terms \n",
    "\tfor termPair in mostFrequentTerms[:50]: \n",
    "\t\tprint template.format(*termPair)\n",
    "        \n",
    "\t# Update counter \n",
    "\tsys.stderr.write(\"reporter:counter:User-Defined,Number of Reducers,1\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.3 Wrapper\n",
    "\n",
    "Below is the bash script that's used to submit the Hadoop Streaming job. It maps user input to variables and specifies the options for the job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wrapperQ33.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile wrapperQ33.sh\n",
    "#!/bin/bash\n",
    "\n",
    "\n",
    "# Initialize\n",
    "RAW_DATA=$1\n",
    "RAW_MAPPER=$2\n",
    "RAW_REDUCER=$3\n",
    "\n",
    "\n",
    "# Hadoop variables \n",
    "HDFS_DIR=\"/user/john/notebook\"\n",
    "HDFS_INPUT=\"$HDFS_DIR/input\"\n",
    "HDFS_OUTPUT=\"$HDFS_DIR/output\"\n",
    "HDFS_FILES=\"$HDFS_DIR/files\"\n",
    "\n",
    "\n",
    "# Local variables \n",
    "PROJECT_DIR=\"/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook\"\n",
    "DATA=\"$PROJECT_DIR/$RAW_DATA\"\n",
    "MAPPER=\"$PROJECT_DIR/$RAW_MAPPER\"\n",
    "REDUCER=\"$PROJECT_DIR/$RAW_REDUCER\"\n",
    "\n",
    "\n",
    "# NAIVE=\"$PROJECT_DIR/naiveBayes.py\"\n",
    "STREAMING_JAR=\"$PROJECT_DIR/hadoop-streaming-2.6.0.jar\"\n",
    "\n",
    "\n",
    "# Make directories and put file \n",
    "hdfs dfs -rm -r $HDFS_DIR\n",
    "hdfs dfs -mkdir $HDFS_DIR $HDFS_INPUT $HDFS_FILES\n",
    "hdfs dfs -put $DATA $HDFS_INPUT\n",
    "\n",
    "\n",
    "# Execute\n",
    "hadoop jar $STREAMING_JAR \\\n",
    "\t-file \"$MAPPER\" -mapper \"$MAPPER\" \\\n",
    "\t-file \"$REDUCER\" -reducer \"$REDUCER\" \\\n",
    "\t-input $HDFS_INPUT \\\n",
    "\t-output $HDFS_OUTPUT \n",
    "\n",
    "\n",
    "# Output results  \n",
    "if [ $? -eq 0 ]; then \n",
    "\thdfs dfs -cat $HDFS_OUTPUT/part-00000\n",
    "fi\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.3 Hadoop MapReduce Submit Job\n",
    "\n",
    "Below we submit the job. The wrapper takes as arguments the data, mapper, and reducer. Later versions of the wrapper also allow combiner arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 16:05:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 16:05:42 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/john/notebook\n",
      "16/01/30 16:05:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 16:05:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 16:05:49 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/30 16:05:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/mapperQ33.py, /Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/reducerQ33.py] [] /var/folders/0w/8hzv7rsj3qgdynsjlqy3gjsc0000gn/T/streamjob3210653833237786513.jar tmpDir=null\n",
      "16/01/30 16:05:50 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/30 16:05:50 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/30 16:05:50 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/30 16:05:51 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/30 16:05:51 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/30 16:05:51 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1059638235_0001\n",
      "16/01/30 16:05:52 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/mapperQ33.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1454187951883/mapperQ33.py\n",
      "16/01/30 16:05:52 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/reducerQ33.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1454187951884/reducerQ33.py\n",
      "16/01/30 16:05:52 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/30 16:05:52 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/30 16:05:52 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 16:05:52 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/30 16:05:52 INFO mapreduce.Job: Running job: job_local1059638235_0001\n",
      "16/01/30 16:05:52 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/30 16:05:52 INFO mapred.LocalJobRunner: Starting task: attempt_local1059638235_0001_m_000000_0\n",
      "16/01/30 16:05:52 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 16:05:52 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 16:05:52 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 16:05:52 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/john/notebook/input/ProductPurchaseData.txt:0+3458517\n",
      "16/01/30 16:05:52 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/30 16:05:52 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/30 16:05:52 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/30 16:05:52 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/30 16:05:52 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/30 16:05:52 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/30 16:05:52 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/30 16:05:53 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./mapperQ33.py]\n",
      "16/01/30 16:05:53 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/30 16:05:53 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/30 16:05:53 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/30 16:05:53 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/30 16:05:53 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/30 16:05:53 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/30 16:05:53 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/30 16:05:53 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/30 16:05:53 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/30 16:05:53 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/30 16:05:53 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/30 16:05:53 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/30 16:05:53 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:05:53 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:05:53 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:05:53 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:05:53 INFO streaming.PipeMapRed: Records R/W=1216/1\n",
      "16/01/30 16:05:53 INFO mapreduce.Job: Job job_local1059638235_0001 running in uber mode : false\n",
      "16/01/30 16:05:53 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/30 16:05:54 INFO streaming.PipeMapRed: R/W/S=10000/119481/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:05:55 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 16:05:55 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 16:05:55 INFO mapred.LocalJobRunner: \n",
      "16/01/30 16:05:55 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/30 16:05:55 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 16:05:55 INFO mapred.MapTask: bufstart = 0; bufend = 6397909; bufvoid = 104857600\n",
      "16/01/30 16:05:55 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24691116(98764464); length = 1523281/6553600\n",
      "16/01/30 16:05:56 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/30 16:05:56 INFO mapred.Task: Task:attempt_local1059638235_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/30 16:05:56 INFO mapred.LocalJobRunner: Records R/W=1216/1\n",
      "16/01/30 16:05:56 INFO mapred.Task: Task 'attempt_local1059638235_0001_m_000000_0' done.\n",
      "16/01/30 16:05:56 INFO mapred.LocalJobRunner: Finishing task: attempt_local1059638235_0001_m_000000_0\n",
      "16/01/30 16:05:56 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/30 16:05:56 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/30 16:05:56 INFO mapred.LocalJobRunner: Starting task: attempt_local1059638235_0001_r_000000_0\n",
      "16/01/30 16:05:56 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 16:05:56 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 16:05:56 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 16:05:56 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@70e26d9a\n",
      "16/01/30 16:05:56 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/30 16:05:56 INFO reduce.EventFetcher: attempt_local1059638235_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/30 16:05:56 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1059638235_0001_m_000000_0 decomp: 7159553 len: 7159557 to MEMORY\n",
      "16/01/30 16:05:56 INFO reduce.InMemoryMapOutput: Read 7159553 bytes from map-output for attempt_local1059638235_0001_m_000000_0\n",
      "16/01/30 16:05:56 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 7159553, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->7159553\n",
      "16/01/30 16:05:56 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/30 16:05:56 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 16:05:56 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/30 16:05:56 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/30 16:05:56 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 7159542 bytes\n",
      "16/01/30 16:05:57 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/30 16:05:57 INFO reduce.MergeManagerImpl: Merged 1 segments, 7159553 bytes to disk to satisfy reduce memory limit\n",
      "16/01/30 16:05:57 INFO reduce.MergeManagerImpl: Merging 1 files, 7159557 bytes from disk\n",
      "16/01/30 16:05:57 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/30 16:05:57 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/30 16:05:57 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 7159542 bytes\n",
      "16/01/30 16:05:57 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 16:05:57 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./reducerQ33.py]\n",
      "16/01/30 16:05:57 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/30 16:05:57 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/30 16:05:57 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:05:57 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:05:57 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:05:57 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:05:57 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:06:00 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:33333=100000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 16:06:02 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 16:06:03 INFO mapreduce.Job:  map 100% reduce 82%\n",
      "16/01/30 16:06:03 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:40000=200000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 16:06:05 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 16:06:06 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:37500=300000/8 [rec/s] out:0=0/8 [rec/s]\n",
      "16/01/30 16:06:06 INFO mapreduce.Job:  map 100% reduce 91%\n",
      "16/01/30 16:06:08 INFO streaming.PipeMapRed: Records R/W=380821/1\n",
      "16/01/30 16:06:08 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 16:06:08 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 16:06:08 INFO mapred.LocalJobRunner: Records R/W=380821/1 > reduce\n",
      "16/01/30 16:06:08 INFO mapred.Task: Task:attempt_local1059638235_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/30 16:06:08 INFO mapred.LocalJobRunner: Records R/W=380821/1 > reduce\n",
      "16/01/30 16:06:08 INFO mapred.Task: Task attempt_local1059638235_0001_r_000000_0 is allowed to commit now\n",
      "16/01/30 16:06:08 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1059638235_0001_r_000000_0' to hdfs://localhost:9000/user/john/notebook/output/_temporary/0/task_local1059638235_0001_r_000000\n",
      "16/01/30 16:06:08 INFO mapred.LocalJobRunner: Records R/W=380821/1 > reduce\n",
      "16/01/30 16:06:08 INFO mapred.Task: Task 'attempt_local1059638235_0001_r_000000_0' done.\n",
      "16/01/30 16:06:08 INFO mapred.LocalJobRunner: Finishing task: attempt_local1059638235_0001_r_000000_0\n",
      "16/01/30 16:06:08 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/30 16:06:09 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/30 16:06:09 INFO mapreduce.Job: Job job_local1059638235_0001 completed successfully\n",
      "16/01/30 16:06:09 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=14326178\n",
      "\t\tFILE: Number of bytes written=22073423\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6917034\n",
      "\t\tHDFS: Number of bytes written=3434\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=380821\n",
      "\t\tMap output bytes=6397909\n",
      "\t\tMap output materialized bytes=7159557\n",
      "\t\tInput split bytes=122\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12592\n",
      "\t\tReduce shuffle bytes=7159557\n",
      "\t\tReduce input records=380821\n",
      "\t\tReduce output records=63\n",
      "\t\tSpilled Records=761642\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=500170752\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3434\n",
      "16/01/30 16:06:09 INFO streaming.StreamJob: Output directory: /user/john/notebook/output\n",
      "16/01/30 16:06:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\t\n",
      "========== Number of Unique Products ==========\t\n",
      "\t\n",
      "Answer: 12592\t\n",
      "\t\n",
      "\t\n",
      "========= Largest Basket ==========\t\n",
      "\t\n",
      "Answer: 37\t\n",
      "\t\n",
      "========== Most Frequent Terms ==========\t\n",
      "\t\n",
      "ITEM                |FREQUENCY           |RELATIVE FREQUENCY  \t\n",
      "DAI62779            |                6667|              0.0175\t\n",
      "FRO40251            |                3881|              0.0102\t\n",
      "ELE17451            |                3875|              0.0102\t\n",
      "GRO73461            |                3602|              0.0095\t\n",
      "SNA80324            |                3044|               0.008\t\n",
      "ELE32164            |                2851|              0.0075\t\n",
      "DAI75645            |                2736|              0.0072\t\n",
      "SNA45677            |                2455|              0.0064\t\n",
      "FRO31317            |                2330|              0.0061\t\n",
      "DAI85309            |                2293|               0.006\t\n",
      "ELE26917            |                2292|               0.006\t\n",
      "FRO80039            |                2233|              0.0059\t\n",
      "GRO21487            |                2115|              0.0056\t\n",
      "SNA99873            |                2083|              0.0055\t\n",
      "GRO59710            |                2004|              0.0053\t\n",
      "GRO71621            |                1920|               0.005\t\n",
      "FRO85978            |                1918|               0.005\t\n",
      "GRO30386            |                1840|              0.0048\t\n",
      "ELE74009            |                1816|              0.0048\t\n",
      "GRO56726            |                1784|              0.0047\t\n",
      "DAI63921            |                1773|              0.0047\t\n",
      "GRO46854            |                1756|              0.0046\t\n",
      "ELE66600            |                1713|              0.0045\t\n",
      "DAI83733            |                1712|              0.0045\t\n",
      "FRO32293            |                1702|              0.0045\t\n",
      "ELE66810            |                1697|              0.0045\t\n",
      "SNA55762            |                1646|              0.0043\t\n",
      "DAI22177            |                1627|              0.0043\t\n",
      "FRO78087            |                1531|               0.004\t\n",
      "ELE99737            |                1516|               0.004\t\n",
      "ELE34057            |                1489|              0.0039\t\n",
      "GRO94758            |                1489|              0.0039\t\n",
      "FRO35904            |                1436|              0.0038\t\n",
      "FRO53271            |                1420|              0.0037\t\n",
      "SNA93860            |                1407|              0.0037\t\n",
      "SNA90094            |                1390|              0.0036\t\n",
      "GRO38814            |                1352|              0.0036\t\n",
      "ELE56788            |                1345|              0.0035\t\n",
      "GRO61133            |                1321|              0.0035\t\n",
      "ELE74482            |                1316|              0.0035\t\n",
      "DAI88807            |                1316|              0.0035\t\n",
      "ELE59935            |                1311|              0.0034\t\n",
      "SNA96271            |                1295|              0.0034\t\n",
      "DAI43223            |                1290|              0.0034\t\n",
      "ELE91337            |                1289|              0.0034\t\n",
      "GRO15017            |                1275|              0.0033\t\n",
      "DAI31081            |                1261|              0.0033\t\n",
      "GRO81087            |                1220|              0.0032\t\n",
      "DAI22896            |                1219|              0.0032\t\n",
      "GRO85051            |                1214|              0.0032\t\n"
     ]
    }
   ],
   "source": [
    "!bash wrapperQ33.sh ProductPurchaseData.txt mapperQ33.py reducerQ33.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## HW3.4\n",
    "\n",
    "*Write a map-reduce program to find products which are frequently browsed together. Fix the support count (cooccurence count) to s = 100 \n",
    "(i.e. product pairs need to occur together at least 100 times to be considered frequent) \n",
    "and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more.*\n",
    "\n",
    "*List the top 50 product pairs with corresponding support count (aka frequency), and relative frequency or support (number of records where they coccur, the number of records where they coccur/the number of baskets in the dataset)  in decreasing order of support  for frequent (100>count) itemsets of size 2. *\n",
    "\n",
    "*Use the Pairs pattern (lecture 3)  to  extract these frequent itemsets of size 2. Free free to use combiners if they bring value. Instrument your code with counters for count the number of times your mapper, combiner and reducers are called.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Approach A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper\n",
    "\n",
    "Below is the mapper code for the pairs implementation. It generates tuple-combinations from the input lines, sorts them to get unique keys, then emits them to the reducer or optional combiner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapperQ34.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapperQ34.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from __future__ import division \n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "def readInput(file, separator=None):\n",
    "\tfor line in file:\n",
    "\t\tyield line.split(separator)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\t# Read input \n",
    "\tdata = readInput(sys.stdin)\n",
    "\ttotalBaskets = 0\n",
    "\n",
    "\tfor line in data: \n",
    "\n",
    "\t\t# Increment\n",
    "\t\ttotalBaskets += 1\n",
    "\n",
    "\t\t# Get unique keys \n",
    "\t\tpairs = list(combinations(line, 2))\n",
    "\t\t\n",
    "\n",
    "\t\t# Sort keys \n",
    "\t\tsortedPairs = []\n",
    "\t\tfor pair in pairs: \n",
    "\t\t\tpList = list(pair)\n",
    "\t\t\tpList.sort()\n",
    "\t\t\tsortedPairs.append(tuple(pList))\n",
    "\n",
    "\t\t# Emit\n",
    "\t\tfor pair in sortedPairs: \n",
    "\t\t\tprint '%s%s%s' % (pair, '\\t', 1)\n",
    "\t\t\t\n",
    "\t\t\n",
    "\t# Update counter \n",
    "\tsys.stderr.write(\"reporter:counter:User-Defined,Number of Mappers,1\\n\")\n",
    "\n",
    "\t# Emit basket count \n",
    "\tprint '%s%s%s' % ('*', '\\t', str(totalBaskets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.4 Reducer\n",
    "\n",
    "Below is the reducer for this process. It uses a simple key-aggregation to collate the results from the mappers, where the keys are unique tuples from the lines read in by the mapper. The uniqueness condition is specified at the line-level when read by the mapper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducerQ34.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducerQ34.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from __future__ import division \n",
    "import sys \n",
    "from collections import defaultdict\n",
    "import ast \n",
    "\n",
    "\n",
    "def readInput(file, separator='\\t'):\n",
    "\tfor line in file:\n",
    "\t\tyield line.split(separator)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\t# Final store \n",
    "\tstoringDict = defaultdict(int)\n",
    "\tsupport = 100\n",
    "\ttotalBaskets = 0\n",
    "\n",
    "\n",
    "\t# Read data \n",
    "\tdata = readInput(sys.stdin)\n",
    "\tfor line in data: \n",
    "\n",
    "\t\t# Check for basket \n",
    "\t\ttoken = line[0]\n",
    "\n",
    "\t\tif token == '*': \n",
    "\t\t\ttotalBaskets += int(line[1])\n",
    "\n",
    "\t\telse:\n",
    "\n",
    "\t\t\t# Parse \n",
    "\t\t\ttermCount = int(line[1])\n",
    "\n",
    "\t\t\t# Store results \n",
    "\t\t\tstoringDict[token] += termCount\n",
    "\n",
    "\t\n",
    "\t# Filter\n",
    "\tfilterDict = defaultdict(int)\n",
    "\tfor k, v in storingDict.iteritems(): \n",
    "\t\tif v >= support:\n",
    "\t\t\tfilterDict[k] += v \n",
    "\n",
    "\n",
    "\t# Find most frequent terms \n",
    "\tmostFrequentTerms = [(k, v, round(int(v) / totalBaskets, 3)) for k, v in filterDict.iteritems()]\n",
    "\tmostFrequentTerms = sorted(mostFrequentTerms, \n",
    "\t\t\t\t\t\t\t\tkey = lambda x: x[1], \n",
    "\t\t\t\t\t\t\t\treverse = True)\n",
    "\n",
    "\n",
    "\t# Get results \n",
    "\tprint '\\n' + '========== Most Frequent Terms ==========' + '\\n'\n",
    "\ttemplate = \"{0:30}|{1:20}|{2:20}\"\n",
    "\tprint template.format(\"PAIR\", \"SUPPORT COUNT\", \"RELATIVE FREQUENCY\")\n",
    "\t\n",
    "\t# Print terms \n",
    "\tfor termPair in mostFrequentTerms[:50]: \n",
    "\t\tprint template.format(*termPair)\n",
    "        \n",
    "\t# Update counter \n",
    "\tsys.stderr.write(\"reporter:counter:User-Defined,Number of Reducers,1\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.4 Combiner (Optional)\n",
    "\n",
    "Below is the code for an optional combiner. It has the same signature as the reducer and does the same essential aggregation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combinerQ34.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combinerQ34.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from __future__ import division \n",
    "import sys \n",
    "from collections import defaultdict\n",
    "import ast \n",
    "\n",
    "\n",
    "def readInput(file, separator='\\t'):\n",
    "\tfor line in file:\n",
    "\t\tyield line.split(separator)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\t# Final store \n",
    "\tstoringDict = defaultdict(int)\n",
    "\n",
    "\n",
    "\t# Read data \n",
    "\tdata = readInput(sys.stdin)\n",
    "\ttotalBaskets = 0\n",
    "\n",
    "\t\n",
    "\tfor line in data: \n",
    "\n",
    "\t\t# Check for basket \n",
    "\t\ttoken = line[0]\n",
    "\n",
    "\t\tif token == '*': \n",
    "\t\t\ttotalBaskets += int(line[1])\n",
    "\n",
    "\t\telse:\n",
    "\n",
    "\t\t\t# Parse \n",
    "\t\t\ttermCount = int(line[1])\n",
    "\n",
    "\t\t\t# Store results \n",
    "\t\t\tstoringDict[token] += termCount\n",
    "\n",
    "\n",
    "\t# Emit\n",
    "\tfor k, v in storingDict.iteritems(): \n",
    "\t\tprint '%s%s%s' % (k, '\\t', v)\n",
    "        \n",
    "\t# Update counter \n",
    "\tsys.stderr.write(\"reporter:counter:User-Defined,Number of Combiners,1\\n\")\n",
    "\n",
    "\t# Emit basket count \n",
    "\tprint '%s%s%s' % ('*', '\\t', str(totalBaskets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.4 Wrapper\n",
    "\n",
    "Here is the wrapper for this particular submission. The noticeable difference is that it now has been modified to pass a combiner file to Hadoop Streaming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wrapperQ34.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile wrapperQ34.sh\n",
    "#!/bin/bash\n",
    "\n",
    "\n",
    "# Initialize\n",
    "RAW_DATA=$1\n",
    "RAW_MAPPER=$2\n",
    "RAW_REDUCER=$3\n",
    "RAW_COMBINER=$4\n",
    "\n",
    "\n",
    "# Hadoop variables \n",
    "HDFS_DIR=\"/user/john/notebook\"\n",
    "HDFS_INPUT=\"$HDFS_DIR/input\"\n",
    "HDFS_OUTPUT=\"$HDFS_DIR/output\"\n",
    "HDFS_FILES=\"$HDFS_DIR/files\"\n",
    "\n",
    "\n",
    "# Local variables \n",
    "PROJECT_DIR=\"/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook\"\n",
    "DATA=\"$PROJECT_DIR/$RAW_DATA\"\n",
    "MAPPER=\"$PROJECT_DIR/$RAW_MAPPER\"\n",
    "REDUCER=\"$PROJECT_DIR/$RAW_REDUCER\"\n",
    "COMBINER=\"$PROJECT_DIR/$RAW_COMBINER\"\n",
    "\n",
    "\n",
    "# NAIVE=\"$PROJECT_DIR/naiveBayes.py\"\n",
    "STREAMING_JAR=\"$PROJECT_DIR/hadoop-streaming-2.6.0.jar\"\n",
    "\n",
    "\n",
    "# Make directories and put file \n",
    "hdfs dfs -rm -r $HDFS_DIR\n",
    "hdfs dfs -mkdir $HDFS_DIR $HDFS_INPUT $HDFS_FILES\n",
    "hdfs dfs -put $DATA $HDFS_INPUT\n",
    "\n",
    "\n",
    "# Execute\n",
    "hadoop jar $STREAMING_JAR \\\n",
    "\t-file \"$MAPPER\" -mapper \"$MAPPER\" \\\n",
    "\t-file \"$REDUCER\" -reducer \"$REDUCER\" \\\n",
    "    -file \"$COMBINER\" -combiner \"$COMBINER\" \\\n",
    "\t-input $HDFS_INPUT \\\n",
    "\t-output $HDFS_OUTPUT \\\n",
    "\t-cmdenv mapred.map.max.attempts=1 \\\n",
    "\t-cmdenv mapred.reduce.max.attempts=1 \\\n",
    "\n",
    "\n",
    "# Output results  \n",
    "if [ $? -eq 0 ]; then \n",
    "\thdfs dfs -cat $HDFS_OUTPUT/part-00000\n",
    "fi\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Submit Job\n",
    "\n",
    "We now submit the job for this process. Notice that an argument has been added for the combiner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 19:56:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 19:56:53 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/john/notebook\n",
      "16/02/01 19:56:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 19:56:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 19:57:01 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/01 19:57:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/mapperQ34.py, /Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/reducerQ34.py, /Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/combinerQ34.py] [] /var/folders/0w/8hzv7rsj3qgdynsjlqy3gjsc0000gn/T/streamjob6486516089282375945.jar tmpDir=null\n",
      "16/02/01 19:57:03 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 19:57:03 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 19:57:04 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 19:57:05 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 19:57:05 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 19:57:05 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local887715370_0001\n",
      "16/02/01 19:57:06 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/mapperQ34.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1454374626230/mapperQ34.py\n",
      "16/02/01 19:57:06 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/reducerQ34.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1454374626231/reducerQ34.py\n",
      "16/02/01 19:57:06 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/combinerQ34.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1454374626232/combinerQ34.py\n",
      "16/02/01 19:57:07 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 19:57:07 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 19:57:07 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 19:57:07 INFO mapreduce.Job: Running job: job_local887715370_0001\n",
      "16/02/01 19:57:07 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 19:57:07 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 19:57:07 INFO mapred.LocalJobRunner: Starting task: attempt_local887715370_0001_m_000000_0\n",
      "16/02/01 19:57:07 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 19:57:07 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 19:57:07 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 19:57:07 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/john/notebook/input/ProductPurchaseData.txt:0+3458517\n",
      "16/02/01 19:57:07 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/01 19:57:07 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 19:57:07 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 19:57:07 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 19:57:07 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 19:57:07 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 19:57:07 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 19:57:07 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./mapperQ34.py]\n",
      "16/02/01 19:57:07 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 19:57:07 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 19:57:07 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 19:57:07 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 19:57:07 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 19:57:07 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 19:57:07 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 19:57:07 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 19:57:07 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 19:57:07 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 19:57:07 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 19:57:07 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 19:57:07 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:57:07 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:57:07 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:57:07 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:57:07 INFO streaming.PipeMapRed: Records R/W=1216/1\n",
      "16/02/01 19:57:08 INFO mapreduce.Job: Job job_local887715370_0001 running in uber mode : false\n",
      "16/02/01 19:57:08 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/01 19:57:12 INFO streaming.PipeMapRed: R/W/S=10000/854395/0 in:2000=10000/5 [rec/s] out:170879=854395/5 [rec/s]\n",
      "16/02/01 19:57:13 INFO mapred.LocalJobRunner: Records R/W=1216/1 > map\n",
      "16/02/01 19:57:14 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "16/02/01 19:57:16 INFO mapred.LocalJobRunner: Records R/W=1216/1 > map\n",
      "16/02/01 19:57:17 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "16/02/01 19:57:17 INFO streaming.PipeMapRed: Records R/W=17860/1356232\n",
      "16/02/01 19:57:19 INFO mapred.LocalJobRunner: Records R/W=17860/1356232 > map\n",
      "16/02/01 19:57:20 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "16/02/01 19:57:22 INFO mapred.LocalJobRunner: Records R/W=17860/1356232 > map\n",
      "16/02/01 19:57:22 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 19:57:22 INFO mapred.MapTask: bufstart = 0; bufend = 52672653; bufvoid = 104857600\n",
      "16/02/01 19:57:22 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 18411044(73644176); length = 7803353/6553600\n",
      "16/02/01 19:57:22 INFO mapred.MapTask: (EQUATOR) 60475997 kvi 15118992(60475968)\n",
      "16/02/01 19:57:23 INFO mapreduce.Job:  map 53% reduce 0%\n",
      "16/02/01 19:57:25 INFO mapred.LocalJobRunner: Records R/W=17860/1356232 > map\n",
      "16/02/01 19:57:26 INFO mapreduce.Job:  map 61% reduce 0%\n",
      "16/02/01 19:57:28 INFO mapred.LocalJobRunner: Records R/W=17860/1356232 > map\n",
      "16/02/01 19:57:29 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/02/01 19:57:30 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./combinerQ34.py]\n",
      "16/02/01 19:57:30 INFO Configuration.deprecation: mapred.skip.map.auto.incr.proc.count is deprecated. Instead, use mapreduce.map.skip.proc-count.auto-incr\n",
      "16/02/01 19:57:30 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:57:30 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:57:30 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:57:30 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:57:31 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:57:31 INFO mapred.LocalJobRunner: Records R/W=17860/1356232 > map\n",
      "16/02/01 19:57:31 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:57:32 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:200000=200000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 19:57:33 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:150000=300000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/01 19:57:33 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:200000=400000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/01 19:57:34 INFO mapred.LocalJobRunner: Records R/W=17860/1356232 > map\n",
      "16/02/01 19:57:34 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:166666=500000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/02/01 19:57:34 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:200000=600000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/02/01 19:57:35 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:175000=700000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/02/01 19:57:35 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:160000=800000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/02/01 19:57:36 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:180000=900000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/02/01 19:57:37 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:166666=1000000/6 [rec/s] out:0=0/6 [rec/s]\n",
      "16/02/01 19:57:37 INFO mapred.LocalJobRunner: Records R/W=17860/1356232 > map\n",
      "16/02/01 19:57:37 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:157142=1100000/7 [rec/s] out:0=0/7 [rec/s]\n",
      "16/02/01 19:57:38 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:171428=1200000/7 [rec/s] out:0=0/7 [rec/s]\n",
      "16/02/01 19:57:38 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:185714=1300000/7 [rec/s] out:0=0/7 [rec/s]\n",
      "16/02/01 19:57:39 INFO streaming.PipeMapRed: R/W/S=1400000/0/0 in:175000=1400000/8 [rec/s] out:0=0/8 [rec/s]\n",
      "16/02/01 19:57:40 INFO streaming.PipeMapRed: R/W/S=1500000/0/0 in:166666=1500000/9 [rec/s] out:0=0/9 [rec/s]\n",
      "16/02/01 19:57:40 INFO mapred.LocalJobRunner: Records R/W=17860/1356232 > map\n",
      "16/02/01 19:57:40 INFO streaming.PipeMapRed: R/W/S=1600000/0/0 in:160000=1600000/10 [rec/s] out:0=0/10 [rec/s]\n",
      "16/02/01 19:57:41 INFO streaming.PipeMapRed: R/W/S=1700000/0/0 in:170000=1700000/10 [rec/s] out:0=0/10 [rec/s]\n",
      "16/02/01 19:57:42 INFO streaming.PipeMapRed: R/W/S=1800000/0/0 in:163636=1800000/11 [rec/s] out:0=0/11 [rec/s]\n",
      "16/02/01 19:57:42 INFO streaming.PipeMapRed: R/W/S=1900000/0/0 in:172727=1900000/11 [rec/s] out:0=0/11 [rec/s]\n",
      "16/02/01 19:57:42 INFO streaming.PipeMapRed: Records R/W=1950839/1\n",
      "16/02/01 19:57:43 INFO mapred.LocalJobRunner: Records R/W=1950839/1 > map\n",
      "16/02/01 19:57:45 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 19:57:45 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 19:57:45 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 19:57:45 INFO mapred.MapTask: (RESET) equator 60475997 kv 15118992(60475968) kvi 13168168(52672672)\n",
      "16/02/01 19:57:45 INFO streaming.PipeMapRed: Records R/W=31101/2438546\n",
      "16/02/01 19:57:45 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 19:57:45 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 19:57:45 INFO mapred.LocalJobRunner: Records R/W=1950839/1 > map\n",
      "16/02/01 19:57:45 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 19:57:45 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 19:57:45 INFO mapred.MapTask: bufstart = 60475997; bufend = 76222891; bufvoid = 104857600\n",
      "16/02/01 19:57:45 INFO mapred.MapTask: kvstart = 15118992(60475968); kvend = 12786120(51144480); length = 2332873/6553600\n",
      "16/02/01 19:57:46 INFO mapred.LocalJobRunner: Records R/W=31101/2438546 > sort\n",
      "16/02/01 19:57:47 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./combinerQ34.py]\n",
      "16/02/01 19:57:47 INFO Configuration.deprecation: mapred.skip.reduce.auto.incr.proc.count is deprecated. Instead, use mapreduce.reduce.skip.proc-count.auto-incr\n",
      "16/02/01 19:57:47 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:57:47 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:57:47 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:57:47 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:57:47 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:57:47 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:57:48 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:200000=200000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 19:57:48 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 19:57:49 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 19:57:49 INFO mapred.LocalJobRunner: Records R/W=31101/2438546 > sort\n",
      "16/02/01 19:57:49 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:250000=500000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/01 19:57:50 INFO streaming.PipeMapRed: Records R/W=583219/1\n",
      "16/02/01 19:57:51 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 19:57:51 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 19:57:51 INFO mapred.MapTask: Finished spill 1\n",
      "16/02/01 19:57:51 INFO mapred.Merger: Merging 2 sorted segments\n",
      "16/02/01 19:57:51 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 28603089 bytes\n",
      "16/02/01 19:57:52 INFO mapred.LocalJobRunner: Records R/W=583219/1 > sort > \n",
      "16/02/01 19:57:53 INFO mapreduce.Job:  map 71% reduce 0%\n",
      "16/02/01 19:57:55 INFO mapred.LocalJobRunner: Records R/W=583219/1 > sort > \n",
      "16/02/01 19:57:55 INFO mapred.Task: Task:attempt_local887715370_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 19:57:55 INFO mapred.LocalJobRunner: Records R/W=583219/1 > sort\n",
      "16/02/01 19:57:55 INFO mapred.Task: Task 'attempt_local887715370_0001_m_000000_0' done.\n",
      "16/02/01 19:57:55 INFO mapred.LocalJobRunner: Finishing task: attempt_local887715370_0001_m_000000_0\n",
      "16/02/01 19:57:55 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 19:57:55 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 19:57:55 INFO mapred.LocalJobRunner: Starting task: attempt_local887715370_0001_r_000000_0\n",
      "16/02/01 19:57:55 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 19:57:55 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 19:57:55 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 19:57:55 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@31ba89b4\n",
      "16/02/01 19:57:55 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 19:57:55 INFO reduce.EventFetcher: attempt_local887715370_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 19:57:56 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local887715370_0001_m_000000_0 decomp: 28603133 len: 28603137 to MEMORY\n",
      "16/02/01 19:57:56 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/01 19:57:56 INFO reduce.InMemoryMapOutput: Read 28603133 bytes from map-output for attempt_local887715370_0001_m_000000_0\n",
      "16/02/01 19:57:56 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 28603133, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->28603133\n",
      "16/02/01 19:57:56 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 19:57:56 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 19:57:56 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 19:57:56 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 19:57:56 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 28603106 bytes\n",
      "16/02/01 19:57:58 INFO reduce.MergeManagerImpl: Merged 1 segments, 28603133 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 19:57:58 INFO reduce.MergeManagerImpl: Merging 1 files, 28603137 bytes from disk\n",
      "16/02/01 19:57:58 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 19:57:58 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 19:57:58 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 28603106 bytes\n",
      "16/02/01 19:57:58 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 19:57:58 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./reducerQ34.py]\n",
      "16/02/01 19:57:58 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 19:57:58 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 19:57:59 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:57:59 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:57:59 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:57:59 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:57:59 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:58:00 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:100000=100000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 19:58:01 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:100000=200000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/01 19:58:01 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:150000=300000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/01 19:58:01 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 19:58:02 INFO mapreduce.Job:  map 100% reduce 78%\n",
      "16/02/01 19:58:02 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:133333=400000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/02/01 19:58:03 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:125000=500000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/02/01 19:58:03 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:150000=600000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/02/01 19:58:04 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:140000=700000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/02/01 19:58:04 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 19:58:05 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:133333=800000/6 [rec/s] out:0=0/6 [rec/s]\n",
      "16/02/01 19:58:05 INFO mapreduce.Job:  map 100% reduce 93%\n",
      "16/02/01 19:58:05 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:150000=900000/6 [rec/s] out:0=0/6 [rec/s]\n",
      "16/02/01 19:58:07 INFO streaming.PipeMapRed: Records R/W=985039/1\n",
      "16/02/01 19:58:07 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 19:58:07 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 19:58:07 INFO mapred.Task: Task:attempt_local887715370_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 19:58:07 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 19:58:07 INFO mapred.Task: Task attempt_local887715370_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 19:58:07 INFO output.FileOutputCommitter: Saved output of task 'attempt_local887715370_0001_r_000000_0' to hdfs://localhost:9000/user/john/notebook/output/_temporary/0/task_local887715370_0001_r_000000\n",
      "16/02/01 19:58:07 INFO mapred.LocalJobRunner: Records R/W=985039/1 > reduce\n",
      "16/02/01 19:58:07 INFO mapred.Task: Task 'attempt_local887715370_0001_r_000000_0' done.\n",
      "16/02/01 19:58:07 INFO mapred.LocalJobRunner: Finishing task: attempt_local887715370_0001_r_000000_0\n",
      "16/02/01 19:58:07 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 19:58:08 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 19:58:08 INFO mapreduce.Job: Job job_local887715370_0001 completed successfully\n",
      "16/02/01 19:58:08 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=114422786\n",
      "\t\tFILE: Number of bytes written=143613365\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6917034\n",
      "\t\tHDFS: Number of bytes written=3821\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=2534058\n",
      "\t\tMap output bytes=68419547\n",
      "\t\tMap output materialized bytes=28603137\n",
      "\t\tInput split bytes=122\n",
      "\t\tCombine input records=2534058\n",
      "\t\tCombine output records=985039\n",
      "\t\tReduce input groups=985038\n",
      "\t\tReduce shuffle bytes=28603137\n",
      "\t\tReduce input records=985039\n",
      "\t\tReduce output records=54\n",
      "\t\tSpilled Records=2955117\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=20\n",
      "\t\tTotal committed heap usage (bytes)=526385152\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tUser-Defined\n",
      "\t\tNumber of Combiners=2\n",
      "\t\tNumber of Mappers=1\n",
      "\t\tNumber of Reducers=1\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3821\n",
      "16/02/01 19:58:08 INFO streaming.StreamJob: Output directory: /user/john/notebook/output\n",
      "16/02/01 19:58:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\t\n",
      "========== Most Frequent Terms ==========\t\n",
      "\t\n",
      "PAIR                          |SUPPORT COUNT       |RELATIVE FREQUENCY  \t\n",
      "('DAI62779', 'ELE17451')      |                1592|               0.051\t\n",
      "('FRO40251', 'SNA80324')      |                1412|               0.045\t\n",
      "('DAI75645', 'FRO40251')      |                1254|                0.04\t\n",
      "('FRO40251', 'GRO85051')      |                1213|               0.039\t\n",
      "('DAI62779', 'GRO73461')      |                1139|               0.037\t\n",
      "('DAI75645', 'SNA80324')      |                1130|               0.036\t\n",
      "('DAI62779', 'FRO40251')      |                1070|               0.034\t\n",
      "('DAI62779', 'SNA80324')      |                 923|                0.03\t\n",
      "('DAI62779', 'DAI85309')      |                 918|                0.03\t\n",
      "('ELE32164', 'GRO59710')      |                 911|               0.029\t\n",
      "('FRO40251', 'GRO73461')      |                 882|               0.028\t\n",
      "('DAI62779', 'DAI75645')      |                 882|               0.028\t\n",
      "('DAI62779', 'ELE92920')      |                 877|               0.028\t\n",
      "('FRO40251', 'FRO92469')      |                 835|               0.027\t\n",
      "('DAI62779', 'ELE32164')      |                 832|               0.027\t\n",
      "('DAI75645', 'GRO73461')      |                 712|               0.023\t\n",
      "('DAI43223', 'ELE32164')      |                 711|               0.023\t\n",
      "('DAI62779', 'GRO30386')      |                 709|               0.023\t\n",
      "('ELE17451', 'FRO40251')      |                 697|               0.022\t\n",
      "('DAI85309', 'ELE99737')      |                 659|               0.021\t\n",
      "('DAI62779', 'ELE26917')      |                 650|               0.021\t\n",
      "('GRO21487', 'GRO73461')      |                 631|                0.02\t\n",
      "('DAI62779', 'SNA45677')      |                 604|               0.019\t\n",
      "('ELE17451', 'SNA80324')      |                 597|               0.019\t\n",
      "('DAI62779', 'GRO71621')      |                 595|               0.019\t\n",
      "('DAI62779', 'SNA55762')      |                 593|               0.019\t\n",
      "('DAI62779', 'DAI83733')      |                 586|               0.019\t\n",
      "('ELE17451', 'GRO73461')      |                 580|               0.019\t\n",
      "('GRO73461', 'SNA80324')      |                 562|               0.018\t\n",
      "('DAI62779', 'GRO59710')      |                 561|               0.018\t\n",
      "('DAI62779', 'FRO80039')      |                 550|               0.018\t\n",
      "('DAI75645', 'ELE17451')      |                 547|               0.018\t\n",
      "('DAI62779', 'SNA93860')      |                 537|               0.017\t\n",
      "('DAI55148', 'DAI62779')      |                 526|               0.017\t\n",
      "('DAI43223', 'GRO59710')      |                 512|               0.016\t\n",
      "('ELE17451', 'ELE32164')      |                 511|               0.016\t\n",
      "('DAI62779', 'SNA18336')      |                 506|               0.016\t\n",
      "('ELE32164', 'GRO73461')      |                 486|               0.016\t\n",
      "('DAI85309', 'ELE17451')      |                 482|               0.015\t\n",
      "('DAI62779', 'FRO78087')      |                 482|               0.015\t\n",
      "('DAI62779', 'GRO94758')      |                 479|               0.015\t\n",
      "('GRO85051', 'SNA80324')      |                 471|               0.015\t\n",
      "('DAI62779', 'GRO21487')      |                 471|               0.015\t\n",
      "('ELE17451', 'GRO30386')      |                 468|               0.015\t\n",
      "('FRO85978', 'SNA95666')      |                 463|               0.015\t\n",
      "('DAI62779', 'FRO19221')      |                 462|               0.015\t\n",
      "('DAI62779', 'GRO46854')      |                 461|               0.015\t\n",
      "('DAI43223', 'DAI62779')      |                 459|               0.015\t\n",
      "('ELE92920', 'SNA18336')      |                 455|               0.015\t\n",
      "('DAI88079', 'FRO40251')      |                 446|               0.014\t\n"
     ]
    }
   ],
   "source": [
    "!bash wrapperQ34.sh ProductPurchaseData.txt mapperQ34.py reducerQ34.py combinerQ34.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the counters, check the 'User-Defined' category in the job logs appearing just prior to the reducer results. It should appear as follows: \n",
    "\n",
    "\tUser-Defined\n",
    "\t\tNumber of Combiners=2\n",
    "\t\tNumber of Mappers=1\n",
    "\t\tNumber of Reducers=1\n",
    "\n",
    "The job in total took 1 minute and 16 seconds to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.4 Solution Approach B\n",
    "\n",
    "### Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper-3-4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper-3-4.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "from itertools import combinations\n",
    "\n",
    "totalBaskets = 0 #field to hold total number of baskets\n",
    "sys.stderr.write('reporter:counter:Mapper-Counter,Total,1\\n') #increment mapper counter\n",
    "for line in sys.stdin:\n",
    "    totalBaskets += 1 #increment\n",
    "    line = line.strip()\n",
    "    line = line.split()\n",
    "    \n",
    "    pairs = list(combinations(line, 2)) #this give all pair combinations for all items in a basket\n",
    "    for pair in pairs:\n",
    "        pair = sorted(list(pair)) #sort the pairs in lexicographic order\n",
    "        print pair[0] + ' ' + pair[1] + '\\t' + str(1) #print result: item1 + item2 + count of 1\n",
    "print '*' + '\\t' + str(totalBaskets) #print total baskets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer-3-4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer-3-4.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "\n",
    "support = 100 #level of support\n",
    "totalBaskets = 0 #total basket\n",
    "pairs = defaultdict(int) #in-memory dictionary to hold reults. This wouldn't work as a scalable solution\n",
    "\n",
    "sys.stderr.write('reporter:counter:Reducer-Counter,Total,1\\n') #increment counter\n",
    "for line in sys.stdin:\n",
    "    line = line.split('\\t')\n",
    "    if line[0] == '*': #order inversion says this will be the total \n",
    "        totalBaskets = int(line[1])\n",
    "    else:\n",
    "        pairs[line[0]] += int(line[1]) #increment the default dictionary for pair by counter. \n",
    "        #note when using a regular dictionary this takes an extremely long time. \n",
    "\n",
    "freqDict = {}\n",
    "for pair, count in pairs.iteritems(): #filter dictionary for only items with support greater than level set\n",
    "    if count > support: \n",
    "        freqDict[pair] = count\n",
    "\n",
    "print \"Top 50 item pairs:\"\n",
    "print '\\n'\n",
    "print 'Item Pair' + '\\t' + 'Support Count' + '\\t' + 'Relative Support Count'\n",
    "print '\\n'\n",
    "        \n",
    "sortedFreqDict = sorted(freqDict.items(), key = lambda x: (-x[1], x[0])) #sort results by number and by lexicographic order\n",
    "for i in range(50):\n",
    "    print sortedFreqDict[i][0] + '\\t' + str(sortedFreqDict[i][1]) + '\\t' + str(float(sortedFreqDict[i][1])/totalBaskets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 17:56:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 17:56:36 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 17:56:36 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 17:56:36 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 17:56:37 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 17:56:37 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 17:56:37 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local350127535_0001\n",
      "16/02/01 17:56:37 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 17:56:37 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 17:56:37 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 17:56:37 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 17:56:37 INFO mapreduce.Job: Running job: job_local350127535_0001\n",
      "16/02/01 17:56:37 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 17:56:37 INFO mapred.LocalJobRunner: Starting task: attempt_local350127535_0001_m_000000_0\n",
      "16/02/01 17:56:37 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 17:56:37 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 17:56:37 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 17:56:37 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/ProductPurchaseData.txt:0+3458517\n",
      "16/02/01 17:56:37 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/01 17:56:37 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 17:56:37 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 17:56:37 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 17:56:37 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 17:56:37 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 17:56:37 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 17:56:37 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./mapper-3-4.py]\n",
      "16/02/01 17:56:37 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 17:56:37 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 17:56:37 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 17:56:37 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 17:56:37 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 17:56:37 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 17:56:37 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 17:56:37 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 17:56:37 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 17:56:37 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 17:56:37 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 17:56:37 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 17:56:37 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:56:37 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:56:37 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:56:37 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:56:37 INFO streaming.PipeMapRed: Records R/W=1216/1\n",
      "16/02/01 17:56:38 INFO mapreduce.Job: Job job_local350127535_0001 running in uber mode : false\n",
      "16/02/01 17:56:38 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/01 17:56:39 INFO streaming.PipeMapRed: R/W/S=10000/854425/0 in:10000=10000/1 [rec/s] out:854425=854425/1 [rec/s]\n",
      "16/02/01 17:56:41 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 17:56:41 INFO mapred.MapTask: bufstart = 0; bufend = 46603380; bufvoid = 104857600\n",
      "16/02/01 17:56:41 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 16893724(67574896); length = 9320673/6553600\n",
      "16/02/01 17:56:41 INFO mapred.MapTask: (EQUATOR) 55924036 kvi 13981004(55924016)\n",
      "16/02/01 17:56:42 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 17:56:42 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 17:56:42 INFO mapred.LocalJobRunner: \n",
      "16/02/01 17:56:42 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 17:56:43 INFO mapred.LocalJobRunner: Records R/W=1216/1 > sort\n",
      "16/02/01 17:56:44 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/02/01 17:56:46 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 17:56:46 INFO mapred.MapTask: (RESET) equator 55924036 kv 13981004(55924016) kvi 13165448(52661792)\n",
      "16/02/01 17:56:46 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 17:56:46 INFO mapred.MapTask: bufstart = 55924036; bufend = 60001804; bufvoid = 104857600\n",
      "16/02/01 17:56:46 INFO mapred.MapTask: kvstart = 13981004(55924016); kvend = 13165452(52661808); length = 815553/6553600\n",
      "16/02/01 17:56:46 INFO mapred.MapTask: Finished spill 1\n",
      "16/02/01 17:56:46 INFO mapred.Merger: Merging 2 sorted segments\n",
      "16/02/01 17:56:46 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 55749252 bytes\n",
      "16/02/01 17:56:46 INFO mapred.LocalJobRunner: Records R/W=1216/1 > sort > \n",
      "16/02/01 17:56:47 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "16/02/01 17:56:47 INFO mapred.Task: Task:attempt_local350127535_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 17:56:47 INFO mapred.LocalJobRunner: Records R/W=1216/1 > sort\n",
      "16/02/01 17:56:47 INFO mapred.Task: Task 'attempt_local350127535_0001_m_000000_0' done.\n",
      "16/02/01 17:56:47 INFO mapred.LocalJobRunner: Finishing task: attempt_local350127535_0001_m_000000_0\n",
      "16/02/01 17:56:47 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 17:56:47 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 17:56:47 INFO mapred.LocalJobRunner: Starting task: attempt_local350127535_0001_r_000000_0\n",
      "16/02/01 17:56:47 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 17:56:47 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 17:56:47 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 17:56:47 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1a05e6dd\n",
      "16/02/01 17:56:47 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 17:56:47 INFO reduce.EventFetcher: attempt_local350127535_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 17:56:47 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local350127535_0001_m_000000_0 decomp: 55749266 len: 55749270 to MEMORY\n",
      "16/02/01 17:56:47 INFO reduce.InMemoryMapOutput: Read 55749266 bytes from map-output for attempt_local350127535_0001_m_000000_0\n",
      "16/02/01 17:56:47 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 55749266, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->55749266\n",
      "16/02/01 17:56:47 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 17:56:47 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 17:56:47 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 17:56:47 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 17:56:47 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 55749262 bytes\n",
      "16/02/01 17:56:48 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/01 17:56:48 INFO reduce.MergeManagerImpl: Merged 1 segments, 55749266 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 17:56:48 INFO reduce.MergeManagerImpl: Merging 1 files, 55749270 bytes from disk\n",
      "16/02/01 17:56:48 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 17:56:48 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 17:56:48 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 55749262 bytes\n",
      "16/02/01 17:56:48 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 17:56:48 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./reducer-3-4.py]\n",
      "16/02/01 17:56:48 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 17:56:48 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 17:56:48 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:56:48 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:56:48 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:56:48 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:56:48 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:56:49 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:56:49 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:56:49 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:56:49 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:56:49 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:56:49 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:600000=600000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 17:56:50 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:700000=700000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 17:56:50 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:800000=800000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 17:56:50 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:900000=900000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 17:56:50 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:1000000=1000000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 17:56:50 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:1100000=1100000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 17:56:50 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:600000=1200000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/01 17:56:51 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:650000=1300000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/01 17:56:51 INFO streaming.PipeMapRed: R/W/S=1400000/0/0 in:700000=1400000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/01 17:56:51 INFO streaming.PipeMapRed: R/W/S=1500000/0/0 in:750000=1500000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/01 17:56:51 INFO streaming.PipeMapRed: R/W/S=1600000/0/0 in:800000=1600000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/01 17:56:51 INFO streaming.PipeMapRed: R/W/S=1700000/0/0 in:850000=1700000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/01 17:56:51 INFO streaming.PipeMapRed: R/W/S=1800000/0/0 in:600000=1800000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/02/01 17:56:52 INFO streaming.PipeMapRed: R/W/S=1900000/0/0 in:633333=1900000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/02/01 17:56:52 INFO streaming.PipeMapRed: R/W/S=2000000/0/0 in:666666=2000000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/02/01 17:56:52 INFO streaming.PipeMapRed: R/W/S=2100000/0/0 in:700000=2100000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/02/01 17:56:52 INFO streaming.PipeMapRed: R/W/S=2200000/0/0 in:733333=2200000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/02/01 17:56:52 INFO streaming.PipeMapRed: R/W/S=2300000/0/0 in:766666=2300000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/02/01 17:56:53 INFO streaming.PipeMapRed: R/W/S=2400000/0/0 in:600000=2400000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/02/01 17:56:53 INFO streaming.PipeMapRed: R/W/S=2500000/0/0 in:625000=2500000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/02/01 17:56:53 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 17:56:54 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 17:56:54 INFO streaming.PipeMapRed: Records R/W=2534058/1\n",
      "16/02/01 17:56:54 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 17:56:54 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 17:56:55 INFO mapred.Task: Task:attempt_local350127535_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 17:56:55 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 17:56:55 INFO mapred.Task: Task attempt_local350127535_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 17:56:55 INFO output.FileOutputCommitter: Saved output of task 'attempt_local350127535_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/sortedProducts/_temporary/0/task_local350127535_0001_r_000000\n",
      "16/02/01 17:56:55 INFO mapred.LocalJobRunner: Records R/W=2534058/1 > reduce\n",
      "16/02/01 17:56:55 INFO mapred.Task: Task 'attempt_local350127535_0001_r_000000_0' done.\n",
      "16/02/01 17:56:55 INFO mapred.LocalJobRunner: Finishing task: attempt_local350127535_0001_r_000000_0\n",
      "16/02/01 17:56:55 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 17:56:55 INFO mapreduce.Job: Job job_local350127535_0001 completed successfully\n",
      "16/02/01 17:56:55 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=223209220\n",
      "\t\tFILE: Number of bytes written=279545226\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6917034\n",
      "\t\tHDFS: Number of bytes written=1973\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=2534058\n",
      "\t\tMap output bytes=50681148\n",
      "\t\tMap output materialized bytes=55749270\n",
      "\t\tInput split bytes=111\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=877099\n",
      "\t\tReduce shuffle bytes=55749270\n",
      "\t\tReduce input records=2534058\n",
      "\t\tReduce output records=56\n",
      "\t\tSpilled Records=7602174\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=6\n",
      "\t\tTotal committed heap usage (bytes)=620756992\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1973\n",
      "16/02/01 17:56:55 INFO streaming.StreamJob: Output directory: sortedProducts\n",
      "CPU times: user 107 ms, sys: 28.8 ms, total: 136 ms\n",
      "Wall time: 20.6 s\n"
     ]
    }
   ],
   "source": [
    "time !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper-3-4.py \\\n",
    "-reducer reducer-3-4.py \\\n",
    "-input ProductPurchaseData.txt \\\n",
    "-output sortedProducts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 17:57:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Top 50 item pairs:\t\n",
      "\t\n",
      "\t\n",
      "Item Pair\tSupport Count\tRelative Support Count\n",
      "\t\n",
      "\t\n",
      "DAI62779 ELE17451\t1592\t0.0511880646925\n",
      "FRO40251 SNA80324\t1412\t0.0454004694383\n",
      "DAI75645 FRO40251\t1254\t0.0403202469374\n",
      "FRO40251 GRO85051\t1213\t0.0390019613517\n",
      "DAI62779 GRO73461\t1139\t0.0366226166361\n",
      "DAI75645 SNA80324\t1130\t0.0363332368734\n",
      "DAI62779 FRO40251\t1070\t0.0344040384554\n",
      "DAI62779 SNA80324\t923\t0.0296775023311\n",
      "DAI62779 DAI85309\t918\t0.0295167357963\n",
      "ELE32164 GRO59710\t911\t0.0292916626475\n",
      "DAI62779 DAI75645\t882\t0.0283592167454\n",
      "FRO40251 GRO73461\t882\t0.0283592167454\n",
      "DAI62779 ELE92920\t877\t0.0281984502106\n",
      "FRO40251 FRO92469\t835\t0.026848011318\n",
      "DAI62779 ELE32164\t832\t0.0267515513971\n",
      "DAI75645 GRO73461\t712\t0.0228931545609\n",
      "DAI43223 ELE32164\t711\t0.022861001254\n",
      "DAI62779 GRO30386\t709\t0.02279669464\n",
      "ELE17451 FRO40251\t697\t0.0224108549564\n",
      "DAI85309 ELE99737\t659\t0.0211890292917\n",
      "DAI62779 ELE26917\t650\t0.020899649529\n",
      "GRO21487 GRO73461\t631\t0.0202887366966\n",
      "DAI62779 SNA45677\t604\t0.0194205974084\n",
      "ELE17451 SNA80324\t597\t0.0191955242597\n",
      "DAI62779 GRO71621\t595\t0.0191312176457\n",
      "DAI62779 SNA55762\t593\t0.0190669110318\n",
      "DAI62779 DAI83733\t586\t0.018841837883\n",
      "ELE17451 GRO73461\t580\t0.0186489180412\n",
      "GRO73461 SNA80324\t562\t0.0180701585158\n",
      "DAI62779 GRO59710\t561\t0.0180380052088\n",
      "DAI62779 FRO80039\t550\t0.0176843188322\n",
      "DAI75645 ELE17451\t547\t0.0175878589113\n",
      "DAI62779 SNA93860\t537\t0.0172663258416\n",
      "DAI55148 DAI62779\t526\t0.016912639465\n",
      "DAI43223 GRO59710\t512\t0.0164624931674\n",
      "ELE17451 ELE32164\t511\t0.0164303398605\n",
      "DAI62779 SNA18336\t506\t0.0162695733256\n",
      "ELE32164 GRO73461\t486\t0.0156265071863\n",
      "DAI62779 FRO78087\t482\t0.0154978939584\n",
      "DAI85309 ELE17451\t482\t0.0154978939584\n",
      "DAI62779 GRO94758\t479\t0.0154014340375\n",
      "DAI62779 GRO21487\t471\t0.0151442075817\n",
      "GRO85051 SNA80324\t471\t0.0151442075817\n",
      "ELE17451 GRO30386\t468\t0.0150477476608\n",
      "FRO85978 SNA95666\t463\t0.014886981126\n",
      "DAI62779 FRO19221\t462\t0.014854827819\n",
      "DAI62779 GRO46854\t461\t0.0148226745121\n",
      "DAI43223 DAI62779\t459\t0.0147583678981\n",
      "ELE92920 SNA18336\t455\t0.0146297546703\n",
      "DAI88079 FRO40251\t446\t0.0143403749076\n"
     ]
    }
   ],
   "source": [
    "#show results\n",
    "!hdfs dfs -cat /user/dunmireg/sortedProducts/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total time taken is displayed here: \n",
    "\n",
    "CPU times: user 112 ms, sys: 28.3 ms, total: 140 ms\n",
    "Wall time: 21.4 s\n",
    "\n",
    "This code used 1 mapper and 1 reducer. This was run on an 8 GB Macbook Air with a 2.2 GHz Intel Core i7 quad core processor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## HW3.5\n",
    "*Repeat 3.4 using the stripes design pattern for finding cooccuring pairs.*\n",
    "\n",
    "*Report  the compute times for stripes job versus the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)*\n",
    "\n",
    "*Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts. Discuss the differences in these counts between the Pairs and Stripes jobs*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Approach A: \n",
    "\n",
    "### Mapper\n",
    "\n",
    "The mapper for the stripes implementation is noticeably different than the one used for the pairs implementation. In particular, a dictionary stripe is emitted which is then parsed literally by the reducer or optional combiner. From here, the stripes are aggregated per token then divided by two to compensate for the two combinations in which a key can be updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapperQ35.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapperQ35.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "def readInput(file, separator=None):\n",
    "\tfor line in file:\n",
    "\t\tyield line.split(separator)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\t# Read input \n",
    "\tdata = readInput(sys.stdin)\n",
    "\ttotalBaskets = 0\n",
    "    \n",
    "\tfor line in data: \n",
    "\t\t\n",
    "\t\ttotalBaskets += 1\n",
    "\t\t\n",
    "\t\tfor token in line: \n",
    "\n",
    "\t\t\toccurrence = defaultdict(int) \n",
    "\n",
    "\t\t\t# Remove token from neighbors\n",
    "\t\t\tstripe = [x for x in line if x != token]\n",
    "\t\t\t\n",
    "\t\t\t# Create co-occurrence array \n",
    "\t\t\tfor neighbor in stripe: \n",
    "\t\t\t\toccurrence[neighbor] += 1\n",
    "\n",
    "\t\t\t# Emit\n",
    "\t\t\tcArray = dict(occurrence)\n",
    "\n",
    "\t\t\tprint '%s%s%s' % (token, '\\t', str(cArray))\n",
    "\n",
    "\t# Update counter \n",
    "\tsys.stderr.write(\"reporter:counter:User-Defined,Number of Mappers,1\\n\")\n",
    "\n",
    "\t# Emit basket count\n",
    "\tprint '%s%s%s' % ('*', '\\t', str(totalBaskets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer\n",
    "\n",
    "Here, the reducer has been changed to compensate for the adjusted input. The reducer aggregates the stripes by aggregating the incoming dictionaries. It does this efficiently using `defaultdict`, an automatically instantiating key-value dictionary included in `collections`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducerQ35.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducerQ35.py \n",
    "#!/usr/bin/env python\n",
    "\n",
    "from __future__ import division \n",
    "import sys \n",
    "from collections import defaultdict\n",
    "from itertools import combinations, chain \n",
    "import ast \n",
    "\n",
    "\n",
    "# Read input from mapper \n",
    "def readInput(file, separator='\\t'):\n",
    "\tfor line in file:\n",
    "\t\tyield line.split(separator)\t\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\ttokenDict = defaultdict(list)\n",
    "\tmergedDict = defaultdict(dict)\n",
    "\ttupleList = defaultdict(float)\n",
    "\tflattenedTerms = defaultdict(int)\n",
    "\tsupport = 100\n",
    "\ttotalBaskets = 0\n",
    "\n",
    "\n",
    "\t# Read data \n",
    "\tdata = readInput(sys.stdin)\n",
    "\tfor line in data: \n",
    "\n",
    "\t\t\n",
    "\t\ttoken = line[0]\n",
    "\t\tif token == '*': \n",
    "\n",
    "\t\t\t# Increment basket \n",
    "\t\t\ttotalBaskets += int(line[1])\t\n",
    "\n",
    "\t\telse:\n",
    "\n",
    "\t\t\t# Parse normally \n",
    "\t\t\tstripe = ast.literal_eval(line[1])\n",
    "\t\t\t\t\n",
    "\t\t\t# Combine dictionaries \n",
    "\t\t\ttokenDict[token].append(stripe) \n",
    "\n",
    "\n",
    "\t# Merge stripes \n",
    "\tfor k, v in tokenDict.iteritems(): \n",
    "\t\tmerged = defaultdict(int)\n",
    "\t\t\n",
    "\t\t# Loop and aggregate \n",
    "\t\tfor stripe in v: \n",
    "\t\t\tfor stripeKey in stripe: \n",
    "\t\t\t\tmerged[stripeKey] += stripe[stripeKey]\n",
    "\n",
    "\t\tmergedDict[k] = merged\n",
    "\n",
    "\n",
    "\t# Create key-value pairs \n",
    "\tfor token, stripe in mergedDict.iteritems(): \n",
    "\t\tfor innerToken, count in stripe.iteritems(): \n",
    "\n",
    "\t\t\t# Get unique keys \n",
    "\t\t\ttokenPair = [token, innerToken]\n",
    "\t\t\ttokenPair.sort()\n",
    "\t\t\ttuplePair = tuple(tokenPair)\n",
    "\n",
    "\t\t\t# Overcounting exactly twice per pair\n",
    "\t\t\ttupleList[tuplePair] += count / 2\n",
    "\t\n",
    "\n",
    "\n",
    "\t# Find most frequent terms and filter\n",
    "\tmostFrequentTerms = [(k, int(v), round(int(v) / totalBaskets, 3)) for k, v in tupleList.iteritems() \n",
    "                         if int(v) >= support]\n",
    "    \n",
    "\tmostFrequentTerms = sorted(mostFrequentTerms, \n",
    "\t\t\t\t\t\t\t\tkey = lambda x: x[1], \n",
    "\t\t\t\t\t\t\t\treverse = True)\n",
    "\n",
    "\n",
    "\t# Get results \n",
    "\tprint '\\n' + '========== Most Frequent Terms ==========' + '\\n'\n",
    "\ttemplate = \"{0:30}|{1:20}|{2:20}\"\n",
    "\tprint template.format(\"PAIR\", \"SUPPORT COUNT\", \"RELATIVE FREQUENCY\")\n",
    "\t\n",
    "\t# Print terms \n",
    "\tfor termPair in mostFrequentTerms[:50]: \n",
    "\t\tprint template.format(*termPair)\n",
    "\n",
    "\t# Update counter \n",
    "\tsys.stderr.write(\"reporter:counter:User-Defined,Number of Reducers,1\\n\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combiner\n",
    "\n",
    "Below is the combiner used optionally in the process. It aggregates in a similar way to the reduce and outputes partially aggregated stripes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combinerQ35.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combinerQ35.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from __future__ import division \n",
    "import sys \n",
    "from collections import defaultdict \n",
    "import ast \n",
    "\n",
    "\n",
    "# Read input from mapper \n",
    "def readInput(file, separator='\\t'):\n",
    "\tfor line in file:\n",
    "\t\tyield line.split(separator)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\ttokenDict = defaultdict(list)\n",
    "\tmergedDict = defaultdict(dict)\n",
    "\ttotalBaskets = 0\n",
    "\n",
    "\n",
    "\t# Read data \n",
    "\tdata = readInput(sys.stdin)\n",
    "\tfor line in data: \n",
    "\n",
    "\t\ttoken = line[0]\n",
    "\t\tif token == '*': \n",
    "\n",
    "\t\t\t# Increment basket \n",
    "\t\t\ttotalBaskets += int(line[1])\t\n",
    "\n",
    "\t\telse:\n",
    "\n",
    "\t\t\t# Parse normally \n",
    "\t\t\tstripe = ast.literal_eval(line[1])\n",
    "\t\t\t\t\n",
    "\t\t\t# Combine dictionaries \n",
    "\t\t\ttokenDict[token].append(stripe) \n",
    "\n",
    "\n",
    "\t# # Merge stripes \n",
    "\tfor k, v in tokenDict.iteritems(): \n",
    "\t\tmerged = defaultdict(int)\n",
    "\t\t\n",
    "\t\t# Loop and aggregate \n",
    "\t\tfor stripe in v: \n",
    "\t\t\tfor stripeKey in stripe: \n",
    "\t\t\t\tmerged[stripeKey] += stripe[stripeKey]\n",
    "\n",
    "\t\tmergedDict[k] = dict(merged)\n",
    "\n",
    "\t# Emit results \n",
    "\tfor k, v in mergedDict.iteritems(): \n",
    "\t\tprint '%s%s%s' % (k, '\\t', str(v))\n",
    "\n",
    "\t# Update counter \n",
    "\tsys.stderr.write(\"reporter:counter:User-Defined,Number of Combiners,1\\n\")      \n",
    "\n",
    "\t# Emit baskets \n",
    "\tprint '%s%s%s' % ('*', '\\t', str(totalBaskets))          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Submit Job \n",
    "\n",
    "Our wrapper remains unchanged from the prior implementation that has an argument input for the combiner. We re-use it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 19:50:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 19:50:34 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/john/notebook\n",
      "16/02/01 19:50:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 19:50:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 19:50:42 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/01 19:50:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/mapperQ35.py, /Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/reducerQ35.py, /Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/combinerQ35.py] [] /var/folders/0w/8hzv7rsj3qgdynsjlqy3gjsc0000gn/T/streamjob6872178788643398508.jar tmpDir=null\n",
      "16/02/01 19:50:44 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 19:50:44 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 19:50:44 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 19:50:44 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 19:50:44 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 19:50:45 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local150026337_0001\n",
      "16/02/01 19:50:45 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/mapperQ35.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1454374245453/mapperQ35.py\n",
      "16/02/01 19:50:45 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/reducerQ35.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1454374245454/reducerQ35.py\n",
      "16/02/01 19:50:45 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/combinerQ35.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1454374245455/combinerQ35.py\n",
      "16/02/01 19:50:46 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 19:50:46 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 19:50:46 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 19:50:46 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 19:50:46 INFO mapreduce.Job: Running job: job_local150026337_0001\n",
      "16/02/01 19:50:46 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 19:50:46 INFO mapred.LocalJobRunner: Starting task: attempt_local150026337_0001_m_000000_0\n",
      "16/02/01 19:50:46 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 19:50:46 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 19:50:46 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 19:50:46 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/john/notebook/input/ProductPurchaseData.txt:0+3458517\n",
      "16/02/01 19:50:46 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/01 19:50:46 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 19:50:46 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 19:50:46 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 19:50:46 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 19:50:46 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 19:50:46 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 19:50:46 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./mapperQ35.py]\n",
      "16/02/01 19:50:46 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 19:50:46 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 19:50:46 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 19:50:46 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 19:50:46 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 19:50:46 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 19:50:46 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 19:50:46 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 19:50:46 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 19:50:46 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 19:50:46 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 19:50:46 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 19:50:46 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:50:46 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:50:46 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:50:46 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:50:46 INFO streaming.PipeMapRed: Records R/W=1216/1\n",
      "16/02/01 19:50:47 INFO mapreduce.Job: Job job_local150026337_0001 running in uber mode : false\n",
      "16/02/01 19:50:47 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/01 19:50:49 INFO streaming.PipeMapRed: R/W/S=10000/120238/0 in:3333=10000/3 [rec/s] out:40079=120238/3 [rec/s]\n",
      "16/02/01 19:50:52 INFO mapred.LocalJobRunner: Records R/W=1216/1 > map\n",
      "16/02/01 19:50:53 INFO mapreduce.Job:  map 38% reduce 0%\n",
      "16/02/01 19:50:55 INFO mapred.LocalJobRunner: Records R/W=1216/1 > map\n",
      "16/02/01 19:50:56 INFO mapreduce.Job:  map 53% reduce 0%\n",
      "16/02/01 19:50:56 INFO streaming.PipeMapRed: Records R/W=26917/313473\n",
      "16/02/01 19:50:57 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 19:50:57 INFO mapred.MapTask: bufstart = 0; bufend = 77965406; bufvoid = 104857600\n",
      "16/02/01 19:50:57 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24734212(98936848); length = 1480185/6553600\n",
      "16/02/01 19:50:57 INFO mapred.MapTask: (EQUATOR) 79450094 kvi 19862516(79450064)\n",
      "16/02/01 19:50:58 INFO mapred.LocalJobRunner: Records R/W=26917/313473 > map\n",
      "16/02/01 19:50:58 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 19:50:58 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 19:50:58 INFO mapred.LocalJobRunner: Records R/W=26917/313473 > map\n",
      "16/02/01 19:50:58 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 19:50:58 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./combinerQ35.py]\n",
      "16/02/01 19:50:58 INFO Configuration.deprecation: mapred.skip.map.auto.incr.proc.count is deprecated. Instead, use mapreduce.map.skip.proc-count.auto-incr\n",
      "16/02/01 19:50:58 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:50:58 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:50:58 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:50:59 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:50:59 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/02/01 19:51:00 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:10000=10000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 19:51:01 INFO mapred.LocalJobRunner: Records R/W=26917/313473 > sort\n",
      "16/02/01 19:51:04 INFO mapred.LocalJobRunner: Records R/W=26917/313473 > sort\n",
      "16/02/01 19:51:07 INFO mapred.LocalJobRunner: Records R/W=26917/313473 > sort\n",
      "16/02/01 19:51:10 INFO mapred.LocalJobRunner: Records R/W=26917/313473 > sort\n",
      "16/02/01 19:51:11 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:8333=100000/12 [rec/s] out:0=0/12 [rec/s]\n",
      "16/02/01 19:51:13 INFO mapred.LocalJobRunner: Records R/W=26917/313473 > sort\n",
      "16/02/01 19:51:16 INFO mapred.LocalJobRunner: Records R/W=26917/313473 > sort\n",
      "16/02/01 19:51:19 INFO mapred.LocalJobRunner: Records R/W=26917/313473 > sort\n",
      "16/02/01 19:51:22 INFO mapred.LocalJobRunner: Records R/W=26917/313473 > sort\n",
      "16/02/01 19:51:25 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:7692=200000/26 [rec/s] out:0=0/26 [rec/s]\n",
      "16/02/01 19:51:25 INFO mapred.LocalJobRunner: Records R/W=26917/313473 > sort\n",
      "16/02/01 19:51:28 INFO mapred.LocalJobRunner: Records R/W=26917/313473 > sort\n",
      "16/02/01 19:51:31 INFO mapred.LocalJobRunner: Records R/W=26917/313473 > sort\n",
      "16/02/01 19:51:34 INFO mapred.LocalJobRunner: Records R/W=26917/313473 > sort\n",
      "16/02/01 19:51:37 INFO mapred.LocalJobRunner: Records R/W=26917/313473 > sort\n",
      "16/02/01 19:51:38 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:7692=300000/39 [rec/s] out:0=0/39 [rec/s]\n",
      "16/02/01 19:51:40 INFO mapred.LocalJobRunner: Records R/W=26917/313473 > sort\n",
      "16/02/01 19:51:43 INFO mapred.LocalJobRunner: Records R/W=26917/313473 > sort\n",
      "16/02/01 19:51:46 INFO mapred.LocalJobRunner: Records R/W=26917/313473 > sort\n",
      "16/02/01 19:51:49 INFO mapred.LocalJobRunner: Records R/W=26917/313473 > sort\n",
      "16/02/01 19:51:52 INFO streaming.PipeMapRed: Records R/W=370047/1\n",
      "16/02/01 19:51:52 INFO mapred.LocalJobRunner: Records R/W=370047/1 > sort\n",
      "16/02/01 19:51:54 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 19:51:54 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 19:51:54 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 19:51:54 INFO mapred.MapTask: (RESET) equator 79450094 kv 19862516(79450064) kvi 19819404(79277616)\n",
      "16/02/01 19:51:54 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 19:51:54 INFO mapred.MapTask: bufstart = 79450094; bufend = 81699127; bufvoid = 104857600\n",
      "16/02/01 19:51:54 INFO mapred.MapTask: kvstart = 19862516(79450064); kvend = 19819408(79277632); length = 43109/6553600\n",
      "16/02/01 19:51:54 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./combinerQ35.py]\n",
      "16/02/01 19:51:54 INFO Configuration.deprecation: mapred.skip.reduce.auto.incr.proc.count is deprecated. Instead, use mapreduce.reduce.skip.proc-count.auto-incr\n",
      "16/02/01 19:51:54 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:51:54 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:51:54 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:51:54 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:51:55 INFO mapred.LocalJobRunner: Records R/W=370047/1 > sort\n",
      "16/02/01 19:51:55 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:10000=10000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/01 19:51:55 INFO streaming.PipeMapRed: Records R/W=10778/1\n",
      "16/02/01 19:51:55 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 19:51:55 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 19:51:55 INFO mapred.MapTask: Finished spill 1\n",
      "16/02/01 19:51:55 INFO mapred.Merger: Merging 2 sorted segments\n",
      "16/02/01 19:51:55 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 27515029 bytes\n",
      "16/02/01 19:51:56 INFO mapred.Task: Task:attempt_local150026337_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 19:51:56 INFO mapred.LocalJobRunner: Records R/W=10778/1 > sort\n",
      "16/02/01 19:51:56 INFO mapred.Task: Task 'attempt_local150026337_0001_m_000000_0' done.\n",
      "16/02/01 19:51:56 INFO mapred.LocalJobRunner: Finishing task: attempt_local150026337_0001_m_000000_0\n",
      "16/02/01 19:51:56 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 19:51:56 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 19:51:56 INFO mapred.LocalJobRunner: Starting task: attempt_local150026337_0001_r_000000_0\n",
      "16/02/01 19:51:56 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/01 19:51:56 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 19:51:56 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 19:51:56 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@32592bbb\n",
      "16/02/01 19:51:56 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/01 19:51:56 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 19:51:56 INFO reduce.EventFetcher: attempt_local150026337_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 19:51:56 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local150026337_0001_m_000000_0 decomp: 27515044 len: 27515048 to MEMORY\n",
      "16/02/01 19:51:56 INFO reduce.InMemoryMapOutput: Read 27515044 bytes from map-output for attempt_local150026337_0001_m_000000_0\n",
      "16/02/01 19:51:56 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 27515044, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->27515044\n",
      "16/02/01 19:51:56 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 19:51:56 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 19:51:56 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 19:51:56 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 19:51:56 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 27515031 bytes\n",
      "16/02/01 19:51:56 INFO reduce.MergeManagerImpl: Merged 1 segments, 27515044 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 19:51:56 INFO reduce.MergeManagerImpl: Merging 1 files, 27515048 bytes from disk\n",
      "16/02/01 19:51:57 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 19:51:57 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 19:51:57 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 27515031 bytes\n",
      "16/02/01 19:51:57 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 19:51:57 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./reducerQ35.py]\n",
      "16/02/01 19:51:57 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 19:51:57 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 19:51:57 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:51:57 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:51:57 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:51:57 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 19:52:02 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 19:52:02 INFO mapreduce.Job:  map 100% reduce 78%\n",
      "16/02/01 19:52:05 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 19:52:05 INFO mapreduce.Job:  map 100% reduce 84%\n",
      "16/02/01 19:52:07 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:1111=10000/9 [rec/s] out:0=0/9 [rec/s]\n",
      "16/02/01 19:52:08 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 19:52:08 INFO mapreduce.Job:  map 100% reduce 91%\n",
      "16/02/01 19:52:11 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 19:52:11 INFO mapreduce.Job:  map 100% reduce 97%\n",
      "16/02/01 19:52:14 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 19:52:14 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 19:52:17 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 19:52:22 INFO streaming.PipeMapRed: Records R/W=14619/1\n",
      "16/02/01 19:52:22 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 19:52:22 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 19:52:22 INFO mapred.Task: Task:attempt_local150026337_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 19:52:22 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 19:52:22 INFO mapred.Task: Task attempt_local150026337_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 19:52:22 INFO output.FileOutputCommitter: Saved output of task 'attempt_local150026337_0001_r_000000_0' to hdfs://localhost:9000/user/john/notebook/output/_temporary/0/task_local150026337_0001_r_000000\n",
      "16/02/01 19:52:22 INFO mapred.LocalJobRunner: Records R/W=14619/1 > reduce\n",
      "16/02/01 19:52:22 INFO mapred.Task: Task 'attempt_local150026337_0001_r_000000_0' done.\n",
      "16/02/01 19:52:22 INFO mapred.LocalJobRunner: Finishing task: attempt_local150026337_0001_r_000000_0\n",
      "16/02/01 19:52:22 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 19:52:23 INFO mapreduce.Job: Job job_local150026337_0001 completed successfully\n",
      "16/02/01 19:52:23 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=110073226\n",
      "\t\tFILE: Number of bytes written=138175744\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6917034\n",
      "\t\tHDFS: Number of bytes written=3821\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=380825\n",
      "\t\tMap output bytes=80214439\n",
      "\t\tMap output materialized bytes=27515048\n",
      "\t\tInput split bytes=122\n",
      "\t\tCombine input records=380825\n",
      "\t\tCombine output records=14619\n",
      "\t\tReduce input groups=14617\n",
      "\t\tReduce shuffle bytes=27515048\n",
      "\t\tReduce input records=14619\n",
      "\t\tReduce output records=54\n",
      "\t\tSpilled Records=43857\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=26\n",
      "\t\tTotal committed heap usage (bytes)=534249472\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tUser-Defined\n",
      "\t\tNumber of Combiners=2\n",
      "\t\tNumber of Mappers=1\n",
      "\t\tNumber of Reducers=1\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3821\n",
      "16/02/01 19:52:23 INFO streaming.StreamJob: Output directory: /user/john/notebook/output\n",
      "16/02/01 19:52:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\t\n",
      "========== Most Frequent Terms ==========\t\n",
      "\t\n",
      "PAIR                          |SUPPORT COUNT       |RELATIVE FREQUENCY  \t\n",
      "('DAI62779', 'ELE17451')      |                1592|               0.051\t\n",
      "('FRO40251', 'SNA80324')      |                1412|               0.045\t\n",
      "('DAI75645', 'FRO40251')      |                1254|                0.04\t\n",
      "('FRO40251', 'GRO85051')      |                1213|               0.039\t\n",
      "('DAI62779', 'GRO73461')      |                1139|               0.037\t\n",
      "('DAI75645', 'SNA80324')      |                1130|               0.036\t\n",
      "('DAI62779', 'FRO40251')      |                1070|               0.034\t\n",
      "('DAI62779', 'SNA80324')      |                 923|                0.03\t\n",
      "('DAI62779', 'DAI85309')      |                 918|                0.03\t\n",
      "('ELE32164', 'GRO59710')      |                 911|               0.029\t\n",
      "('FRO40251', 'GRO73461')      |                 882|               0.028\t\n",
      "('DAI62779', 'DAI75645')      |                 882|               0.028\t\n",
      "('DAI62779', 'ELE92920')      |                 877|               0.028\t\n",
      "('FRO40251', 'FRO92469')      |                 835|               0.027\t\n",
      "('DAI62779', 'ELE32164')      |                 832|               0.027\t\n",
      "('DAI75645', 'GRO73461')      |                 712|               0.023\t\n",
      "('DAI43223', 'ELE32164')      |                 711|               0.023\t\n",
      "('DAI62779', 'GRO30386')      |                 709|               0.023\t\n",
      "('ELE17451', 'FRO40251')      |                 697|               0.022\t\n",
      "('DAI85309', 'ELE99737')      |                 659|               0.021\t\n",
      "('DAI62779', 'ELE26917')      |                 650|               0.021\t\n",
      "('GRO21487', 'GRO73461')      |                 631|                0.02\t\n",
      "('DAI62779', 'SNA45677')      |                 604|               0.019\t\n",
      "('ELE17451', 'SNA80324')      |                 597|               0.019\t\n",
      "('DAI62779', 'GRO71621')      |                 595|               0.019\t\n",
      "('DAI62779', 'SNA55762')      |                 593|               0.019\t\n",
      "('DAI62779', 'DAI83733')      |                 586|               0.019\t\n",
      "('ELE17451', 'GRO73461')      |                 580|               0.019\t\n",
      "('GRO73461', 'SNA80324')      |                 562|               0.018\t\n",
      "('DAI62779', 'GRO59710')      |                 561|               0.018\t\n",
      "('DAI62779', 'FRO80039')      |                 550|               0.018\t\n",
      "('DAI75645', 'ELE17451')      |                 547|               0.018\t\n",
      "('DAI62779', 'SNA93860')      |                 537|               0.017\t\n",
      "('DAI55148', 'DAI62779')      |                 526|               0.017\t\n",
      "('DAI43223', 'GRO59710')      |                 512|               0.016\t\n",
      "('ELE17451', 'ELE32164')      |                 511|               0.016\t\n",
      "('DAI62779', 'SNA18336')      |                 506|               0.016\t\n",
      "('ELE32164', 'GRO73461')      |                 486|               0.016\t\n",
      "('DAI85309', 'ELE17451')      |                 482|               0.015\t\n",
      "('DAI62779', 'FRO78087')      |                 482|               0.015\t\n",
      "('DAI62779', 'GRO94758')      |                 479|               0.015\t\n",
      "('DAI62779', 'GRO21487')      |                 471|               0.015\t\n",
      "('GRO85051', 'SNA80324')      |                 471|               0.015\t\n",
      "('ELE17451', 'GRO30386')      |                 468|               0.015\t\n",
      "('FRO85978', 'SNA95666')      |                 463|               0.015\t\n",
      "('DAI62779', 'FRO19221')      |                 462|               0.015\t\n",
      "('DAI62779', 'GRO46854')      |                 461|               0.015\t\n",
      "('DAI43223', 'DAI62779')      |                 459|               0.015\t\n",
      "('ELE92920', 'SNA18336')      |                 455|               0.015\t\n",
      "('DAI88079', 'FRO40251')      |                 446|               0.014\t\n"
     ]
    }
   ],
   "source": [
    "!bash wrapperQ34.sh ProductPurchaseData.txt mapperQ35.py reducerQ35.py combinerQ35.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job was run on a Macbook Pro with a 2.66GHz Intel dual-core processor and 4GB of memory. \n",
    "\n",
    "This job took 1 minute and 51 seconds, so only slighly longer than the previous job. \n",
    "\n",
    "Based on the results from Question 3.4 and Question 3.5, the two jobs use the same number of combiners, mappers, and reducers to accomplish their respective tasks. These are 2 combiners, 1 mapper, and 1 reducer for both jobs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## HW3.6 (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A\n",
    "\n",
    "*What is the Apriori algorithm? Describe an example use in your domain of expertise.*\n",
    "\n",
    "#### Solution: \n",
    "\n",
    "The Apriori algorithm is an algorithm for associative rule learning over transactional data sets. In particular, it implements a multi-scan approach with different tuning parameters like minimum support and minimum confidence. These are used to improve efficiency and control performance. In particular, the Apriori algorithm will identify frequent terms from the given support, and recurisvely compute larger baskets and search for additional frequent items. The Apriori algorithm stops when no further baskets can be found that are frequent. \n",
    "\n",
    "Within the domain of telecommunications, Apriori-like algorithms are used to describe the customer journey from signing-up to porting out. Identifying patterns that lead to churn is very important for Big Telecom, as the market continues to grow more volatile and competitive. Looking at information like clicks, calls to customer support, or billing events and building associative rules is important in finding business insights that improve customer retention. \n",
    "\n",
    "\n",
    "### Part B\n",
    "*Define confidence and lift*\n",
    "\n",
    "#### Solution: \n",
    "\n",
    "Given elements A and B, the confidence of A => B is equal to the support of A and B divided by the support of A. In words, confidence represents the likelihood of observing a basket containing B given a basket containing A. \n",
    "\n",
    "Lift is defined as a measure of performance of a model at predicting enhanced responses against a random choice targetting. In other words, Lift describes how well a model performs in identifying association rules that are more confident than selecting association rules at random. Given elements A and B, the lift of A => B is defined as the confidence of A and B given the average confidence across the entire basket list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## HW3.7. Shopping Cart Analysis\n",
    "Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a program using the A-priori algorithm\n",
    "to find products which are frequently browsed together. Fix the support to s = 100 \n",
    "(i.e. product sets need to occur together at least 100 times to be considered frequent) \n",
    "and find itemsets of size 2 and 3.\n",
    "\n",
    "Then extract association rules from these frequent items. \n",
    "\n",
    "A rule is of the form: \n",
    "\n",
    "(item1, item5) ⇒ item2.\n",
    "\n",
    "List the top 10 discovered rules in descreasing order of confidence in the following format\n",
    " \n",
    "(item1, item5) ⇒ item2, supportCount ,support, confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.7 Mapper\n",
    "\n",
    "We use the pairs implementation here as it requires minimal code changes, albeit being less efficient. To this end, the mapper now outputs both 2 and 3-tuples which are treated the same by the reducer. The reducer changes its printing functionality to this end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapperQ37.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapperQ37.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from __future__ import division \n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "def readInput(file, separator=None):\n",
    "\tfor line in file:\n",
    "\t\tyield line.split(separator)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\t# Read input \n",
    "\tdata = readInput(sys.stdin)\n",
    "\tfor line in data: \n",
    "\n",
    "\t\t# Get unique keys \n",
    "\t\tdoublePairs = list(combinations(line, 2))\n",
    "\t\ttriplePairs = list(combinations(line, 3))\n",
    "\t\t\n",
    "\n",
    "\t\t# Sort doubles \n",
    "\t\tsortedDoubles = []\n",
    "\t\tfor pair in doublePairs: \n",
    "\t\t\tpList = list(pair)\n",
    "\t\t\tpList.sort()\n",
    "\t\t\tsortedDoubles.append(tuple(pList))\n",
    "\n",
    "\t\t# Emit\n",
    "\t\tfor pair in sortedDoubles: \n",
    "\t\t\tprint '%s%s%s' % (pair, '\\t', 1)\n",
    "\n",
    "\n",
    "\t\t# Sort triples \n",
    "\t\tsortedTriples = []\n",
    "\t\tfor pair in triplePairs: \n",
    "\t\t\tpList = list(pair)\n",
    "\t\t\tpList.sort()\n",
    "\t\t\tsortedTriples.append(tuple(pList))\n",
    "\n",
    "\t\t# Emit\n",
    "\t\tfor pair in sortedTriples: \n",
    "\t\t\tprint '%s%s%s' % (pair, '\\t', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.7 Reducer\n",
    "\n",
    "We modify the reducer so it outputs the top 50 of both the length-2 and length-3 tuples. This is mainly a cosmetic change in the printing functionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducerQ37.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducerQ37.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from __future__ import division \n",
    "import sys \n",
    "from collections import defaultdict\n",
    "import ast \n",
    "\n",
    "\n",
    "def readInput(file, separator='\\t'):\n",
    "\tfor line in file:\n",
    "\t\tyield line.split(separator)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\t# Final store \n",
    "\tstoringDict = defaultdict(int)\n",
    "\tsupport = 100\n",
    "\n",
    "\n",
    "\t# Read data \n",
    "\tdata = readInput(sys.stdin)\n",
    "\tfor line in data: \n",
    "\n",
    "\t\t# Parse value \n",
    "\t\ttoken = line[0]\n",
    "\t\ttermCount = int(line[1])\n",
    "\n",
    "\n",
    "\t\t# Store results \n",
    "\t\tstoringDict[token] += termCount\n",
    "\n",
    "\t\n",
    "\t# Filter\n",
    "\tfilterDict = defaultdict(int)\n",
    "\tfor k, v in storingDict.iteritems(): \n",
    "\t\tif v >= support:\n",
    "\t\t\tfilterDict[k] += v \n",
    "\n",
    "\n",
    "\t# Find most frequent doubles \n",
    "\tmfqDoubles = [(k, v, support) for k, v in filterDict.iteritems()\n",
    "\t\t\t\t\tif k.count(',') == 1]\n",
    "\n",
    "\tmfqDoubles = sorted(mfqDoubles, \n",
    "\t\t\t\t\t\t\t\tkey = lambda x: x[1], \n",
    "\t\t\t\t\t\t\t\treverse = True)\n",
    "\n",
    "\n",
    "\t# Get results \n",
    "\tprint '\\n' + '========== Most Frequent Doubles ==========' + '\\n'\n",
    "\ttemplate = \"{0:30}|{1:20}|{2:20}\"\n",
    "\tprint template.format(\"PAIR\", \"SUPPORT COUNT\", \"SUPPORT\")\n",
    "\t\n",
    "\t# Print terms \n",
    "\tfor termPair in mfqDoubles[:50]: \n",
    "\t\tprint template.format(*termPair)\n",
    "\n",
    "\n",
    "\n",
    "\t# Find most frequent triples \n",
    "\tmfqTriples = [(k, v, support) for k, v in filterDict.iteritems()\n",
    "\t\t\t\t\tif k.count(',') == 2]\n",
    "\t\t\t\t\t\n",
    "\tmfqTriples = sorted(mfqTriples, \n",
    "\t\t\t\t\t\t\t\tkey = lambda x: x[1], \n",
    "\t\t\t\t\t\t\t\treverse = True)\n",
    "\n",
    "\n",
    "\t# Get results \n",
    "\tprint '\\n' + '========== Most Frequent Triples ==========' + '\\n'\n",
    "\ttemplate = \"{0:50}|{1:20}|{2:20}\"\n",
    "\tprint template.format(\"PAIR\", \"SUPPORT COUNT\", \"SUPPORT\")\n",
    "\t\n",
    "\t# Print terms \n",
    "\tfor termPair in mfqTriples[:50]: \n",
    "\t\tprint template.format(*termPair)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.7 Hadoop MapReduce Submit Job \n",
    "\n",
    "We now submit the job using the combiner and wrapper defined in Question 3.4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 20:39:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 20:39:33 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/john/notebook\n",
      "16/01/30 20:39:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 20:39:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 20:39:42 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/30 20:39:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/mapperQ37.py, /Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/reducerQ37.py, /Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/combinerQ34.py] [] /var/folders/0w/8hzv7rsj3qgdynsjlqy3gjsc0000gn/T/streamjob5966942647828294044.jar tmpDir=null\n",
      "16/01/30 20:39:44 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/30 20:39:44 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/30 20:39:44 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/30 20:39:45 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/30 20:39:45 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/30 20:39:45 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local583153488_0001\n",
      "16/01/30 20:39:45 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/mapperQ37.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1454204385598/mapperQ37.py\n",
      "16/01/30 20:39:46 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/reducerQ37.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1454204385599/reducerQ37.py\n",
      "16/01/30 20:39:46 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/combinerQ34.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1454204385600/combinerQ34.py\n",
      "16/01/30 20:39:46 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/30 20:39:46 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/30 20:39:46 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/30 20:39:46 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 20:39:46 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/30 20:39:46 INFO mapred.LocalJobRunner: Starting task: attempt_local583153488_0001_m_000000_0\n",
      "16/01/30 20:39:46 INFO mapreduce.Job: Running job: job_local583153488_0001\n",
      "16/01/30 20:39:46 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 20:39:46 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 20:39:46 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 20:39:46 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/john/notebook/input/ProductPurchaseData.txt:0+3458517\n",
      "16/01/30 20:39:46 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/30 20:39:46 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/30 20:39:46 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/30 20:39:46 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/30 20:39:46 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/30 20:39:46 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/30 20:39:46 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/30 20:39:46 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./mapperQ37.py]\n",
      "16/01/30 20:39:46 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/30 20:39:46 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/30 20:39:46 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/30 20:39:46 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/30 20:39:46 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/30 20:39:46 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/30 20:39:46 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/30 20:39:46 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/30 20:39:46 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/30 20:39:46 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/30 20:39:46 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/30 20:39:46 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/30 20:39:46 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:39:46 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:39:46 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:39:46 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:39:46 INFO streaming.PipeMapRed: Records R/W=1216/1\n",
      "16/01/30 20:39:47 INFO mapreduce.Job: Job job_local583153488_0001 running in uber mode : false\n",
      "16/01/30 20:39:47 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/30 20:39:52 INFO mapred.LocalJobRunner: Records R/W=1216/1 > map\n",
      "16/01/30 20:39:53 INFO mapreduce.Job:  map 8% reduce 0%\n",
      "16/01/30 20:39:54 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 20:39:54 INFO mapred.MapTask: bufstart = 0; bufend = 58481682; bufvoid = 104857600\n",
      "16/01/30 20:39:54 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 19863304(79453216); length = 6351093/6553600\n",
      "16/01/30 20:39:54 INFO mapred.MapTask: (EQUATOR) 64934450 kvi 16233608(64934432)\n",
      "16/01/30 20:39:55 INFO mapred.LocalJobRunner: Records R/W=1216/1 > map\n",
      "16/01/30 20:39:56 INFO mapreduce.Job:  map 10% reduce 0%\n",
      "16/01/30 20:39:56 INFO streaming.PipeMapRed: Records R/W=5743/1928237\n",
      "16/01/30 20:39:58 INFO mapred.LocalJobRunner: Records R/W=5743/1928237 > map\n",
      "16/01/30 20:39:59 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "16/01/30 20:39:59 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./combinerQ34.py]\n",
      "16/01/30 20:39:59 INFO Configuration.deprecation: mapred.skip.map.auto.incr.proc.count is deprecated. Instead, use mapreduce.map.skip.proc-count.auto-incr\n",
      "16/01/30 20:39:59 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:39:59 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:39:59 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:39:59 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:39:59 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:00 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:00 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:00 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:40:01 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:40:01 INFO mapred.LocalJobRunner: Records R/W=5743/1928237 > map\n",
      "16/01/30 20:40:01 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:500000=500000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:40:01 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:300000=600000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:40:02 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:350000=700000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:40:02 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:400000=800000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:40:02 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:300000=900000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:40:03 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:333333=1000000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:40:03 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:366666=1100000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:40:03 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:300000=1200000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:40:04 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:325000=1300000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:40:04 INFO mapred.LocalJobRunner: Records R/W=5743/1928237 > map\n",
      "16/01/30 20:40:04 INFO streaming.PipeMapRed: R/W/S=1400000/0/0 in:350000=1400000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:40:04 INFO streaming.PipeMapRed: R/W/S=1500000/0/0 in:300000=1500000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:40:05 INFO streaming.PipeMapRed: Records R/W=1587774/1\n",
      "16/01/30 20:40:07 INFO mapred.LocalJobRunner: Records R/W=1587774/1 > map\n",
      "16/01/30 20:40:07 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 20:40:07 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 20:40:07 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/30 20:40:07 INFO mapred.MapTask: (RESET) equator 64934450 kv 16233608(64934432) kvi 14659840(58639360)\n",
      "16/01/30 20:40:07 INFO streaming.PipeMapRed: Records R/W=5743/1981217\n",
      "16/01/30 20:40:10 INFO mapred.LocalJobRunner: Records R/W=5743/1981217 > map\n",
      "16/01/30 20:40:11 INFO mapreduce.Job:  map 15% reduce 0%\n",
      "16/01/30 20:40:13 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 20:40:13 INFO mapred.MapTask: bufstart = 64934450; bufend = 18648555; bufvoid = 104857577\n",
      "16/01/30 20:40:13 INFO mapred.MapTask: kvstart = 16233608(64934432); kvend = 9905012(39620048); length = 6328597/6553600\n",
      "16/01/30 20:40:13 INFO mapred.MapTask: (EQUATOR) 25101307 kvi 6275320(25101280)\n",
      "16/01/30 20:40:13 INFO mapred.LocalJobRunner: Records R/W=5743/1981217 > map\n",
      "16/01/30 20:40:13 INFO mapreduce.Job:  map 18% reduce 0%\n",
      "16/01/30 20:40:16 INFO mapred.LocalJobRunner: Records R/W=5743/1981217 > map\n",
      "16/01/30 20:40:17 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./combinerQ34.py]\n",
      "16/01/30 20:40:17 INFO Configuration.deprecation: mapred.skip.reduce.auto.incr.proc.count is deprecated. Instead, use mapreduce.reduce.skip.proc-count.auto-incr\n",
      "16/01/30 20:40:17 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:17 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:17 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:17 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:17 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:17 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:18 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:18 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:40:18 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:40:19 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:500000=500000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:40:19 INFO mapred.LocalJobRunner: Records R/W=5743/1981217 > map\n",
      "16/01/30 20:40:19 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:300000=600000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:40:19 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:350000=700000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:40:20 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:400000=800000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:40:20 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:300000=900000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:40:20 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:333333=1000000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:40:21 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:275000=1100000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:40:21 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:300000=1200000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:40:22 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:325000=1300000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:40:22 INFO mapred.LocalJobRunner: Records R/W=5743/1981217 > map\n",
      "16/01/30 20:40:22 INFO streaming.PipeMapRed: R/W/S=1400000/0/0 in:280000=1400000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:40:23 INFO streaming.PipeMapRed: R/W/S=1500000/0/0 in:300000=1500000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:40:23 INFO streaming.PipeMapRed: Records R/W=1582150/1\n",
      "16/01/30 20:40:25 INFO mapred.LocalJobRunner: Records R/W=1582150/1 > map\n",
      "16/01/30 20:40:26 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 20:40:26 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 20:40:26 INFO mapred.MapTask: Finished spill 1\n",
      "16/01/30 20:40:26 INFO mapred.MapTask: (RESET) equator 25101307 kv 6275320(25101280) kvi 4710532(18842128)\n",
      "16/01/30 20:40:26 INFO streaming.PipeMapRed: Records R/W=7778/3561122\n",
      "16/01/30 20:40:28 INFO mapred.LocalJobRunner: Records R/W=7778/3561122 > map\n",
      "16/01/30 20:40:28 INFO mapreduce.Job:  map 20% reduce 0%\n",
      "16/01/30 20:40:31 INFO mapred.LocalJobRunner: Records R/W=7778/3561122 > map\n",
      "16/01/30 20:40:31 INFO mapreduce.Job:  map 23% reduce 0%\n",
      "16/01/30 20:40:32 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 20:40:32 INFO mapred.MapTask: bufstart = 25101307; bufend = 83736112; bufvoid = 104857600\n",
      "16/01/30 20:40:32 INFO mapred.MapTask: kvstart = 6275320(25101280); kvend = 26176912(104707648); length = 6312809/6553600\n",
      "16/01/30 20:40:32 INFO mapred.MapTask: (EQUATOR) 90067136 kvi 22516780(90067120)\n",
      "16/01/30 20:40:34 INFO mapred.LocalJobRunner: Records R/W=7778/3561122 > map\n",
      "16/01/30 20:40:37 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./combinerQ34.py]\n",
      "16/01/30 20:40:37 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:37 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:37 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:37 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:37 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:37 INFO mapred.LocalJobRunner: Records R/W=7778/3561122 > map\n",
      "16/01/30 20:40:37 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:38 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:40:38 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:40:38 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:40:39 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:500000=500000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:40:39 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:300000=600000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:40:39 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:350000=700000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:40:40 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:400000=800000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:40:40 INFO mapred.LocalJobRunner: Records R/W=7778/3561122 > map\n",
      "16/01/30 20:40:41 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:300000=900000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:40:42 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:200000=1000000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:40:42 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:220000=1100000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:40:43 INFO mapred.LocalJobRunner: Records R/W=7778/3561122 > map\n",
      "16/01/30 20:40:43 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:200000=1200000/6 [rec/s] out:0=0/6 [rec/s]\n",
      "16/01/30 20:40:43 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:216666=1300000/6 [rec/s] out:0=0/6 [rec/s]\n",
      "16/01/30 20:40:44 INFO streaming.PipeMapRed: R/W/S=1400000/0/0 in:200000=1400000/7 [rec/s] out:0=0/7 [rec/s]\n",
      "16/01/30 20:40:45 INFO streaming.PipeMapRed: R/W/S=1500000/0/0 in:214285=1500000/7 [rec/s] out:0=0/7 [rec/s]\n",
      "16/01/30 20:40:45 INFO streaming.PipeMapRed: Records R/W=1578203/1\n",
      "16/01/30 20:40:46 INFO mapred.LocalJobRunner: Records R/W=1578203/1 > map\n",
      "16/01/30 20:40:49 INFO mapred.LocalJobRunner: Records R/W=1578203/1 > map\n",
      "16/01/30 20:40:49 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 20:40:49 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 20:40:49 INFO mapred.MapTask: Finished spill 2\n",
      "16/01/30 20:40:49 INFO mapred.MapTask: (RESET) equator 90067136 kv 22516780(90067120) kvi 20941520(83766080)\n",
      "16/01/30 20:40:49 INFO streaming.PipeMapRed: Records R/W=9736/5141943\n",
      "16/01/30 20:40:49 INFO streaming.PipeMapRed: R/W/S=10000/5165981/0 in:158=10000/63 [rec/s] out:81999=5165981/63 [rec/s]\n",
      "16/01/30 20:40:52 INFO mapred.LocalJobRunner: Records R/W=9736/5141943 > map\n",
      "16/01/30 20:40:52 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "16/01/30 20:40:55 INFO mapred.LocalJobRunner: Records R/W=9736/5141943 > map\n",
      "16/01/30 20:40:55 INFO mapreduce.Job:  map 28% reduce 0%\n",
      "16/01/30 20:40:56 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 20:40:56 INFO mapred.MapTask: bufstart = 90067136; bufend = 43824763; bufvoid = 104857600\n",
      "16/01/30 20:40:56 INFO mapred.MapTask: kvstart = 22516780(90067120); kvend = 16199068(64796272); length = 6317713/6553600\n",
      "16/01/30 20:40:56 INFO mapred.MapTask: (EQUATOR) 50155771 kvi 12538936(50155744)\n",
      "16/01/30 20:40:58 INFO mapred.LocalJobRunner: Records R/W=9736/5141943 > map\n",
      "16/01/30 20:40:58 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "16/01/30 20:41:00 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./combinerQ34.py]\n",
      "16/01/30 20:41:00 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:00 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:00 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:00 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:00 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:01 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:01 INFO mapred.LocalJobRunner: Records R/W=9736/5141943 > map\n",
      "16/01/30 20:41:01 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:02 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:41:02 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:200000=400000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:41:03 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:250000=500000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:41:03 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:200000=600000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:41:04 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:233333=700000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:41:04 INFO mapred.LocalJobRunner: Records R/W=9736/5141943 > map\n",
      "16/01/30 20:41:04 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:266666=800000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:41:05 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:225000=900000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:41:05 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:250000=1000000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:41:05 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:220000=1100000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:41:06 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:240000=1200000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:41:06 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:260000=1300000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:41:07 INFO streaming.PipeMapRed: R/W/S=1400000/0/0 in:233333=1400000/6 [rec/s] out:0=0/6 [rec/s]\n",
      "16/01/30 20:41:07 INFO mapred.LocalJobRunner: Records R/W=9736/5141943 > map\n",
      "16/01/30 20:41:07 INFO streaming.PipeMapRed: R/W/S=1500000/0/0 in:250000=1500000/6 [rec/s] out:0=0/6 [rec/s]\n",
      "16/01/30 20:41:07 INFO streaming.PipeMapRed: Records R/W=1579429/1\n",
      "16/01/30 20:41:10 INFO mapred.LocalJobRunner: Records R/W=1579429/1 > map\n",
      "16/01/30 20:41:10 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 20:41:10 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 20:41:10 INFO mapred.MapTask: Finished spill 3\n",
      "16/01/30 20:41:10 INFO mapred.MapTask: (RESET) equator 50155771 kv 12538936(50155744) kvi 10963592(43854368)\n",
      "16/01/30 20:41:10 INFO streaming.PipeMapRed: Records R/W=13103/6721393\n",
      "16/01/30 20:41:13 INFO mapred.LocalJobRunner: Records R/W=13103/6721393 > map\n",
      "16/01/30 20:41:16 INFO mapred.LocalJobRunner: Records R/W=13103/6721393 > map\n",
      "16/01/30 20:41:16 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 20:41:16 INFO mapred.MapTask: bufstart = 50155771; bufend = 3874239; bufvoid = 104857588\n",
      "16/01/30 20:41:16 INFO mapred.MapTask: kvstart = 12538936(50155744); kvend = 6211436(24845744); length = 6327501/6553600\n",
      "16/01/30 20:41:16 INFO mapred.MapTask: (EQUATOR) 10205247 kvi 2551304(10205216)\n",
      "16/01/30 20:41:16 INFO mapreduce.Job:  map 35% reduce 0%\n",
      "16/01/30 20:41:19 INFO mapred.LocalJobRunner: Records R/W=13103/6721393 > map\n",
      "16/01/30 20:41:19 INFO mapreduce.Job:  map 40% reduce 0%\n",
      "16/01/30 20:41:20 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./combinerQ34.py]\n",
      "16/01/30 20:41:20 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:20 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:20 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:20 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:20 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:20 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:20 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:21 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:41:21 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:41:22 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:500000=500000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:41:22 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:300000=600000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:41:22 INFO mapred.LocalJobRunner: Records R/W=13103/6721393 > map\n",
      "16/01/30 20:41:22 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:350000=700000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:41:23 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:400000=800000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:41:23 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:300000=900000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:41:23 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:333333=1000000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:41:24 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:366666=1100000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:41:24 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:300000=1200000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:41:24 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:325000=1300000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:41:25 INFO streaming.PipeMapRed: R/W/S=1400000/0/0 in:350000=1400000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:41:25 INFO mapred.LocalJobRunner: Records R/W=13103/6721393 > map\n",
      "16/01/30 20:41:25 INFO streaming.PipeMapRed: R/W/S=1500000/0/0 in:300000=1500000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:41:25 INFO streaming.PipeMapRed: Records R/W=1581876/1\n",
      "16/01/30 20:41:28 INFO mapred.LocalJobRunner: Records R/W=1581876/1 > map\n",
      "16/01/30 20:41:28 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 20:41:28 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 20:41:28 INFO mapred.MapTask: Finished spill 4\n",
      "16/01/30 20:41:28 INFO mapred.MapTask: (RESET) equator 10205247 kv 2551304(10205216) kvi 968564(3874256)\n",
      "16/01/30 20:41:28 INFO streaming.PipeMapRed: Records R/W=18924/8305118\n",
      "16/01/30 20:41:31 INFO mapred.LocalJobRunner: Records R/W=18924/8305118 > map\n",
      "16/01/30 20:41:31 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "16/01/30 20:41:34 INFO mapred.LocalJobRunner: Records R/W=18924/8305118 > map\n",
      "16/01/30 20:41:34 INFO mapreduce.Job:  map 45% reduce 0%\n",
      "16/01/30 20:41:34 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 20:41:34 INFO mapred.MapTask: bufstart = 10205247; bufend = 68611041; bufvoid = 104857600\n",
      "16/01/30 20:41:34 INFO mapred.MapTask: kvstart = 2551304(10205216); kvend = 22395636(89582544); length = 6370069/6553600\n",
      "16/01/30 20:41:34 INFO mapred.MapTask: (EQUATOR) 75063793 kvi 18765944(75063776)\n",
      "16/01/30 20:41:37 INFO mapred.LocalJobRunner: Records R/W=18924/8305118 > map\n",
      "16/01/30 20:41:38 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./combinerQ34.py]\n",
      "16/01/30 20:41:38 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:38 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:38 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:38 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:38 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:38 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:39 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:39 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:41:39 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:41:40 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:500000=500000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:41:40 INFO mapred.LocalJobRunner: Records R/W=18924/8305118 > map\n",
      "16/01/30 20:41:40 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:300000=600000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:41:41 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:350000=700000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:41:41 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:400000=800000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:41:41 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:300000=900000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:41:42 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:333333=1000000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:41:42 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:366666=1100000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:41:42 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:300000=1200000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:41:43 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:325000=1300000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:41:43 INFO mapred.LocalJobRunner: Records R/W=18924/8305118 > map\n",
      "16/01/30 20:41:43 INFO streaming.PipeMapRed: R/W/S=1400000/0/0 in:350000=1400000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:41:43 INFO streaming.PipeMapRed: R/W/S=1500000/0/0 in:300000=1500000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:41:44 INFO streaming.PipeMapRed: Records R/W=1592518/1\n",
      "16/01/30 20:41:46 INFO mapred.LocalJobRunner: Records R/W=1592518/1 > map\n",
      "16/01/30 20:41:46 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 20:41:46 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 20:41:46 INFO mapred.MapTask: Finished spill 5\n",
      "16/01/30 20:41:46 INFO mapred.MapTask: (RESET) equator 75063793 kv 18765944(75063776) kvi 17202988(68811952)\n",
      "16/01/30 20:41:46 INFO streaming.PipeMapRed: Records R/W=20935/9892690\n",
      "16/01/30 20:41:49 INFO mapred.LocalJobRunner: Records R/W=20935/9892690 > map\n",
      "16/01/30 20:41:49 INFO mapreduce.Job:  map 48% reduce 0%\n",
      "16/01/30 20:41:51 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 20:41:51 INFO mapred.MapTask: bufstart = 75063793; bufend = 28812705; bufvoid = 104857567\n",
      "16/01/30 20:41:51 INFO mapred.MapTask: kvstart = 18765944(75063776); kvend = 12446056(49784224); length = 6319889/6553600\n",
      "16/01/30 20:41:51 INFO mapred.MapTask: (EQUATOR) 35265473 kvi 8816364(35265456)\n",
      "16/01/30 20:41:52 INFO mapred.LocalJobRunner: Records R/W=20935/9892690 > map\n",
      "16/01/30 20:41:52 INFO mapreduce.Job:  map 51% reduce 0%\n",
      "16/01/30 20:41:55 INFO mapred.LocalJobRunner: Records R/W=20935/9892690 > map\n",
      "16/01/30 20:41:55 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./combinerQ34.py]\n",
      "16/01/30 20:41:55 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:55 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:55 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:55 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:55 INFO mapreduce.Job:  map 53% reduce 0%\n",
      "16/01/30 20:41:55 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:56 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:56 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:41:57 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:41:57 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:200000=400000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:41:58 INFO mapred.LocalJobRunner: Records R/W=20935/9892690 > map\n",
      "16/01/30 20:41:58 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:250000=500000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:41:59 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:200000=600000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:42:00 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:175000=700000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:42:00 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:160000=800000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:42:01 INFO mapred.LocalJobRunner: Records R/W=20935/9892690 > map\n",
      "16/01/30 20:42:01 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:150000=900000/6 [rec/s] out:0=0/6 [rec/s]\n",
      "16/01/30 20:42:02 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:166666=1000000/6 [rec/s] out:0=0/6 [rec/s]\n",
      "16/01/30 20:42:02 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:157142=1100000/7 [rec/s] out:0=0/7 [rec/s]\n",
      "16/01/30 20:42:03 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:171428=1200000/7 [rec/s] out:0=0/7 [rec/s]\n",
      "16/01/30 20:42:03 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:185714=1300000/7 [rec/s] out:0=0/7 [rec/s]\n",
      "16/01/30 20:42:03 INFO streaming.PipeMapRed: R/W/S=1400000/0/0 in:175000=1400000/8 [rec/s] out:0=0/8 [rec/s]\n",
      "16/01/30 20:42:04 INFO streaming.PipeMapRed: R/W/S=1500000/0/0 in:187500=1500000/8 [rec/s] out:0=0/8 [rec/s]\n",
      "16/01/30 20:42:04 INFO mapred.LocalJobRunner: Records R/W=20935/9892690 > map\n",
      "16/01/30 20:42:04 INFO streaming.PipeMapRed: Records R/W=1579973/1\n",
      "16/01/30 20:42:07 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 20:42:07 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 20:42:07 INFO mapred.MapTask: Finished spill 6\n",
      "16/01/30 20:42:07 INFO mapred.MapTask: (RESET) equator 35265473 kv 8816364(35265456) kvi 7254956(29019824)\n",
      "16/01/30 20:42:07 INFO streaming.PipeMapRed: Records R/W=24268/11472276\n",
      "16/01/30 20:42:07 INFO mapred.LocalJobRunner: Records R/W=24268/11472276 > map\n",
      "16/01/30 20:42:10 INFO mapred.LocalJobRunner: Records R/W=24268/11472276 > map\n",
      "16/01/30 20:42:10 INFO mapreduce.Job:  map 56% reduce 0%\n",
      "16/01/30 20:42:11 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 20:42:11 INFO mapred.MapTask: bufstart = 35265473; bufend = 93773681; bufvoid = 104857600\n",
      "16/01/30 20:42:11 INFO mapred.MapTask: kvstart = 8816364(35265456); kvend = 2471904(9887616); length = 6344461/6553600\n",
      "16/01/30 20:42:11 INFO mapred.MapTask: (EQUATOR) 100226449 kvi 25056608(100226432)\n",
      "16/01/30 20:42:13 INFO mapred.LocalJobRunner: Records R/W=24268/11472276 > map\n",
      "16/01/30 20:42:13 INFO mapreduce.Job:  map 61% reduce 0%\n",
      "16/01/30 20:42:15 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./combinerQ34.py]\n",
      "16/01/30 20:42:15 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:15 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:15 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:15 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:15 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:16 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:16 INFO mapred.LocalJobRunner: Records R/W=24268/11472276 > map\n",
      "16/01/30 20:42:16 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:16 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:42:17 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:42:17 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:500000=500000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:42:17 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:300000=600000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:42:18 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:350000=700000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:42:18 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:400000=800000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:42:19 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:300000=900000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:42:19 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:333333=1000000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:42:19 INFO mapred.LocalJobRunner: Records R/W=24268/11472276 > map\n",
      "16/01/30 20:42:19 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:275000=1100000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:42:20 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:300000=1200000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:42:20 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:325000=1300000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:42:20 INFO streaming.PipeMapRed: R/W/S=1400000/0/0 in:280000=1400000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:42:21 INFO streaming.PipeMapRed: R/W/S=1500000/0/0 in:300000=1500000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:42:21 INFO streaming.PipeMapRed: Records R/W=1586116/1\n",
      "16/01/30 20:42:22 INFO mapred.LocalJobRunner: Records R/W=1586116/1 > map\n",
      "16/01/30 20:42:25 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 20:42:25 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 20:42:25 INFO mapred.MapTask: Finished spill 7\n",
      "16/01/30 20:42:25 INFO mapred.MapTask: (RESET) equator 100226449 kv 25056608(100226432) kvi 23468064(93872256)\n",
      "16/01/30 20:42:25 INFO streaming.PipeMapRed: Records R/W=28078/13065176\n",
      "16/01/30 20:42:25 INFO mapred.LocalJobRunner: Records R/W=28078/13065176 > map\n",
      "16/01/30 20:42:28 INFO mapred.LocalJobRunner: Records R/W=28078/13065176 > map\n",
      "16/01/30 20:42:28 INFO mapreduce.Job:  map 63% reduce 0%\n",
      "16/01/30 20:42:31 INFO mapred.LocalJobRunner: Records R/W=28078/13065176 > map\n",
      "16/01/30 20:42:31 INFO mapreduce.Job:  map 66% reduce 0%\n",
      "16/01/30 20:42:32 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 20:42:32 INFO mapred.MapTask: bufstart = 100226449; bufend = 53764836; bufvoid = 104857582\n",
      "16/01/30 20:42:32 INFO mapred.MapTask: kvstart = 25056608(100226432); kvend = 18684088(74736352); length = 6372521/6553600\n",
      "16/01/30 20:42:32 INFO mapred.MapTask: (EQUATOR) 60217604 kvi 15054396(60217584)\n",
      "16/01/30 20:42:34 INFO mapred.LocalJobRunner: Records R/W=28078/13065176 > map\n",
      "16/01/30 20:42:34 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/01/30 20:42:36 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./combinerQ34.py]\n",
      "16/01/30 20:42:36 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:36 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:36 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:36 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:36 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:36 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:37 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:37 INFO mapred.LocalJobRunner: Records R/W=28078/13065176 > map\n",
      "16/01/30 20:42:37 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:42:37 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:42:38 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:500000=500000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:42:38 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:300000=600000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:42:39 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:350000=700000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:42:39 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:400000=800000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:42:39 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:300000=900000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:42:40 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:333333=1000000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:42:40 INFO mapred.LocalJobRunner: Records R/W=28078/13065176 > map\n",
      "16/01/30 20:42:40 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:275000=1100000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:42:40 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:300000=1200000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:42:41 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:325000=1300000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:42:41 INFO streaming.PipeMapRed: R/W/S=1400000/0/0 in:280000=1400000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:42:41 INFO streaming.PipeMapRed: R/W/S=1500000/0/0 in:300000=1500000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:42:42 INFO streaming.PipeMapRed: Records R/W=1593131/1\n",
      "16/01/30 20:42:43 INFO mapred.LocalJobRunner: Records R/W=1593131/1 > map\n",
      "16/01/30 20:42:44 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 20:42:44 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 20:42:44 INFO mapred.MapTask: Finished spill 8\n",
      "16/01/30 20:42:44 INFO mapred.MapTask: (RESET) equator 60217604 kv 15054396(60217584) kvi 13476216(53904864)\n",
      "16/01/30 20:42:44 INFO streaming.PipeMapRed: Records R/W=31101/14655716\n",
      "16/01/30 20:42:45 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 20:42:45 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 20:42:45 INFO mapred.LocalJobRunner: Records R/W=1593131/1 > map\n",
      "16/01/30 20:42:45 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/30 20:42:45 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 20:42:45 INFO mapred.MapTask: bufstart = 60217604; bufend = 77432318; bufvoid = 104857600\n",
      "16/01/30 20:42:45 INFO mapred.MapTask: kvstart = 15054396(60217584); kvend = 13184280(52737120); length = 1870117/6553600\n",
      "16/01/30 20:42:46 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./combinerQ34.py]\n",
      "16/01/30 20:42:46 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:46 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:46 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:46 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:46 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:46 INFO mapred.LocalJobRunner: Records R/W=31101/14655716 > sort\n",
      "16/01/30 20:42:46 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:46 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:47 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:42:47 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:42:47 INFO streaming.PipeMapRed: Records R/W=467530/1\n",
      "16/01/30 20:42:48 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 20:42:48 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 20:42:48 INFO mapred.MapTask: Finished spill 9\n",
      "16/01/30 20:42:48 INFO mapred.Merger: Merging 10 sorted segments\n",
      "16/01/30 20:42:48 INFO mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 477989196 bytes\n",
      "16/01/30 20:42:48 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./combinerQ34.py]\n",
      "16/01/30 20:42:48 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:48 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:48 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:48 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:49 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:49 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:42:49 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:42:50 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:200000=200000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:42:50 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:150000=300000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:42:51 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:200000=400000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:42:51 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:166666=500000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:42:52 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:200000=600000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:42:52 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:42:52 INFO mapreduce.Job:  map 68% reduce 0%\n",
      "16/01/30 20:42:52 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:233333=700000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:42:54 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:160000=800000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:42:55 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:150000=900000/6 [rec/s] out:0=0/6 [rec/s]\n",
      "16/01/30 20:42:55 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:42:55 INFO mapreduce.Job:  map 69% reduce 0%\n",
      "16/01/30 20:42:56 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:142857=1000000/7 [rec/s] out:0=0/7 [rec/s]\n",
      "16/01/30 20:42:56 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:157142=1100000/7 [rec/s] out:0=0/7 [rec/s]\n",
      "16/01/30 20:42:57 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:150000=1200000/8 [rec/s] out:0=0/8 [rec/s]\n",
      "16/01/30 20:42:58 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:144444=1300000/9 [rec/s] out:0=0/9 [rec/s]\n",
      "16/01/30 20:42:58 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:42:58 INFO streaming.PipeMapRed: R/W/S=1400000/0/0 in:155555=1400000/9 [rec/s] out:0=0/9 [rec/s]\n",
      "16/01/30 20:42:58 INFO mapreduce.Job:  map 70% reduce 0%\n",
      "16/01/30 20:42:59 INFO streaming.PipeMapRed: R/W/S=1500000/0/0 in:150000=1500000/10 [rec/s] out:0=0/10 [rec/s]\n",
      "16/01/30 20:43:00 INFO streaming.PipeMapRed: R/W/S=1600000/0/0 in:145454=1600000/11 [rec/s] out:0=0/11 [rec/s]\n",
      "16/01/30 20:43:00 INFO streaming.PipeMapRed: R/W/S=1700000/0/0 in:154545=1700000/11 [rec/s] out:0=0/11 [rec/s]\n",
      "16/01/30 20:43:01 INFO streaming.PipeMapRed: R/W/S=1800000/0/0 in:150000=1800000/12 [rec/s] out:0=0/12 [rec/s]\n",
      "16/01/30 20:43:01 INFO streaming.PipeMapRed: R/W/S=1900000/0/0 in:158333=1900000/12 [rec/s] out:0=0/12 [rec/s]\n",
      "16/01/30 20:43:01 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:01 INFO streaming.PipeMapRed: R/W/S=2000000/0/0 in:166666=2000000/12 [rec/s] out:0=0/12 [rec/s]\n",
      "16/01/30 20:43:01 INFO mapreduce.Job:  map 72% reduce 0%\n",
      "16/01/30 20:43:02 INFO streaming.PipeMapRed: R/W/S=2100000/0/0 in:161538=2100000/13 [rec/s] out:0=0/13 [rec/s]\n",
      "16/01/30 20:43:02 INFO streaming.PipeMapRed: R/W/S=2200000/0/0 in:169230=2200000/13 [rec/s] out:0=0/13 [rec/s]\n",
      "16/01/30 20:43:02 INFO streaming.PipeMapRed: R/W/S=2300000/0/0 in:164285=2300000/14 [rec/s] out:0=0/14 [rec/s]\n",
      "16/01/30 20:43:03 INFO streaming.PipeMapRed: R/W/S=2400000/0/0 in:171428=2400000/14 [rec/s] out:0=0/14 [rec/s]\n",
      "16/01/30 20:43:03 INFO streaming.PipeMapRed: R/W/S=2500000/0/0 in:178571=2500000/14 [rec/s] out:0=0/14 [rec/s]\n",
      "16/01/30 20:43:04 INFO streaming.PipeMapRed: R/W/S=2600000/0/0 in:173333=2600000/15 [rec/s] out:0=0/15 [rec/s]\n",
      "16/01/30 20:43:04 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:04 INFO mapreduce.Job:  map 74% reduce 0%\n",
      "16/01/30 20:43:05 INFO streaming.PipeMapRed: R/W/S=2700000/0/0 in:168750=2700000/16 [rec/s] out:0=0/16 [rec/s]\n",
      "16/01/30 20:43:05 INFO streaming.PipeMapRed: R/W/S=2800000/0/0 in:175000=2800000/16 [rec/s] out:0=0/16 [rec/s]\n",
      "16/01/30 20:43:05 INFO streaming.PipeMapRed: R/W/S=2900000/0/0 in:170588=2900000/17 [rec/s] out:0=0/17 [rec/s]\n",
      "16/01/30 20:43:06 INFO streaming.PipeMapRed: R/W/S=3000000/0/0 in:176470=3000000/17 [rec/s] out:0=0/17 [rec/s]\n",
      "16/01/30 20:43:07 INFO streaming.PipeMapRed: R/W/S=3100000/0/0 in:172222=3100000/18 [rec/s] out:0=0/18 [rec/s]\n",
      "16/01/30 20:43:07 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:07 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "16/01/30 20:43:07 INFO streaming.PipeMapRed: R/W/S=3200000/0/0 in:168421=3200000/19 [rec/s] out:0=0/19 [rec/s]\n",
      "16/01/30 20:43:08 INFO streaming.PipeMapRed: R/W/S=3300000/0/0 in:173684=3300000/19 [rec/s] out:0=0/19 [rec/s]\n",
      "16/01/30 20:43:09 INFO streaming.PipeMapRed: R/W/S=3400000/0/0 in:170000=3400000/20 [rec/s] out:0=0/20 [rec/s]\n",
      "16/01/30 20:43:10 INFO streaming.PipeMapRed: R/W/S=3500000/0/0 in:166666=3500000/21 [rec/s] out:0=0/21 [rec/s]\n",
      "16/01/30 20:43:10 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:10 INFO streaming.PipeMapRed: R/W/S=3600000/0/0 in:171428=3600000/21 [rec/s] out:0=0/21 [rec/s]\n",
      "16/01/30 20:43:10 INFO mapreduce.Job:  map 77% reduce 0%\n",
      "16/01/30 20:43:11 INFO streaming.PipeMapRed: R/W/S=3700000/0/0 in:168181=3700000/22 [rec/s] out:0=0/22 [rec/s]\n",
      "16/01/30 20:43:11 INFO streaming.PipeMapRed: R/W/S=3800000/0/0 in:172727=3800000/22 [rec/s] out:0=0/22 [rec/s]\n",
      "16/01/30 20:43:12 INFO streaming.PipeMapRed: R/W/S=3900000/0/0 in:169565=3900000/23 [rec/s] out:0=0/23 [rec/s]\n",
      "16/01/30 20:43:12 INFO streaming.PipeMapRed: R/W/S=4000000/0/0 in:173913=4000000/23 [rec/s] out:0=0/23 [rec/s]\n",
      "16/01/30 20:43:12 INFO streaming.PipeMapRed: R/W/S=4100000/0/0 in:178260=4100000/23 [rec/s] out:0=0/23 [rec/s]\n",
      "16/01/30 20:43:13 INFO streaming.PipeMapRed: R/W/S=4200000/0/0 in:175000=4200000/24 [rec/s] out:0=0/24 [rec/s]\n",
      "16/01/30 20:43:13 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:13 INFO streaming.PipeMapRed: R/W/S=4300000/0/0 in:179166=4300000/24 [rec/s] out:0=0/24 [rec/s]\n",
      "16/01/30 20:43:13 INFO mapreduce.Job:  map 78% reduce 0%\n",
      "16/01/30 20:43:14 INFO streaming.PipeMapRed: R/W/S=4400000/0/0 in:176000=4400000/25 [rec/s] out:0=0/25 [rec/s]\n",
      "16/01/30 20:43:14 INFO streaming.PipeMapRed: R/W/S=4500000/0/0 in:180000=4500000/25 [rec/s] out:0=0/25 [rec/s]\n",
      "16/01/30 20:43:14 INFO streaming.PipeMapRed: R/W/S=4600000/0/0 in:176923=4600000/26 [rec/s] out:0=0/26 [rec/s]\n",
      "16/01/30 20:43:15 INFO streaming.PipeMapRed: R/W/S=4700000/0/0 in:180769=4700000/26 [rec/s] out:0=0/26 [rec/s]\n",
      "16/01/30 20:43:15 INFO streaming.PipeMapRed: R/W/S=4800000/0/0 in:184615=4800000/26 [rec/s] out:0=0/26 [rec/s]\n",
      "16/01/30 20:43:16 INFO streaming.PipeMapRed: R/W/S=4900000/0/0 in:181481=4900000/27 [rec/s] out:0=0/27 [rec/s]\n",
      "16/01/30 20:43:16 INFO streaming.PipeMapRed: R/W/S=5000000/0/0 in:185185=5000000/27 [rec/s] out:0=0/27 [rec/s]\n",
      "16/01/30 20:43:16 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:16 INFO mapreduce.Job:  map 81% reduce 0%\n",
      "16/01/30 20:43:16 INFO streaming.PipeMapRed: R/W/S=5100000/0/0 in:182142=5100000/28 [rec/s] out:0=0/28 [rec/s]\n",
      "16/01/30 20:43:17 INFO streaming.PipeMapRed: R/W/S=5200000/0/0 in:185714=5200000/28 [rec/s] out:0=0/28 [rec/s]\n",
      "16/01/30 20:43:18 INFO streaming.PipeMapRed: R/W/S=5300000/0/0 in:182758=5300000/29 [rec/s] out:0=0/29 [rec/s]\n",
      "16/01/30 20:43:18 INFO streaming.PipeMapRed: R/W/S=5400000/0/0 in:186206=5400000/29 [rec/s] out:0=0/29 [rec/s]\n",
      "16/01/30 20:43:19 INFO streaming.PipeMapRed: R/W/S=5500000/0/0 in:183333=5500000/30 [rec/s] out:0=0/30 [rec/s]\n",
      "16/01/30 20:43:19 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:19 INFO mapreduce.Job:  map 82% reduce 0%\n",
      "16/01/30 20:43:20 INFO streaming.PipeMapRed: R/W/S=5600000/0/0 in:180645=5600000/31 [rec/s] out:0=0/31 [rec/s]\n",
      "16/01/30 20:43:20 INFO streaming.PipeMapRed: R/W/S=5700000/0/0 in:183870=5700000/31 [rec/s] out:0=0/31 [rec/s]\n",
      "16/01/30 20:43:21 INFO streaming.PipeMapRed: R/W/S=5800000/0/0 in:181250=5800000/32 [rec/s] out:0=0/32 [rec/s]\n",
      "16/01/30 20:43:21 INFO streaming.PipeMapRed: R/W/S=5900000/0/0 in:178787=5900000/33 [rec/s] out:0=0/33 [rec/s]\n",
      "16/01/30 20:43:22 INFO streaming.PipeMapRed: R/W/S=6000000/0/0 in:181818=6000000/33 [rec/s] out:0=0/33 [rec/s]\n",
      "16/01/30 20:43:22 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:22 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "16/01/30 20:43:23 INFO streaming.PipeMapRed: R/W/S=6100000/0/0 in:179411=6100000/34 [rec/s] out:0=0/34 [rec/s]\n",
      "16/01/30 20:43:24 INFO streaming.PipeMapRed: R/W/S=6200000/0/0 in:177142=6200000/35 [rec/s] out:0=0/35 [rec/s]\n",
      "16/01/30 20:43:25 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:25 INFO mapreduce.Job:  map 84% reduce 0%\n",
      "16/01/30 20:43:27 INFO streaming.PipeMapRed: R/W/S=6300000/0/0 in:165789=6300000/38 [rec/s] out:0=0/38 [rec/s]\n",
      "16/01/30 20:43:28 INFO streaming.PipeMapRed: R/W/S=6400000/0/0 in:164102=6400000/39 [rec/s] out:0=0/39 [rec/s]\n",
      "16/01/30 20:43:28 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:28 INFO streaming.PipeMapRed: R/W/S=6500000/0/0 in:166666=6500000/39 [rec/s] out:0=0/39 [rec/s]\n",
      "16/01/30 20:43:29 INFO streaming.PipeMapRed: R/W/S=6600000/0/0 in:165000=6600000/40 [rec/s] out:0=0/40 [rec/s]\n",
      "16/01/30 20:43:29 INFO streaming.PipeMapRed: R/W/S=6700000/0/0 in:163414=6700000/41 [rec/s] out:0=0/41 [rec/s]\n",
      "16/01/30 20:43:30 INFO streaming.PipeMapRed: R/W/S=6800000/0/0 in:165853=6800000/41 [rec/s] out:0=0/41 [rec/s]\n",
      "16/01/30 20:43:30 INFO streaming.PipeMapRed: R/W/S=6900000/0/0 in:164285=6900000/42 [rec/s] out:0=0/42 [rec/s]\n",
      "16/01/30 20:43:31 INFO streaming.PipeMapRed: R/W/S=7000000/0/0 in:166666=7000000/42 [rec/s] out:0=0/42 [rec/s]\n",
      "16/01/30 20:43:31 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:31 INFO mapreduce.Job:  map 86% reduce 0%\n",
      "16/01/30 20:43:32 INFO streaming.PipeMapRed: R/W/S=7100000/0/0 in:165116=7100000/43 [rec/s] out:0=0/43 [rec/s]\n",
      "16/01/30 20:43:32 INFO streaming.PipeMapRed: R/W/S=7200000/0/0 in:167441=7200000/43 [rec/s] out:0=0/43 [rec/s]\n",
      "16/01/30 20:43:33 INFO streaming.PipeMapRed: R/W/S=7300000/0/0 in:165909=7300000/44 [rec/s] out:0=0/44 [rec/s]\n",
      "16/01/30 20:43:33 INFO streaming.PipeMapRed: R/W/S=7400000/0/0 in:168181=7400000/44 [rec/s] out:0=0/44 [rec/s]\n",
      "16/01/30 20:43:34 INFO streaming.PipeMapRed: R/W/S=7500000/0/0 in:166666=7500000/45 [rec/s] out:0=0/45 [rec/s]\n",
      "16/01/30 20:43:34 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:34 INFO streaming.PipeMapRed: R/W/S=7600000/0/0 in:168888=7600000/45 [rec/s] out:0=0/45 [rec/s]\n",
      "16/01/30 20:43:34 INFO mapreduce.Job:  map 88% reduce 0%\n",
      "16/01/30 20:43:35 INFO streaming.PipeMapRed: R/W/S=7700000/0/0 in:167391=7700000/46 [rec/s] out:0=0/46 [rec/s]\n",
      "16/01/30 20:43:35 INFO streaming.PipeMapRed: R/W/S=7800000/0/0 in:169565=7800000/46 [rec/s] out:0=0/46 [rec/s]\n",
      "16/01/30 20:43:36 INFO streaming.PipeMapRed: R/W/S=7900000/0/0 in:168085=7900000/47 [rec/s] out:0=0/47 [rec/s]\n",
      "16/01/30 20:43:36 INFO streaming.PipeMapRed: R/W/S=8000000/0/0 in:170212=8000000/47 [rec/s] out:0=0/47 [rec/s]\n",
      "16/01/30 20:43:37 INFO streaming.PipeMapRed: R/W/S=8100000/0/0 in:168750=8100000/48 [rec/s] out:0=0/48 [rec/s]\n",
      "16/01/30 20:43:37 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:37 INFO streaming.PipeMapRed: R/W/S=8200000/0/0 in:170833=8200000/48 [rec/s] out:0=0/48 [rec/s]\n",
      "16/01/30 20:43:37 INFO mapreduce.Job:  map 89% reduce 0%\n",
      "16/01/30 20:43:38 INFO streaming.PipeMapRed: R/W/S=8300000/0/0 in:169387=8300000/49 [rec/s] out:0=0/49 [rec/s]\n",
      "16/01/30 20:43:38 INFO streaming.PipeMapRed: R/W/S=8400000/0/0 in:171428=8400000/49 [rec/s] out:0=0/49 [rec/s]\n",
      "16/01/30 20:43:39 INFO streaming.PipeMapRed: R/W/S=8500000/0/0 in:170000=8500000/50 [rec/s] out:0=0/50 [rec/s]\n",
      "16/01/30 20:43:39 INFO streaming.PipeMapRed: R/W/S=8600000/0/0 in:172000=8600000/50 [rec/s] out:0=0/50 [rec/s]\n",
      "16/01/30 20:43:40 INFO streaming.PipeMapRed: R/W/S=8700000/0/0 in:170588=8700000/51 [rec/s] out:0=0/51 [rec/s]\n",
      "16/01/30 20:43:40 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:40 INFO streaming.PipeMapRed: R/W/S=8800000/0/0 in:172549=8800000/51 [rec/s] out:0=0/51 [rec/s]\n",
      "16/01/30 20:43:40 INFO mapreduce.Job:  map 91% reduce 0%\n",
      "16/01/30 20:43:41 INFO streaming.PipeMapRed: R/W/S=8900000/0/0 in:171153=8900000/52 [rec/s] out:0=0/52 [rec/s]\n",
      "16/01/30 20:43:41 INFO streaming.PipeMapRed: R/W/S=9000000/0/0 in:173076=9000000/52 [rec/s] out:0=0/52 [rec/s]\n",
      "16/01/30 20:43:42 INFO streaming.PipeMapRed: R/W/S=9100000/0/0 in:171698=9100000/53 [rec/s] out:0=0/53 [rec/s]\n",
      "16/01/30 20:43:42 INFO streaming.PipeMapRed: R/W/S=9200000/0/0 in:173584=9200000/53 [rec/s] out:0=0/53 [rec/s]\n",
      "16/01/30 20:43:43 INFO streaming.PipeMapRed: R/W/S=9300000/0/0 in:172222=9300000/54 [rec/s] out:0=0/54 [rec/s]\n",
      "16/01/30 20:43:43 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:43 INFO streaming.PipeMapRed: R/W/S=9400000/0/0 in:174074=9400000/54 [rec/s] out:0=0/54 [rec/s]\n",
      "16/01/30 20:43:43 INFO mapreduce.Job:  map 93% reduce 0%\n",
      "16/01/30 20:43:44 INFO streaming.PipeMapRed: R/W/S=9500000/0/0 in:172727=9500000/55 [rec/s] out:0=0/55 [rec/s]\n",
      "16/01/30 20:43:44 INFO streaming.PipeMapRed: R/W/S=9600000/0/0 in:174545=9600000/55 [rec/s] out:0=0/55 [rec/s]\n",
      "16/01/30 20:43:45 INFO streaming.PipeMapRed: R/W/S=9700000/0/0 in:173214=9700000/56 [rec/s] out:0=0/56 [rec/s]\n",
      "16/01/30 20:43:45 INFO streaming.PipeMapRed: R/W/S=9800000/0/0 in:171929=9800000/57 [rec/s] out:0=0/57 [rec/s]\n",
      "16/01/30 20:43:46 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:46 INFO streaming.PipeMapRed: R/W/S=9900000/0/0 in:173684=9900000/57 [rec/s] out:0=0/57 [rec/s]\n",
      "16/01/30 20:43:46 INFO mapreduce.Job:  map 94% reduce 0%\n",
      "16/01/30 20:43:47 INFO streaming.PipeMapRed: R/W/S=10000000/0/0 in:172413=10000000/58 [rec/s] out:0=0/58 [rec/s]\n",
      "16/01/30 20:43:47 INFO streaming.PipeMapRed: R/W/S=10100000/0/0 in:174137=10100000/58 [rec/s] out:0=0/58 [rec/s]\n",
      "16/01/30 20:43:48 INFO streaming.PipeMapRed: R/W/S=10200000/0/0 in:172881=10200000/59 [rec/s] out:0=0/59 [rec/s]\n",
      "16/01/30 20:43:48 INFO streaming.PipeMapRed: R/W/S=10300000/0/0 in:174576=10300000/59 [rec/s] out:0=0/59 [rec/s]\n",
      "16/01/30 20:43:49 INFO streaming.PipeMapRed: R/W/S=10400000/0/0 in:173333=10400000/60 [rec/s] out:0=0/60 [rec/s]\n",
      "16/01/30 20:43:49 INFO streaming.PipeMapRed: R/W/S=10500000/0/0 in:175000=10500000/60 [rec/s] out:0=0/60 [rec/s]\n",
      "16/01/30 20:43:49 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:49 INFO mapreduce.Job:  map 96% reduce 0%\n",
      "16/01/30 20:43:49 INFO streaming.PipeMapRed: R/W/S=10600000/0/0 in:173770=10600000/61 [rec/s] out:0=0/61 [rec/s]\n",
      "16/01/30 20:43:50 INFO streaming.PipeMapRed: R/W/S=10700000/0/0 in:175409=10700000/61 [rec/s] out:0=0/61 [rec/s]\n",
      "16/01/30 20:43:50 INFO streaming.PipeMapRed: R/W/S=10800000/0/0 in:177049=10800000/61 [rec/s] out:0=0/61 [rec/s]\n",
      "16/01/30 20:43:51 INFO streaming.PipeMapRed: R/W/S=10900000/0/0 in:175806=10900000/62 [rec/s] out:0=0/62 [rec/s]\n",
      "16/01/30 20:43:51 INFO streaming.PipeMapRed: R/W/S=11000000/0/0 in:174603=11000000/63 [rec/s] out:0=0/63 [rec/s]\n",
      "16/01/30 20:43:52 INFO streaming.PipeMapRed: R/W/S=11100000/0/0 in:176190=11100000/63 [rec/s] out:0=0/63 [rec/s]\n",
      "16/01/30 20:43:52 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:52 INFO mapreduce.Job:  map 97% reduce 0%\n",
      "16/01/30 20:43:53 INFO streaming.PipeMapRed: R/W/S=11200000/0/0 in:175000=11200000/64 [rec/s] out:0=0/64 [rec/s]\n",
      "16/01/30 20:43:54 INFO streaming.PipeMapRed: R/W/S=11300000/0/0 in:173846=11300000/65 [rec/s] out:0=0/65 [rec/s]\n",
      "16/01/30 20:43:54 INFO streaming.PipeMapRed: R/W/S=11400000/0/0 in:175384=11400000/65 [rec/s] out:0=0/65 [rec/s]\n",
      "16/01/30 20:43:55 INFO streaming.PipeMapRed: R/W/S=11500000/0/0 in:174242=11500000/66 [rec/s] out:0=0/66 [rec/s]\n",
      "16/01/30 20:43:55 INFO mapred.LocalJobRunner: Records R/W=467530/1 > sort > \n",
      "16/01/30 20:43:55 INFO mapreduce.Job:  map 98% reduce 0%\n",
      "16/01/30 20:43:55 INFO streaming.PipeMapRed: R/W/S=11600000/0/0 in:173134=11600000/67 [rec/s] out:0=0/67 [rec/s]\n",
      "16/01/30 20:43:56 INFO streaming.PipeMapRed: R/W/S=11700000/0/0 in:174626=11700000/67 [rec/s] out:0=0/67 [rec/s]\n",
      "16/01/30 20:43:56 INFO streaming.PipeMapRed: R/W/S=11800000/0/0 in:173529=11800000/68 [rec/s] out:0=0/68 [rec/s]\n",
      "16/01/30 20:43:57 INFO streaming.PipeMapRed: R/W/S=11900000/0/0 in:175000=11900000/68 [rec/s] out:0=0/68 [rec/s]\n",
      "16/01/30 20:43:57 INFO streaming.PipeMapRed: R/W/S=12000000/0/0 in:176470=12000000/68 [rec/s] out:0=0/68 [rec/s]\n",
      "16/01/30 20:43:58 INFO streaming.PipeMapRed: Records R/W=12070340/1\n",
      "16/01/30 20:43:58 INFO mapred.LocalJobRunner: Records R/W=12070340/1 > sort > \n",
      "16/01/30 20:43:58 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/30 20:44:01 INFO mapred.LocalJobRunner: Records R/W=12070340/1 > sort > \n",
      "16/01/30 20:44:04 INFO mapred.LocalJobRunner: Records R/W=12070340/1 > sort > \n",
      "16/01/30 20:44:07 INFO mapred.LocalJobRunner: Records R/W=12070340/1 > sort > \n",
      "16/01/30 20:44:08 INFO streaming.PipeMapRed: Records R/W=12070340/5951298\n",
      "16/01/30 20:44:10 INFO mapred.LocalJobRunner: Records R/W=12070340/5951298 > sort > \n",
      "16/01/30 20:44:13 INFO mapred.LocalJobRunner: Records R/W=12070340/5951298 > sort > \n",
      "16/01/30 20:44:16 INFO mapred.LocalJobRunner: Records R/W=12070340/5951298 > sort > \n",
      "16/01/30 20:44:19 INFO mapred.LocalJobRunner: Records R/W=12070340/5951298 > sort > \n",
      "16/01/30 20:44:19 INFO streaming.PipeMapRed: Records R/W=12070340/10092674\n",
      "16/01/30 20:44:19 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 20:44:19 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 20:44:19 INFO mapred.Task: Task:attempt_local583153488_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/30 20:44:19 INFO mapred.LocalJobRunner: Records R/W=12070340/10092674 > sort\n",
      "16/01/30 20:44:19 INFO mapred.Task: Task 'attempt_local583153488_0001_m_000000_0' done.\n",
      "16/01/30 20:44:19 INFO mapred.LocalJobRunner: Finishing task: attempt_local583153488_0001_m_000000_0\n",
      "16/01/30 20:44:19 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/30 20:44:19 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/30 20:44:19 INFO mapred.LocalJobRunner: Starting task: attempt_local583153488_0001_r_000000_0\n",
      "16/01/30 20:44:19 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 20:44:19 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 20:44:19 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 20:44:20 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6f95197a\n",
      "16/01/30 20:44:20 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/30 20:44:20 INFO reduce.EventFetcher: attempt_local583153488_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/30 20:44:20 INFO reduce.MergeManagerImpl: attempt_local583153488_0001_m_000000_0: Shuffling to disk since 403369367 is greater than maxSingleShuffleLimit (83584616)\n",
      "16/01/30 20:44:20 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local583153488_0001_m_000000_0 decomp: 403369367 len: 403369371 to DISK\n",
      "16/01/30 20:44:23 INFO reduce.OnDiskMapOutput: Read 403369371 bytes from map-output for attempt_local583153488_0001_m_000000_0\n",
      "16/01/30 20:44:23 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/30 20:44:23 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 20:44:23 INFO reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 1 on-disk map-outputs\n",
      "16/01/30 20:44:23 INFO reduce.MergeManagerImpl: Merging 1 files, 403369371 bytes from disk\n",
      "16/01/30 20:44:23 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/30 20:44:23 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/30 20:44:23 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 403369328 bytes\n",
      "16/01/30 20:44:23 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 20:44:23 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/john/Dropbox/MIDS/W261/Week3/Homework3/notebook/./reducerQ37.py]\n",
      "16/01/30 20:44:23 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/30 20:44:23 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/30 20:44:23 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:44:23 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:44:23 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:44:23 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:44:23 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:44:24 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 20:44:24 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:200000=200000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:44:25 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 20:44:25 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:200000=400000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:44:26 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:44:26 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:250000=500000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:44:26 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:300000=600000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/01/30 20:44:27 INFO mapreduce.Job:  map 100% reduce 68%\n",
      "16/01/30 20:44:27 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:233333=700000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:44:27 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:266666=800000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/01/30 20:44:27 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:225000=900000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:44:28 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:250000=1000000/4 [rec/s] out:0=0/4 [rec/s]\n",
      "16/01/30 20:44:28 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:220000=1100000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:44:29 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:44:29 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:240000=1200000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:44:29 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:260000=1300000/5 [rec/s] out:0=0/5 [rec/s]\n",
      "16/01/30 20:44:29 INFO streaming.PipeMapRed: R/W/S=1400000/0/0 in:233333=1400000/6 [rec/s] out:0=0/6 [rec/s]\n",
      "16/01/30 20:44:30 INFO mapreduce.Job:  map 100% reduce 71%\n",
      "16/01/30 20:44:30 INFO streaming.PipeMapRed: R/W/S=1500000/0/0 in:250000=1500000/6 [rec/s] out:0=0/6 [rec/s]\n",
      "16/01/30 20:44:30 INFO streaming.PipeMapRed: R/W/S=1600000/0/0 in:228571=1600000/7 [rec/s] out:0=0/7 [rec/s]\n",
      "16/01/30 20:44:31 INFO streaming.PipeMapRed: R/W/S=1700000/0/0 in:212500=1700000/8 [rec/s] out:0=0/8 [rec/s]\n",
      "16/01/30 20:44:32 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:44:32 INFO mapreduce.Job:  map 100% reduce 72%\n",
      "16/01/30 20:44:32 INFO streaming.PipeMapRed: R/W/S=1800000/0/0 in:225000=1800000/8 [rec/s] out:0=0/8 [rec/s]\n",
      "16/01/30 20:44:32 INFO streaming.PipeMapRed: R/W/S=1900000/0/0 in:211111=1900000/9 [rec/s] out:0=0/9 [rec/s]\n",
      "16/01/30 20:44:33 INFO streaming.PipeMapRed: R/W/S=2000000/0/0 in:222222=2000000/9 [rec/s] out:0=0/9 [rec/s]\n",
      "16/01/30 20:44:33 INFO streaming.PipeMapRed: R/W/S=2100000/0/0 in:233333=2100000/9 [rec/s] out:0=0/9 [rec/s]\n",
      "16/01/30 20:44:34 INFO streaming.PipeMapRed: R/W/S=2200000/0/0 in:220000=2200000/10 [rec/s] out:0=0/10 [rec/s]\n",
      "16/01/30 20:44:34 INFO streaming.PipeMapRed: R/W/S=2300000/0/0 in:230000=2300000/10 [rec/s] out:0=0/10 [rec/s]\n",
      "16/01/30 20:44:34 INFO streaming.PipeMapRed: R/W/S=2400000/0/0 in:218181=2400000/11 [rec/s] out:0=0/11 [rec/s]\n",
      "16/01/30 20:44:35 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:44:35 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "16/01/30 20:44:35 INFO streaming.PipeMapRed: R/W/S=2500000/0/0 in:227272=2500000/11 [rec/s] out:0=0/11 [rec/s]\n",
      "16/01/30 20:44:36 INFO streaming.PipeMapRed: R/W/S=2600000/0/0 in:216666=2600000/12 [rec/s] out:0=0/12 [rec/s]\n",
      "16/01/30 20:44:36 INFO streaming.PipeMapRed: R/W/S=2700000/0/0 in:207692=2700000/13 [rec/s] out:0=0/13 [rec/s]\n",
      "16/01/30 20:44:37 INFO streaming.PipeMapRed: R/W/S=2800000/0/0 in:215384=2800000/13 [rec/s] out:0=0/13 [rec/s]\n",
      "16/01/30 20:44:38 INFO streaming.PipeMapRed: R/W/S=2900000/0/0 in:207142=2900000/14 [rec/s] out:0=0/14 [rec/s]\n",
      "16/01/30 20:44:38 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:44:38 INFO mapreduce.Job:  map 100% reduce 76%\n",
      "16/01/30 20:44:38 INFO streaming.PipeMapRed: R/W/S=3000000/0/0 in:214285=3000000/14 [rec/s] out:0=0/14 [rec/s]\n",
      "16/01/30 20:44:39 INFO streaming.PipeMapRed: R/W/S=3100000/0/0 in:206666=3100000/15 [rec/s] out:0=0/15 [rec/s]\n",
      "16/01/30 20:44:39 INFO streaming.PipeMapRed: R/W/S=3200000/0/0 in:213333=3200000/15 [rec/s] out:0=0/15 [rec/s]\n",
      "16/01/30 20:44:39 INFO streaming.PipeMapRed: R/W/S=3300000/0/0 in:206250=3300000/16 [rec/s] out:0=0/16 [rec/s]\n",
      "16/01/30 20:44:40 INFO streaming.PipeMapRed: R/W/S=3400000/0/0 in:212500=3400000/16 [rec/s] out:0=0/16 [rec/s]\n",
      "16/01/30 20:44:40 INFO streaming.PipeMapRed: R/W/S=3500000/0/0 in:218750=3500000/16 [rec/s] out:0=0/16 [rec/s]\n",
      "16/01/30 20:44:40 INFO streaming.PipeMapRed: R/W/S=3600000/0/0 in:211764=3600000/17 [rec/s] out:0=0/17 [rec/s]\n",
      "16/01/30 20:44:41 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:44:41 INFO mapreduce.Job:  map 100% reduce 79%\n",
      "16/01/30 20:44:41 INFO streaming.PipeMapRed: R/W/S=3700000/0/0 in:217647=3700000/17 [rec/s] out:0=0/17 [rec/s]\n",
      "16/01/30 20:44:41 INFO streaming.PipeMapRed: R/W/S=3800000/0/0 in:223529=3800000/17 [rec/s] out:0=0/17 [rec/s]\n",
      "16/01/30 20:44:42 INFO streaming.PipeMapRed: R/W/S=3900000/0/0 in:216666=3900000/18 [rec/s] out:0=0/18 [rec/s]\n",
      "16/01/30 20:44:42 INFO streaming.PipeMapRed: R/W/S=4000000/0/0 in:222222=4000000/18 [rec/s] out:0=0/18 [rec/s]\n",
      "16/01/30 20:44:43 INFO streaming.PipeMapRed: R/W/S=4100000/0/0 in:215789=4100000/19 [rec/s] out:0=0/19 [rec/s]\n",
      "16/01/30 20:44:43 INFO streaming.PipeMapRed: R/W/S=4200000/0/0 in:210000=4200000/20 [rec/s] out:0=0/20 [rec/s]\n",
      "16/01/30 20:44:44 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:44:44 INFO mapreduce.Job:  map 100% reduce 81%\n",
      "16/01/30 20:44:44 INFO streaming.PipeMapRed: R/W/S=4300000/0/0 in:215000=4300000/20 [rec/s] out:0=0/20 [rec/s]\n",
      "16/01/30 20:44:44 INFO streaming.PipeMapRed: R/W/S=4400000/0/0 in:209523=4400000/21 [rec/s] out:0=0/21 [rec/s]\n",
      "16/01/30 20:44:45 INFO streaming.PipeMapRed: R/W/S=4500000/0/0 in:214285=4500000/21 [rec/s] out:0=0/21 [rec/s]\n",
      "16/01/30 20:44:45 INFO streaming.PipeMapRed: R/W/S=4600000/0/0 in:219047=4600000/21 [rec/s] out:0=0/21 [rec/s]\n",
      "16/01/30 20:44:45 INFO streaming.PipeMapRed: R/W/S=4700000/0/0 in:213636=4700000/22 [rec/s] out:0=0/22 [rec/s]\n",
      "16/01/30 20:44:46 INFO streaming.PipeMapRed: R/W/S=4800000/0/0 in:218181=4800000/22 [rec/s] out:0=0/22 [rec/s]\n",
      "16/01/30 20:44:46 INFO streaming.PipeMapRed: R/W/S=4900000/0/0 in:213043=4900000/23 [rec/s] out:0=0/23 [rec/s]\n",
      "16/01/30 20:44:47 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:44:47 INFO mapreduce.Job:  map 100% reduce 83%\n",
      "16/01/30 20:44:47 INFO streaming.PipeMapRed: R/W/S=5000000/0/0 in:217391=5000000/23 [rec/s] out:0=0/23 [rec/s]\n",
      "16/01/30 20:44:47 INFO streaming.PipeMapRed: R/W/S=5100000/0/0 in:221739=5100000/23 [rec/s] out:0=0/23 [rec/s]\n",
      "16/01/30 20:44:48 INFO streaming.PipeMapRed: R/W/S=5200000/0/0 in:216666=5200000/24 [rec/s] out:0=0/24 [rec/s]\n",
      "16/01/30 20:44:49 INFO streaming.PipeMapRed: R/W/S=5300000/0/0 in:212000=5300000/25 [rec/s] out:0=0/25 [rec/s]\n",
      "16/01/30 20:44:50 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:44:50 INFO mapreduce.Job:  map 100% reduce 84%\n",
      "16/01/30 20:44:50 INFO streaming.PipeMapRed: R/W/S=5400000/0/0 in:207692=5400000/26 [rec/s] out:0=0/26 [rec/s]\n",
      "16/01/30 20:44:51 INFO streaming.PipeMapRed: R/W/S=5500000/0/0 in:203703=5500000/27 [rec/s] out:0=0/27 [rec/s]\n",
      "16/01/30 20:44:53 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:44:53 INFO mapreduce.Job:  map 100% reduce 85%\n",
      "16/01/30 20:44:56 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:44:57 INFO streaming.PipeMapRed: R/W/S=5600000/0/0 in:169696=5600000/33 [rec/s] out:0=0/33 [rec/s]\n",
      "16/01/30 20:44:58 INFO streaming.PipeMapRed: R/W/S=5700000/0/0 in:167647=5700000/34 [rec/s] out:0=0/34 [rec/s]\n",
      "16/01/30 20:44:59 INFO streaming.PipeMapRed: R/W/S=5800000/0/0 in:165714=5800000/35 [rec/s] out:0=0/35 [rec/s]\n",
      "16/01/30 20:44:59 INFO streaming.PipeMapRed: R/W/S=5900000/0/0 in:163888=5900000/36 [rec/s] out:0=0/36 [rec/s]\n",
      "16/01/30 20:45:00 INFO streaming.PipeMapRed: R/W/S=6000000/0/0 in:162162=6000000/37 [rec/s] out:0=0/37 [rec/s]\n",
      "16/01/30 20:45:01 INFO streaming.PipeMapRed: R/W/S=6100000/0/0 in:164864=6100000/37 [rec/s] out:0=0/37 [rec/s]\n",
      "16/01/30 20:45:01 INFO streaming.PipeMapRed: R/W/S=6200000/0/0 in:163157=6200000/38 [rec/s] out:0=0/38 [rec/s]\n",
      "16/01/30 20:45:02 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:45:02 INFO mapreduce.Job:  map 100% reduce 87%\n",
      "16/01/30 20:45:02 INFO streaming.PipeMapRed: R/W/S=6300000/0/0 in:165789=6300000/38 [rec/s] out:0=0/38 [rec/s]\n",
      "16/01/30 20:45:03 INFO streaming.PipeMapRed: R/W/S=6400000/0/0 in:164102=6400000/39 [rec/s] out:0=0/39 [rec/s]\n",
      "16/01/30 20:45:03 INFO streaming.PipeMapRed: R/W/S=6500000/0/0 in:162500=6500000/40 [rec/s] out:0=0/40 [rec/s]\n",
      "16/01/30 20:45:04 INFO streaming.PipeMapRed: R/W/S=6600000/0/0 in:165000=6600000/40 [rec/s] out:0=0/40 [rec/s]\n",
      "16/01/30 20:45:04 INFO streaming.PipeMapRed: R/W/S=6700000/0/0 in:167500=6700000/40 [rec/s] out:0=0/40 [rec/s]\n",
      "16/01/30 20:45:04 INFO streaming.PipeMapRed: R/W/S=6800000/0/0 in:165853=6800000/41 [rec/s] out:0=0/41 [rec/s]\n",
      "16/01/30 20:45:05 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:45:05 INFO mapreduce.Job:  map 100% reduce 89%\n",
      "16/01/30 20:45:05 INFO streaming.PipeMapRed: R/W/S=6900000/0/0 in:168292=6900000/41 [rec/s] out:0=0/41 [rec/s]\n",
      "16/01/30 20:45:05 INFO streaming.PipeMapRed: R/W/S=7000000/0/0 in:170731=7000000/41 [rec/s] out:0=0/41 [rec/s]\n",
      "16/01/30 20:45:06 INFO streaming.PipeMapRed: R/W/S=7100000/0/0 in:169047=7100000/42 [rec/s] out:0=0/42 [rec/s]\n",
      "16/01/30 20:45:06 INFO streaming.PipeMapRed: R/W/S=7200000/0/0 in:167441=7200000/43 [rec/s] out:0=0/43 [rec/s]\n",
      "16/01/30 20:45:07 INFO streaming.PipeMapRed: R/W/S=7300000/0/0 in:169767=7300000/43 [rec/s] out:0=0/43 [rec/s]\n",
      "16/01/30 20:45:07 INFO streaming.PipeMapRed: R/W/S=7400000/0/0 in:172093=7400000/43 [rec/s] out:0=0/43 [rec/s]\n",
      "16/01/30 20:45:08 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:45:08 INFO mapreduce.Job:  map 100% reduce 91%\n",
      "16/01/30 20:45:08 INFO streaming.PipeMapRed: R/W/S=7500000/0/0 in:170454=7500000/44 [rec/s] out:0=0/44 [rec/s]\n",
      "16/01/30 20:45:08 INFO streaming.PipeMapRed: R/W/S=7600000/0/0 in:168888=7600000/45 [rec/s] out:0=0/45 [rec/s]\n",
      "16/01/30 20:45:09 INFO streaming.PipeMapRed: R/W/S=7700000/0/0 in:171111=7700000/45 [rec/s] out:0=0/45 [rec/s]\n",
      "16/01/30 20:45:10 INFO streaming.PipeMapRed: R/W/S=7800000/0/0 in:169565=7800000/46 [rec/s] out:0=0/46 [rec/s]\n",
      "16/01/30 20:45:10 INFO streaming.PipeMapRed: R/W/S=7900000/0/0 in:168085=7900000/47 [rec/s] out:0=0/47 [rec/s]\n",
      "16/01/30 20:45:11 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:45:11 INFO mapreduce.Job:  map 100% reduce 93%\n",
      "16/01/30 20:45:11 INFO streaming.PipeMapRed: R/W/S=8000000/0/0 in:170212=8000000/47 [rec/s] out:0=0/47 [rec/s]\n",
      "16/01/30 20:45:11 INFO streaming.PipeMapRed: R/W/S=8100000/0/0 in:168750=8100000/48 [rec/s] out:0=0/48 [rec/s]\n",
      "16/01/30 20:45:12 INFO streaming.PipeMapRed: R/W/S=8200000/0/0 in:170833=8200000/48 [rec/s] out:0=0/48 [rec/s]\n",
      "16/01/30 20:45:13 INFO streaming.PipeMapRed: R/W/S=8300000/0/0 in:169387=8300000/49 [rec/s] out:0=0/49 [rec/s]\n",
      "16/01/30 20:45:13 INFO streaming.PipeMapRed: R/W/S=8400000/0/0 in:168000=8400000/50 [rec/s] out:0=0/50 [rec/s]\n",
      "16/01/30 20:45:14 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:45:14 INFO mapreduce.Job:  map 100% reduce 94%\n",
      "16/01/30 20:45:14 INFO streaming.PipeMapRed: R/W/S=8500000/0/0 in:166666=8500000/51 [rec/s] out:0=0/51 [rec/s]\n",
      "16/01/30 20:45:15 INFO streaming.PipeMapRed: R/W/S=8600000/0/0 in:168627=8600000/51 [rec/s] out:0=0/51 [rec/s]\n",
      "16/01/30 20:45:15 INFO streaming.PipeMapRed: R/W/S=8700000/0/0 in:167307=8700000/52 [rec/s] out:0=0/52 [rec/s]\n",
      "16/01/30 20:45:16 INFO streaming.PipeMapRed: R/W/S=8800000/0/0 in:169230=8800000/52 [rec/s] out:0=0/52 [rec/s]\n",
      "16/01/30 20:45:17 INFO streaming.PipeMapRed: R/W/S=8900000/0/0 in:167924=8900000/53 [rec/s] out:0=0/53 [rec/s]\n",
      "16/01/30 20:45:17 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:45:17 INFO mapreduce.Job:  map 100% reduce 96%\n",
      "16/01/30 20:45:17 INFO streaming.PipeMapRed: R/W/S=9000000/0/0 in:166666=9000000/54 [rec/s] out:0=0/54 [rec/s]\n",
      "16/01/30 20:45:18 INFO streaming.PipeMapRed: R/W/S=9100000/0/0 in:168518=9100000/54 [rec/s] out:0=0/54 [rec/s]\n",
      "16/01/30 20:45:19 INFO streaming.PipeMapRed: R/W/S=9200000/0/0 in:167272=9200000/55 [rec/s] out:0=0/55 [rec/s]\n",
      "16/01/30 20:45:19 INFO streaming.PipeMapRed: R/W/S=9300000/0/0 in:166071=9300000/56 [rec/s] out:0=0/56 [rec/s]\n",
      "16/01/30 20:45:20 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:45:20 INFO mapreduce.Job:  map 100% reduce 98%\n",
      "16/01/30 20:45:20 INFO streaming.PipeMapRed: R/W/S=9400000/0/0 in:167857=9400000/56 [rec/s] out:0=0/56 [rec/s]\n",
      "16/01/30 20:45:21 INFO streaming.PipeMapRed: R/W/S=9500000/0/0 in:166666=9500000/57 [rec/s] out:0=0/57 [rec/s]\n",
      "16/01/30 20:45:22 INFO streaming.PipeMapRed: R/W/S=9600000/0/0 in:165517=9600000/58 [rec/s] out:0=0/58 [rec/s]\n",
      "16/01/30 20:45:22 INFO streaming.PipeMapRed: R/W/S=9700000/0/0 in:164406=9700000/59 [rec/s] out:0=0/59 [rec/s]\n",
      "16/01/30 20:45:23 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:45:23 INFO mapreduce.Job:  map 100% reduce 99%\n",
      "16/01/30 20:45:23 INFO streaming.PipeMapRed: R/W/S=9800000/0/0 in:166101=9800000/59 [rec/s] out:0=0/59 [rec/s]\n",
      "16/01/30 20:45:23 INFO streaming.PipeMapRed: R/W/S=9900000/0/0 in:165000=9900000/60 [rec/s] out:0=0/60 [rec/s]\n",
      "16/01/30 20:45:24 INFO streaming.PipeMapRed: R/W/S=10000000/0/0 in:166666=10000000/60 [rec/s] out:0=0/60 [rec/s]\n",
      "16/01/30 20:45:26 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:45:26 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/30 20:45:29 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:45:39 INFO streaming.PipeMapRed: Records R/W=10092961/1\n",
      "16/01/30 20:45:39 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 20:45:39 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 20:45:39 INFO mapred.Task: Task:attempt_local583153488_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/30 20:45:39 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/01/30 20:45:39 INFO mapred.Task: Task attempt_local583153488_0001_r_000000_0 is allowed to commit now\n",
      "16/01/30 20:45:39 INFO output.FileOutputCommitter: Saved output of task 'attempt_local583153488_0001_r_000000_0' to hdfs://localhost:9000/user/john/notebook/output/_temporary/0/task_local583153488_0001_r_000000\n",
      "16/01/30 20:45:39 INFO mapred.LocalJobRunner: Records R/W=10092961/1 > reduce\n",
      "16/01/30 20:45:39 INFO mapred.Task: Task 'attempt_local583153488_0001_r_000000_0' done.\n",
      "16/01/30 20:45:39 INFO mapred.LocalJobRunner: Finishing task: attempt_local583153488_0001_r_000000_0\n",
      "16/01/30 20:45:39 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/30 20:45:40 INFO mapreduce.Job: Job job_local583153488_0001 completed successfully\n",
      "16/01/30 20:45:40 INFO mapreduce.Job: Counters: 36\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1762727936\n",
      "\t\tFILE: Number of bytes written=2166684757\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6917034\n",
      "\t\tHDFS: Number of bytes written=8666\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=14728700\n",
      "\t\tMap output bytes=544010616\n",
      "\t\tMap output materialized bytes=403369371\n",
      "\t\tInput split bytes=122\n",
      "\t\tCombine input records=26799040\n",
      "\t\tCombine output records=22163301\n",
      "\t\tReduce input groups=10092961\n",
      "\t\tReduce shuffle bytes=403369371\n",
      "\t\tReduce input records=10092961\n",
      "\t\tReduce output records=108\n",
      "\t\tSpilled Records=32256262\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=52\n",
      "\t\tTotal committed heap usage (bytes)=526385152\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tUser-Defined\n",
      "\t\tNumber of Combiners=11\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=8666\n",
      "16/01/30 20:45:40 INFO streaming.StreamJob: Output directory: /user/john/notebook/output\n",
      "16/01/30 20:45:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\t\n",
      "========== Most Frequent Doubles ==========\t\n",
      "\t\n",
      "PAIR                          |SUPPORT COUNT       |SUPPORT             \t\n",
      "('DAI62779', 'ELE17451')      |                1592|                 100\t\n",
      "('FRO40251', 'SNA80324')      |                1412|                 100\t\n",
      "('DAI75645', 'FRO40251')      |                1254|                 100\t\n",
      "('FRO40251', 'GRO85051')      |                1213|                 100\t\n",
      "('DAI62779', 'GRO73461')      |                1139|                 100\t\n",
      "('DAI75645', 'SNA80324')      |                1130|                 100\t\n",
      "('DAI62779', 'FRO40251')      |                1070|                 100\t\n",
      "('DAI62779', 'SNA80324')      |                 923|                 100\t\n",
      "('DAI62779', 'DAI85309')      |                 918|                 100\t\n",
      "('ELE32164', 'GRO59710')      |                 911|                 100\t\n",
      "('DAI62779', 'DAI75645')      |                 882|                 100\t\n",
      "('FRO40251', 'GRO73461')      |                 882|                 100\t\n",
      "('DAI62779', 'ELE92920')      |                 877|                 100\t\n",
      "('FRO40251', 'FRO92469')      |                 835|                 100\t\n",
      "('DAI62779', 'ELE32164')      |                 832|                 100\t\n",
      "('DAI75645', 'GRO73461')      |                 712|                 100\t\n",
      "('DAI43223', 'ELE32164')      |                 711|                 100\t\n",
      "('DAI62779', 'GRO30386')      |                 709|                 100\t\n",
      "('ELE17451', 'FRO40251')      |                 697|                 100\t\n",
      "('DAI85309', 'ELE99737')      |                 659|                 100\t\n",
      "('DAI62779', 'ELE26917')      |                 650|                 100\t\n",
      "('GRO21487', 'GRO73461')      |                 631|                 100\t\n",
      "('DAI62779', 'SNA45677')      |                 604|                 100\t\n",
      "('ELE17451', 'SNA80324')      |                 597|                 100\t\n",
      "('DAI62779', 'GRO71621')      |                 595|                 100\t\n",
      "('DAI62779', 'SNA55762')      |                 593|                 100\t\n",
      "('DAI62779', 'DAI83733')      |                 586|                 100\t\n",
      "('ELE17451', 'GRO73461')      |                 580|                 100\t\n",
      "('GRO73461', 'SNA80324')      |                 562|                 100\t\n",
      "('DAI62779', 'GRO59710')      |                 561|                 100\t\n",
      "('DAI62779', 'FRO80039')      |                 550|                 100\t\n",
      "('DAI75645', 'ELE17451')      |                 547|                 100\t\n",
      "('DAI62779', 'SNA93860')      |                 537|                 100\t\n",
      "('DAI55148', 'DAI62779')      |                 526|                 100\t\n",
      "('DAI43223', 'GRO59710')      |                 512|                 100\t\n",
      "('ELE17451', 'ELE32164')      |                 511|                 100\t\n",
      "('DAI62779', 'SNA18336')      |                 506|                 100\t\n",
      "('ELE32164', 'GRO73461')      |                 486|                 100\t\n",
      "('DAI85309', 'ELE17451')      |                 482|                 100\t\n",
      "('DAI62779', 'FRO78087')      |                 482|                 100\t\n",
      "('DAI62779', 'GRO94758')      |                 479|                 100\t\n",
      "('GRO85051', 'SNA80324')      |                 471|                 100\t\n",
      "('DAI62779', 'GRO21487')      |                 471|                 100\t\n",
      "('ELE17451', 'GRO30386')      |                 468|                 100\t\n",
      "('FRO85978', 'SNA95666')      |                 463|                 100\t\n",
      "('DAI62779', 'FRO19221')      |                 462|                 100\t\n",
      "('DAI62779', 'GRO46854')      |                 461|                 100\t\n",
      "('DAI43223', 'DAI62779')      |                 459|                 100\t\n",
      "('ELE92920', 'SNA18336')      |                 455|                 100\t\n",
      "('DAI88079', 'FRO40251')      |                 446|                 100\t\n",
      "\t\n",
      "========== Most Frequent Triples ==========\t\n",
      "\t\n",
      "PAIR                                              |SUPPORT COUNT       |SUPPORT             \t\n",
      "('DAI75645', 'FRO40251', 'SNA80324')              |                 550|                 100\t\n",
      "('DAI62779', 'FRO40251', 'SNA80324')              |                 476|                 100\t\n",
      "('FRO40251', 'GRO85051', 'SNA80324')              |                 471|                 100\t\n",
      "('DAI62779', 'ELE92920', 'SNA18336')              |                 432|                 100\t\n",
      "('DAI62779', 'DAI75645', 'SNA80324')              |                 421|                 100\t\n",
      "('DAI62779', 'ELE17451', 'SNA80324')              |                 417|                 100\t\n",
      "('DAI62779', 'DAI75645', 'FRO40251')              |                 412|                 100\t\n",
      "('DAI62779', 'ELE17451', 'FRO40251')              |                 406|                 100\t\n",
      "('DAI75645', 'FRO40251', 'GRO85051')              |                 395|                 100\t\n",
      "('DAI62779', 'FRO40251', 'GRO85051')              |                 381|                 100\t\n",
      "('ELE17451', 'FRO40251', 'SNA80324')              |                 353|                 100\t\n",
      "('DAI62779', 'ELE17451', 'ELE92920')              |                 345|                 100\t\n",
      "('FRO40251', 'FRO92469', 'SNA80324')              |                 343|                 100\t\n",
      "('DAI62779', 'DAI85309', 'ELE17451')              |                 339|                 100\t\n",
      "('DAI62779', 'DAI75645', 'ELE17451')              |                 328|                 100\t\n",
      "('DAI62779', 'FRO40251', 'GRO73461')              |                 315|                 100\t\n",
      "('DAI62779', 'ELE32164', 'GRO59710')              |                 301|                 100\t\n",
      "('DAI75645', 'ELE17451', 'SNA80324')              |                 300|                 100\t\n",
      "('DAI75645', 'FRO40251', 'GRO73461')              |                 293|                 100\t\n",
      "('DAI75645', 'ELE17451', 'FRO40251')              |                 292|                 100\t\n",
      "('DAI43223', 'DAI62779', 'ELE32164')              |                 287|                 100\t\n",
      "('DAI43223', 'ELE32164', 'GRO59710')              |                 287|                 100\t\n",
      "('DAI62779', 'ELE17451', 'ELE32164')              |                 277|                 100\t\n",
      "('DAI62779', 'DAI85309', 'ELE99737')              |                 272|                 100\t\n",
      "('DAI62779', 'DAI75645', 'GRO73461')              |                 261|                 100\t\n",
      "('DAI75645', 'FRO40251', 'FRO92469')              |                 251|                 100\t\n",
      "('DAI62779', 'ELE17451', 'GRO73461')              |                 245|                 100\t\n",
      "('DAI62779', 'ELE17451', 'SNA18336')              |                 244|                 100\t\n",
      "('DAI62779', 'FRO40251', 'FRO92469')              |                 238|                 100\t\n",
      "('ELE20847', 'FRO40251', 'SNA80324')              |                 232|                 100\t\n",
      "('FRO40251', 'GRO73461', 'SNA80324')              |                 232|                 100\t\n",
      "('DAI75645', 'GRO73461', 'SNA80324')              |                 230|                 100\t\n",
      "('ELE17451', 'ELE92920', 'SNA18336')              |                 228|                 100\t\n",
      "('DAI43223', 'DAI62779', 'ELE17451')              |                 227|                 100\t\n",
      "('DAI62779', 'ELE17451', 'GRO30386')              |                 218|                 100\t\n",
      "('ELE17451', 'FRO40251', 'GRO85051')              |                 217|                 100\t\n",
      "('DAI62779', 'ELE17451', 'GRO59710')              |                 213|                 100\t\n",
      "('FRO40251', 'FRO92469', 'GRO73461')              |                 211|                 100\t\n",
      "('DAI43223', 'ELE17451', 'ELE32164')              |                 206|                 100\t\n",
      "('DAI62779', 'GRO85051', 'SNA80324')              |                 205|                 100\t\n",
      "('DAI43223', 'DAI62779', 'GRO59710')              |                 205|                 100\t\n",
      "('ELE17451', 'ELE32164', 'GRO59710')              |                 202|                 100\t\n",
      "('DAI62779', 'ELE17451', 'SNA59903')              |                 202|                 100\t\n",
      "('DAI62779', 'GRO73461', 'SNA80324')              |                 198|                 100\t\n",
      "('DAI75645', 'GRO85051', 'SNA80324')              |                 192|                 100\t\n",
      "('DAI62779', 'DAI85309', 'ELE92920')              |                 191|                 100\t\n",
      "('DAI55148', 'DAI62779', 'FRO40251')              |                 189|                 100\t\n",
      "('DAI55148', 'DAI62779', 'SNA80324')              |                 188|                 100\t\n",
      "('DAI62779', 'GRO30386', 'GRO73461')              |                 186|                 100\t\n",
      "('FRO40251', 'GRO21487', 'GRO73461')              |                 182|                 100\t\n"
     ]
    }
   ],
   "source": [
    "!bash wrapperQ34.sh ProductPurchaseData.txt mapperQ37.py reducerQ37.py combinerQ34.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Question 3.8 (Optional)\n",
    "**\n",
    "\n",
    "#### Solution: \n",
    "\n",
    "Here's some text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
